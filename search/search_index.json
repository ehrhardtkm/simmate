{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>This directory hosts the build script for Simmate's online API documentation. You can access the live docs here. If you would like to host our documentation locally for offline access, then you can run the following commands:</p> <pre><code>mkdocs build\n</code></pre> <p>You can then open up the generated files to view the offline documentation.</p>"},{"location":"change_log/","title":"Updates","text":""},{"location":"change_log/#check-your-installed-version","title":"Check your installed version","text":"<p>When importing simmate and establishing a connection to the database, a warning message will be displayed if your version is not the most recent one. You can also verify this via the command-line:</p> <pre><code>simmate version\n</code></pre> <pre><code># example output\nInstalled version: v0.10.0\nNewest available: v0.11.1\n</code></pre>"},{"location":"change_log/#upgrading-to-the-latest-version","title":"Upgrading to the Latest Version","text":"<p>We strongly advise installing simmate in a fresh conda environment instead of updating it within your current environment: <pre><code>conda create -n my_env -c conda-forge python=3.11 simmate\n</code></pre></p> <p>Ensure that the expected version is installed: <pre><code>simmate version\n</code></pre></p> <p>Update your database to be compatible with the new installation: <pre><code>simmate database update\n</code></pre></p> <p>Warning</p> <p>The command <code>simmate database update</code> is only effective from <code>v0.15.0</code> onwards. Earlier versions of simmate necessitate a complete database reset when updating versions. Hence, if your version is <code>&lt;=0.14.0</code>, you must execute <code>simmate database reset</code>.</p>"},{"location":"change_log/#understanding-version-numbers","title":"Understanding Version Numbers","text":"<p>Our releases adhere to semantic versioning. This implies that versions (e.g., <code>v1.2.3</code>) correspond to <code>MAJOR.MINOR.PATCH</code>. Each version number increases following these changes:</p> <ul> <li><code>MAJOR</code> = incompatible API changes</li> <li><code>MINOR</code> = addition of new functionality (without API changes)</li> <li><code>PATCH</code> = bug fixes and documentation updates (without API changes)</li> </ul> <p>There is one significant exception to the above rules -- <code>MAJOR</code>=0 releases. Any v0.x.y release is considered developmental, with APIs subject to change and not deemed stable. This is in line with the semantic version specification (refer to point 4).</p>"},{"location":"change_log/#upcoming-release","title":"Upcoming Release","text":"<p>Tip</p> <p>To view ongoing changes that haven't been finalized or merged yet, check our active pull-requests on GitHub</p> <p>Enhancements - added <code>dotdict</code> utility for easy dot-access with nested dictionary objects - all settings can be added via environment variables for cloud-based deployments - add <code>@workflow</code> decorator for easily creating basic workflows - add <code>_incar_updates</code> to <code>VaspWorkflow</code>s for cleaner inheritance &amp; syntax</p> <p>Refactors - Fully reimplemented how all settings are loaded</p> <p>Fixes - fix bug where user-provided <code>command</code> parameter is not properly loaded</p>"},{"location":"change_log/#v0150-20240104","title":"v0.15.0 (2024.01.04)","text":"<p>Enhancements</p> <ul> <li>Allow custom html templates and static files that override those shipped by default</li> <li>add <code>simmate engine</code> commands like <code>stats-detail</code> and <code>workitems</code> that show details of workitems in the database</li> <li>updated third-party archives (Materials Project, COD, JARVIS, OQMD)</li> <li>eased database updates between simmate versions by using django migrations</li> <li>added <code>bypass_nones</code> utility to help handle imperfect datasets where not all entries have a column</li> <li>misc updates to the website interface, especially to the <code>data explorer</code> pages</li> <li>add Microsoft allauth support to sign in use external account</li> <li>allow \"login required\" access to server via evironment variables (disabled by default)</li> <li>allow \"internal only\" access to server via evironment variables (disabled by default)</li> <li>add <code>django-simple-history</code> support to track user changes on specific models</li> <li>add <code>@check_db_conn</code> decorator to help with database connection closures/timeouts</li> <li>add <code>simmate engine start-schedules</code> which let's you configure periodic tasks for individual apps (e.g. check a table for updates every 5 minutes). Includes error handling and email alerts. (Note: this a quick alternative to full Prefect system)</li> <li>add warren_lab app with Warren Lab preferred VASP settings</li> <li>add badelf app with class oriented tools for performing BadELF analyses</li> <li>add basic quantum espresso support</li> </ul> <p>Refactors</p> <ul> <li>remove mamba support now that libmamba is default conda solver</li> <li>switch CI from mamba back to conda</li> </ul> <p>Fixes</p> <ul> <li>fix bug where workers incorrectly grab substring tag matches (e.g. a worker submited with the tag <code>ex</code> would incorrectly grab jobs like <code>ex-01</code> or <code>ex-02</code>)</li> </ul> <p>0.15.1 (2024.01.11)</p> <ul> <li>refactor &amp; fix bugs for <code>warren_lab</code> and <code>badelf</code> apps</li> </ul>"},{"location":"change_log/#v0140-20230706","title":"v0.14.0 (2023.07.06)","text":"<p>Enhancements</p> <ul> <li>add <code>django-unicorn</code> to deps to enable dynamic fullstack web UIs</li> <li>add ChemDoodle js/css to website headers for use elsewhere</li> <li>add \"Example Scripts\" section to doc website with several new scripts</li> <li>many updates to the web UI to accomodate molecular datasets and workflows</li> <li>add <code>simmate engine</code> commands to help with tags and different queues</li> <li>add docs to help with simmate workers, clusters, and tagging</li> <li>add <code>simmate-vasp</code> command for common VASP utilities like testing config, plotting, and prepping inputs</li> <li>add utilities to chunk larger than memory datafiles (csv, sdf, cifs, etc.)</li> </ul> <p>Refactors</p> <ul> <li>VASP potcar references to \"element mappings\" is now standarized to \"potcar mappings\"</li> <li>refactor docs with new \"Apps\" section</li> <li>full refactor of <code>simmate engine</code> commands. many have been shortened/renamed</li> <li>restrategized workflows that use output files from others. The <code>copy_previous_directory</code> input parameter has been removed and replaced with workflow attributes like <code>use_previous_directory</code> and <code>has_prerequisite</code>, combined with an optional <code>previous_directory</code> parameter.</li> </ul> <p>Fixes</p> <ul> <li>fix bug where hyphens aren't allowed in the database name</li> <li>fix guide for DO database setup</li> <li>fix incorrect evolutionary search imports</li> <li>hide pymatgen POTCAR warnings</li> <li>fix github CI bug for MacOS being unstable</li> <li>fix bug for zombie jobs causing evolutionary search to hang</li> <li>fix premature triggering of frozen error</li> </ul>"},{"location":"change_log/#v0130-20230306","title":"v0.13.0 (2023.03.06)","text":"<p>Enhancements</p> <ul> <li>add <code>relax_bulk</code> and <code>relax_endpoints</code> parameters to optionally turn off pre-relaxations in NEB</li> <li>add CLEASE app for cluster expanison calculations (these workflows are highly experimental at the moment - so use with caution)</li> <li>update \"bad-elf\" workflow to accept an empty-atom template structure or a list of empty sites</li> <li>add python 3.11 support</li> <li><code>simmate database reset</code> now supports Postgres (requires admin user)</li> <li>docker images are now published to DockerHub and Github packages</li> </ul> <p>Refactors</p> <ul> <li><code>calculators</code> module is now the <code>apps</code> module and terminology is changed throughout the repo</li> <li>many dependencies are reworked to optional dependencies as all <code>apps</code> are now optional</li> <li><code>workflow_engine</code> module has been renamed to <code>engine</code> to help shorten commands and import lines</li> <li>rework CI to use mamba instead of conda</li> <li>pull out dependencies for some apps that are now optional</li> <li>reorganize <code>Incar</code> class and move some functionality to general <code>utilities</code></li> <li>NEB module is reorganized to help with building custom sets</li> </ul> <p>Fixes</p> <ul> <li>fix site ordering in NEB supercell structures</li> <li>improve installation speed and guide users to conda alternatives like mamba</li> <li>clean up docs and fix several links</li> <li>apps are now registered to the web UI</li> </ul> <p>Warning</p> <p>The refactoring of simmate \"apps\" led to many breaking changes in the python API. We strongly recommend clearing your <code>~/simmate/</code> directory, especially the <code>my_env-apps.yaml</code> file because app names have changed.</p> <p>0.13.1 (2023.03.11)</p> <ul> <li>recover from <code>connection already closed</code> errors after long workflow runs</li> <li>fix bug where <code>simmate database reset</code> fails when there is no database <code>postgres</code> available</li> <li>update django regression of <code>django.db.backends.postgresql_psycopg2</code> to <code>django.db.backends.postgresql</code></li> <li>fix bug where simmate cannot read vasp results due atypical number (e.g. -0.33328-312)</li> <li>fix bug where postgres cannot json serialize bs or dos results (int64 numbers)</li> <li>fix incorrect pointing of VASP potcars in matproj presets</li> <li>from <code>from_directory</code> method of the <code>Relaxation</code> database class</li> <li>fix HSE bandstructure and DOS kpoint file writing</li> </ul> <p>0.13.2 (2023.03.20)</p> <ul> <li>fix pickling error for <code>workflow.run_cloud</code> command</li> <li><code>simmate.website.third_parties</code> module is now the <code>data_explorer</code> module. With this, you can now specify custom database tables to appear in the \"Data\" section for the web UI</li> </ul>"},{"location":"change_log/#v0120-20221023","title":"v0.12.0 (2022.10.23)","text":"<p>Enhancements</p> <ul> <li>add structure creators for <code>ASE</code>, <code>GASP</code>, <code>PyXtal</code>, <code>AIRSS</code>, <code>CALYPSO</code>, <code>USPEX</code>, and <code>XtalOpt</code> as well as documentation for creators.</li> <li>add <code>simmate version</code> command</li> <li>changelog and update guide added to documentation website</li> <li>add <code>show-stats</code>, <code>delete-finished</code>, and <code>delete-all</code> commands to <code>workflow-engine</code></li> <li>add <code>Cluster</code> base class + commands that allow submitting a steady-state cluster via subprocesses or slurm</li> <li>add <code>started_at</code>, <code>created_at</code>, <code>total_time</code>, and <code>queue_time</code> columns to <code>Calculation</code> tables</li> <li>add <code>exlcude_from_archives</code> field to workflows to optionally delete files when compressing outputs to zip archives</li> <li>various improvements added for evolutionary search workflows, such as parameter optimization, new output files, and website views</li> <li>add <code>Fingerprint</code> database table and integrate it with <code>Fingerprint</code> validator</li> <li>support &gt;2 element hull diargrams and complex chemical systems</li> </ul> <p>Refactors</p> <ul> <li>optimize <code>get_series</code> method of <code>relaxation.vasp.staged</code></li> <li>reorganize <code>selectors</code> module for evolutionary structure prediction</li> </ul> <p>Fixes</p> <ul> <li>fix dynamic loading of toolkit structures from third-party databases</li> <li>fix race condition with workers and empty queues</li> <li>increases default query rate for <code>state.result()</code> to lessen database load</li> </ul>"},{"location":"change_log/#v0110-20220910","title":"v0.11.0 (2022.09.10)","text":"<p>Enhancements</p> <ul> <li>REST API fields can now be specified directly with the <code>api_filters</code> attribute of any <code>DatabaseTable</code> class &amp; fields from mix-ins are automatically added</li> <li>add <code>archive_fields</code> attribute that sets the \"raw data\" for the database table &amp; fields from mix-ins are automatically added</li> <li>accept <code>TOML</code> input files in addition to <code>YAML</code></li> <li>convergence plots and extras are now written for many workflow types (such as relaxations)</li> <li>when <code>use_database=True</code>, output files are automatically written and the workup method is directly paired with the database table.</li> <li>NEB workflow now accepts parameters to tune how distinct pathways are determined, including the max pathway length and cutoffs at 1D percolation.</li> <li>add <code>MatplotlibFigure</code> and <code>PlotlyFigure</code> classes to help with writing output files and also implementing these figures in the website UI</li> <li>update website to include workflow calculator types and add API links</li> <li>custom projects and database tables are now registered with Simmate and a intro guide has been added</li> <li>continued updates for <code>structure-prediction</code> workflows</li> <li>add inspection of methods for default input values and display them in metadata</li> </ul> <p>Refactors</p> <ul> <li>the <code>website.core_components.filters</code> module has been absorbed into the <code>DatabaseTable</code> class/module</li> <li>yaml input for custom workflows now matches the python input format</li> <li>workup methods are largely depreciated and now database entries are returned when a workflow has <code>use_database=True</code></li> <li>several NEB input parameters have been renamed to accurate depict their meaning.</li> <li>customized workflow runs now save in the original database table under the \"-custom\" workflow name</li> <li><code>structure_string</code> column renamed to <code>structure</code> to simplify api logic</li> <li>clean up <code>toolkit.validators</code> module and establish fingerprint base class</li> <li><code>calculators</code> and <code>workflows</code> modules are now based on simmate apps</li> </ul> <p>Fixes</p> <ul> <li>fix bug in windows dev env where <code>simmate run-server</code> fails to find python path</li> <li>fix bug in <code>workflows explore</code> command where 'vasp' is the assumed calculator name</li> <li>fix broken example code in custom workflow docs</li> <li>fix broken website links and workflow views</li> </ul> <p>0.11.1 (2022.09.12)</p> <ul> <li>fix transaction error with workers on a PostGres backend</li> </ul>"},{"location":"change_log/#v0100-20220829","title":"v0.10.0 (2022.08.29)","text":"<p>Enhancements</p> <ul> <li>add NEB base classes to inherit from for making new subflows</li> <li>improve formatting of logging and cli using <code>typer</code> and <code>rich</code></li> <li>cli now supports auto-completion to help with long commands</li> <li>add <code>convergence_limit</code> parameter to evolutionary search that works alongside <code>limit_best_survival</code>. This will absorb minor changes in energy with equivalent structures from prolonging the search.</li> <li>add <code>ExtremeSymmetry</code> transformation to attempt symmetry reduction on disordered structure</li> <li>account for structures in <code>fixed-composition</code> having fewer nsites than input becuase of symmetry reduction during relaxation. Also, add <code>min_structures_exact</code> parameter to ensure we have at least N structures with the expected number of sites</li> <li>add experimental <code>variable-composition</code> (variable refers to nsites, not stoichiometry) and <code>binary-composition</code> evolutionary searches</li> <li>allow custom workflows to run from yaml</li> <li>update MatProj data to new api, and add severl new columns for data (e.g. mag + band gap)</li> </ul> <p>Refactors</p> <ul> <li>isolate optional dependencies so that our install is smaller</li> <li>remove click in favor of higher-level package (typer)</li> <li><code>pre_standardize_structure</code> and <code>pre_sanitize_structure</code> functionality is now merged in to a <code>standardize_structure</code> parameter that accepts different mode. <code>symmetry_tolerance</code> and <code>angle_tolerance</code> parameters can also modify the symmetry analysis done.</li> <li>metadata files are now numbered to allow multiple metadata files in the same directory</li> <li>refactor &amp; clean up transformation module for readability</li> <li>remove <code>SimmateFuture</code> class and merge functionality into <code>WorkItem</code></li> <li>switch from pdoc to mkdocs for documentation and remove <code>get_doc_from_readme</code>. Code and doc organization are now decoupled.</li> <li>rename run commands based on user preference. the <code>run</code> is now <code>run-quick</code>. <code>run-yaml</code> is now <code>run</code>. <code>run-cloud</code> now assumes a yaml input.</li> <li>remove <code>tqdm</code> dependency in favor of <code>rich.progress</code></li> <li>refactor transformations to static methods</li> </ul> <p>Fixes</p> <ul> <li>fix <code>module not found</code> error by adding ASE to dependencies</li> <li>fix bug with postgres database trying to delete sqlite locally</li> <li>fix dask throwing errors with logging</li> <li>fix bug where <code>fixed-composition</code> searches fail to detect individuals that have been symmetrically reduced (and therefore have fewer nsites than expected)</li> <li>fix evolutionary search failures when writing output files while files are opened/locked</li> <li>fix NEB workflows failing due to Walltime handler</li> <li>fix NEB workflows hints for <code>workup</code> failure due to missing start/end image folders</li> </ul>"},{"location":"change_log/#v090-20220817","title":"v0.9.0 (2022.08.17)","text":"<p>Enhancements</p> <ul> <li>improve the warning associated with workflow failure because of \"command not found\" issues</li> <li>workers now ignore and reset tasks that fail with \"command not found\". 2 workers failing with this error will result in the WorkItem being canceled</li> <li><code>RandomWySites</code> can now generate wyckoff combinations lazily (or up front) depending on use case</li> <li>add <code>simmate utilities</code> command group with <code>archive-old-runs</code></li> <li>add <code>start-cluster</code> command for starting many local workers</li> <li>add <code>structure-prediction</code> workflows</li> <li>add plotting/output utilities to <code>EvolutionarySearch</code> and <code>relaxation.vasp.staged</code></li> </ul> <p>Refactors</p> <ul> <li>evolutionary search now delay creations, transformations, and validation until runtime (used to be at time of structure submission)</li> <li><code>directory</code>, <code>compress_ouput</code>, and <code>run_id</code> are now default input parameters for subclasses of <code>Workflow</code>. If these are unused, the <code>run_config</code> must include <code>**kwargs</code></li> <li>add <code>isort</code> for organizing module imports throughout package</li> </ul> <p>Fixes</p> <ul> <li>fixed when <code>source</code> is not being registered by several workflows</li> <li>fix docker image for installing anaconda, blender, and simmate on ubuntu</li> </ul>"},{"location":"change_log/#v080-20220811","title":"v0.8.0 (2022.08.11)","text":"<p>Enhancements</p> <ul> <li>NEB workflows now accept parameters for changing supercell size and number of images used</li> <li>add HSE workflows for static energy, relaxation, and DOS/BS electronic states</li> <li>add NPT and MatProj molecular dynamics workflows</li> <li>add SCAN workflows for static energy and relaxation</li> <li>test files can be provided within zip files, fixing excessive line counts on git commits</li> <li>add simmate worker that can run \"out-of-box\" and requires no set up</li> <li>add logging for useful debugging and monitoring of workflows</li> <li>pinned dependencies to maximum versions and manage with dependabot</li> </ul> <p>Refactors</p> <ul> <li>to simplify the creation of new workflows, <code>S3Task</code> is now <code>S3Workflow</code> and database tables are dynamically determined using the workflow name</li> <li>workflows of a given type (e.g. relaxation or static-energy) now share database tables in order to simplify overall database architecture</li> <li>migrate from <code>os.path</code> to <code>pathlib.Path</code> throughout package</li> <li>isolate prefect use to separate executors</li> <li>updated tutorials for new workflow engine and workers</li> <li>remove use of <code>setup.py</code> in favor of <code>pyproject.toml</code></li> </ul>"},{"location":"change_log/#v070-20220719","title":"v0.7.0 (2022.07.19)","text":"<p>Enhancements</p> <ul> <li>add guide for installing VASP v5 to Ubuntu v22.04 (@scott-materials, #183)</li> <li>add <code>simmate database load-remote-archives</code> command and <code>load_remote_archives</code> utility that populates all tables from <code>database.third_parties</code></li> <li>add <code>load_default_sqlite3_build</code> utility that downloads a pre-built database with all third-party data present. This is an alternative to calling <code>load_all_remote_archives</code> if you are using sqlite3 and saves a significant amount of time for users.</li> <li>standardize workflow naming. Note this breaks from python naming conventions for classes (#150)</li> <li>dynamically determine <code>register_kwargs</code> and rename property to <code>parameters_to_register</code></li> <li>add full-run unittests that call workflows and vasp (without emulation)</li> <li>add walltime error handler that properly shuts down calculations when a SLURM job is about to expire</li> <li>add option to restart workflows from a checkpoint</li> <li>automatically build api documentation using github actions</li> </ul> <p>Refactors</p> <ul> <li>refactor <code>start-worker</code> command to use prefect instead of the experimental django executor</li> <li>remove experimental <code>workflow_engine.executor</code></li> <li>move contents of <code>configuration.django.database</code> to <code>database.utilities</code></li> <li>upgraded to Prefect v2 (\"Orion\"). This involved the refactoring the entire <code>workflow_engine</code> module, and therefore the entire workflow library. (#185).</li> </ul> <p>0.7.1 (2022.07.19)</p> <ul> <li>fix incorrect handling of prefect v2 futures by workflows</li> </ul> <p>0.7.2 (2022.08.03)</p> <ul> <li>fix missing SVG files for web UI (#196).</li> </ul> <p>0.7.3 (2022.08.04)</p> <ul> <li>fix incorrect passing of <code>source</code> in NEB all-paths workflow causing it to fail</li> </ul>"},{"location":"change_log/#v060-20220625","title":"v0.6.0 (2022.06.25)","text":"<p>Enhancements</p> <ul> <li>add <code>AflowPrototypes</code> to the <code>database.third_parties</code> module (only includes data distributed through pymatgen)</li> <li>add new modules to <code>toolkit.structure_prediction</code> and <code>toolkit.creation</code>, including ones to provide <code>known</code>, <code>substitution</code>, and <code>prototype</code> based structures.</li> <li>add <code>created_at</code> and <code>updated_at</code> columns to all database tables</li> <li>check if there is a newer version of Simmate available and let the user know about the update</li> <li>add experimental <code>badelf</code> workflow for determining electride character</li> <li>add <code>electronic-structure</code> workflow which carries out both DOS and BS calculations</li> <li>add <code>database_obj</code> attribute to the <code>toolkit.Structure</code> base class that is dynamically set</li> </ul> <p>Refactors</p> <ul> <li>standardize <code>database_table</code> attribute for workflows by merging <code>calculation_table</code> and <code>result_table</code> attributes (#102)</li> <li>removed use of <code>-s</code>, <code>-c</code>, and <code>-d</code> shortcuts from the <code>workflows</code> commands</li> <li>refactor <code>relaxation/staged</code> workflow to run in single parent directory</li> <li>refactor evolutionary search algorithm (alpha feature)</li> <li>condense where parsing/deserialization of workflow parameters occurs to the refactored the <code>load_input_and_register</code> task. Originally, this would occur in multiple places (e.g. in the CLI module before submission, in the workflow run_cloud method, in the LoadInputAndRegister task, etc.) and involved boilerplate code. (#173)</li> <li>refactor experimental features <code>register_kwargs</code> and <code>customized</code> workflows</li> <li>refactor <code>LoadInputAndRegister</code> and <code>SaveOutputTask</code> to <code>load_input_and_register</code> and <code>save_result</code></li> </ul> <p>Fixes</p> <ul> <li>fix import for <code>visualization.structure.blender</code> module (@bocklund, #180)</li> <li>fix bug where <code>command</code> or <code>directory</code> improperly passes <code>None</code> when they are not set in the <code>simmate workflows run</code> command</li> <li>fix bug where <code>update_all_stabilities</code> grabs incomplete calculations (#177)</li> <li>fix bug where SCF calculation is not completed before the non-SCF DOS or BS calculation and causes the workflows to fail (#171)</li> <li>fix bug for Bader workflow by registering the prebader workflow (#174)</li> <li>fix bug where <code>source</code> is not determined with yaml-file submissions (#172)</li> </ul>"},{"location":"change_log/#v050-20220530","title":"v0.5.0 (2022.05.30)","text":"<ul> <li>update CI to test all OSs and pin pytest&lt;7.1 as temporary fix for #162</li> <li>fix spelling typos in <code>keyword_modifiers</code> (@laurenmm, #165)</li> <li>users can now apply their own unique keyword modifiers to Incars -- such as how we allow \"__per_atom\" or \"__smart_ismear\" tags on Incar settings. This change involved refactoring how <code>keyword_modifiers</code> are implemented for the <code>vasp.inputs.Incar</code> class. Rather than static methods attached to the base class, modifiers are now dynamically applied using the <code>add_keyword_modifier</code> classmethod.</li> <li>large update of <code>calculators.vasp.tasks</code> module where many new presets are reimplemented from <code>pymatgen.io.vasp.sets</code>. This includes robust unit testing to confirm that generated inputs match between simmate and pymatgen (see #157 for a list of presets)</li> <li>catch error with vasp freezing when <code>Brmix</code> handler switches to kerker mixing (@becca9835, #159)</li> </ul>"},{"location":"change_log/#v040-20220424","title":"v0.4.0 (2022.04.24)","text":"<ul> <li>add <code>description_doc_short</code> + <code>show_parameters</code> to workflows and use these to update the UI</li> <li>add django-allauth dependency for account management and google/github sign-ins</li> <li>archive directory as <code>simmate_attempt_01.zip</code> whenever an error handler is triggered</li> <li>depreciate the workflow parameter <code>use_prev_directory</code> in favor of <code>copy_previous_directory</code></li> </ul>"},{"location":"change_log/#v030-20220419","title":"v0.3.0 (2022.04.19)","text":"<ul> <li>add highly customizable VASP workflow</li> <li>add Bader analysis and ELF workflows</li> <li>update module readmes to warn of experimental features</li> <li>reorganize <code>toolkit</code> module</li> </ul>"},{"location":"change_log/#v020-20220415","title":"v0.2.0 (2022.04.15)","text":"<ul> <li>start the CHANGELOG!</li> <li>refactor API views and add <code>SimmateAPIViewSet</code> class</li> <li>refactor <code>simmate start-project</code> command and underlying methods</li> <li>refactor <code>simmate workflow-engine run-cluster</code> command and underlying methods</li> <li>continue outlining <code>file_converters</code> module</li> </ul>"},{"location":"change_log/#v014-20220412","title":"v0.1.4 (2022.04.12)","text":"<ul> <li>web interface styling</li> <li>minor bug fixes</li> </ul>"},{"location":"change_log/#v000-20220328","title":"v0.0.0 (2022.03.28)","text":"<ul> <li>initial release</li> <li>adding tests and docs</li> </ul>"},{"location":"home/","title":"Welcome!","text":""},{"location":"home/#before-you-begin","title":"Before you begin","text":"<p>This website is your go-to resource for all our tutorials and guides. Before diving in, you might want to explore:</p> <ul> <li>Our main website at simmate.org</li> <li>Our source code at github.com/jacksund/simmate</li> </ul>"},{"location":"home/#what-is-simmate","title":"What is Simmate?","text":"<p>Simmate, or the Simulated Materials Ecosystem, is a comprehensive toolbox and framework designed for computational materials research. It allows you to explore various crystal databases, predict new materials, and easily calculate properties such as electronic, elastic, thermodynamic, and more.</p> <p>Computational research can be intimidating because there are so many programs to choose from, and it's challenging to select and combine them for your specific project. Simmate is designed to bridge this gap, acting as the link between these diverse programs, databases, and utilities. We take on the heavy lifting and provide clear explanations of these programs along the way.</p> <p>We also provide an extremely powerful toolbox and API for experts. Those familiar with the field can view Simmate as an alternative to the Materials Project stack (Atomate, PyMatGen, MatMiner, and more), where we operate under a different coding philosophy. Our top priorities are usability and readability. We therefore distribute Simmate as an \"all-in-one\" package, including a core material science toolkit, workflow management, database ORM, and a website interface. To learn more about the design choices in Simmate compared to other codes, visit our comparisons and benchmarks page.</p>"},{"location":"home/#a-sneak-peak-of-features","title":"A Sneak-Peak of Features","text":""},{"location":"home/#prebuilt-workflows","title":"Prebuilt Workflows","text":"<p>Simmate comes with ready-to-use workflows for most common material properties, ranging from simple XRD pattern prediction to intensive dynamic simulations. All workflows can be submitted via a website user-interface, the command-line, or custom python scripts:</p> yamlcommand linetomlpythonwebsite <pre><code># in example.yaml\nworkflow_name: relaxation.vasp.matproj\nstructure: NaCl.cif\ncommand: mpirun -n 8 vasp_std &gt; vasp.out\n</code></pre> <pre><code>simmate workflows run example.yaml\n</code></pre> <pre><code>simmate workflows run relaxation.vasp.matproj --structure NaCl.cif\n</code></pre> <pre><code># in example.toml\nworkflow_name = \"relaxation.vasp.matproj\"\nstructure = \"NaCl.cif\"\ncommand = \"mpirun -n 8 vasp_std &gt; vasp.out\"\n</code></pre> <pre><code>simmate workflows run example.yaml\n</code></pre> <pre><code>from simmate.workflows.relaxation import Relaxation__Vasp__Matproj as workflow\n\nstate = workflow.run(structure=\"NaCl.cif\")\nresult = state.result()\n</code></pre> <pre><code>https://simmate.org/workflows/static-energy/vasp/matproj/submit\n</code></pre>"},{"location":"home/#scalable-workflows","title":"Scalable Workflows","text":"<p>Simmate adjusts to your project's scale, whether on a single computer or across thousands of machines. It supports various settings, including university clusters with SLURM or PBS, and cloud platforms using Kubernetes and Docker.</p> create workflowschedule jobsadd remote resources <pre><code>from simmate.workflows import workflow\n\n@workflow(name=\"example.basic.hello\")\ndef hello(**kwargs):\n    print(\"Hello world!\")\n</code></pre> <pre><code>state = workflow.run_cloud(structure=\"NaCl.cif\")  # (1)\nresult = state.result()  # (2)\n</code></pre> <ol> <li>On your local computer, schedule your workflow run. This is as easy as replacing \"run\" with \"run_cloud\". This returns a \"future-like\" object.</li> <li>Calling result will wait until the job completes and grab the result! Note, the job won't run until you start a worker that is connected to the same database</li> </ol> <pre><code>simmate engine start-worker  # (1)\n</code></pre> <ol> <li>In a separate terminal or even on a remote HPC cluster, you can start a worker that will start running any scheduled jobs</li> </ol>"},{"location":"home/#full-feature-database","title":"Full-Feature Database","text":"<p>Simmate's database manages your private data while also integrating with third-party databases such as COD, Materials Project, JARVIS, and others. It automatically constructs tables with common data types by including a wide range of standard columns. You can then access this data through a web interface, REST API, SQL, or Python ORM:</p> pythonSQLREST APIwebsite <pre><code>from simmate.database import connect # (1)\nfrom simmate.database.third_parties import MatprojStructure\n\n# EXAMPLE 1\nstructures = MatprojStructure.objects.filter(nsites__lt=6).all() # (2)\n\n# EXAMPLE 2\nstructures = MatprojStructure.objects.filter(  # (3)\n    nsites__gte=3,\n    energy__isnull=False,\n    density__range=(1,5),\n    elements__icontains='\"C\"',\n    spacegroup__number=167,\n).all()\n\n# Quickly convert to excel, a pandas dataframe, or toolkit structures.\ndf = structures.to_dataframe()\nstructures = structures.to_toolkit()\n</code></pre> <ol> <li>Follow the database tutorial to build our initial database with the command <code>simmate database reset</code></li> <li>Retrieves all structures with less than 6 sites in their unit cell</li> <li>This filter retrieves structures with: greater or equal to 3 sites, an energy value, density between 1 and 5, the element Carbon, and spacegroup number 167</li> </ol> <pre><code>SELECT *\nFROM data_explorer_matprojstructure\nWHERE nsites &gt;= 3\n  AND energy IS NOT NULL\n  AND density BETWEEN 1 AND 5\n  AND elements ILIKE '%\"C\"%'\n  AND spacegroup_number = 167;\n</code></pre> <pre><code>https://simmate.org/third-parties/MatprojStructure/?format=api\n</code></pre> <pre><code>https://simmate.org/third-parties/MatprojStructure/\n</code></pre>"},{"location":"home/#need-help","title":"Need help?","text":"<p>Post your question here in our discussion section. </p>"},{"location":"home/#extra-resources","title":"Extra resources","text":"<ul> <li>Requesting a new feature</li> <li>Exploring alternatives to Simmate</li> <li>Citing Simmate</li> </ul>"},{"location":"parameters/","title":"Parameters","text":""},{"location":"parameters/#overview","title":"Overview","text":"<p>This section provides a detailed overview of all unique parameters for all workflows.</p> <p>To identify the parameters allowed for a specific workflow, use the <code>explore</code> command or <code>workflow.show_parameters()</code>:</p> command linepython <pre><code>simmate workflows explore\n</code></pre> <pre><code>workflow.show_parameters()\n</code></pre>"},{"location":"parameters/#algorithm","title":"algorithm","text":"<p>This parameter is specific to the BadELf workflows in the warrenapp. Options include <code>badelf</code>, <code>voronelf</code>, or <code>zero-flux</code>.</p> yamltomlpython <pre><code>algorithm: badelf\n</code></pre> <pre><code>algorithm = \"badelf\"\n</code></pre> <pre><code>algorithm = \"badelf\"\n</code></pre>"},{"location":"parameters/#angle_tolerance","title":"angle_tolerance","text":"<p>This parameter is used to determine if the angles between sites are symmetrically equivalent when <code>standardize_structure=True</code>. The value is in degrees.</p> yamltomlpython <pre><code>angle_tolerance: 10.0\n</code></pre> <pre><code>angle_tolerance = 10.0\n</code></pre> <pre><code>angle_tolerance = 10.0\n</code></pre>"},{"location":"parameters/#best_survival_cutoff","title":"best_survival_cutoff","text":"<p>In evolutionary searches, fixed compositions will stop when the best individual remains unbeaten for this number of new individuals. </p> <p>To account for similar structures (e.g., identical structures with minor energy differences), structures within the <code>convergence_cutoff</code> parameter (e.g., +1meV) are not considered when counting historical structures. This helps to prevent the search from continuing in cases where the search is likely already converged but making &lt;0.1meV improvements. The default is typically set based on the number of atoms in the composition.</p> yamltomlpython <pre><code>best_survival_cutoff: 100\n</code></pre> <pre><code>best_survival_cutoff = 100\n</code></pre> <pre><code>best_survival_cutoff = 100\n</code></pre>"},{"location":"parameters/#check_for_covalency","title":"check_for_covalency","text":"<p>This parameter is unique to the badelf workflows of the warrenapp. It indicates whether the algorithm should search the structure for covalency features. It is generally recommended to leave this as True. Covalency is not currently handled by the BadELF algorithm and covalency features in the ELF can heavily throw off the partitioning scheme, causing nonsense results.</p> yamltomlpython <pre><code>check_for_covalence: true\n</code></pre> <pre><code>check_for_covalence = true\n</code></pre> <pre><code>check_for_covalence = True\n</code></pre>"},{"location":"parameters/#chemical_system","title":"chemical_system","text":"<p>This parameter specifies the chemical system to be used in the analysis. It should be given as a string in the format <code>Element1-Element2-Element3-...</code>. For example, <code>Na-Cl</code>, <code>Y-C</code>, and <code>Y-C-F</code> are valid chemical systems.</p> yamltomlpython <pre><code>chemical_system: Na-Cl\n</code></pre> <pre><code>chemical_system = \"Na-Cl\"\n</code></pre> <pre><code>chemical_system = \"Na-Cl\"\n</code></pre> <p>Warning</p> <p>Some workflows only accept a chemical system with a specific number of elements. An example of this is the <code>structure-prediction.python.binary-composition</code> search which only allows two elements (e.g. <code>Y-C-F</code> would raise an error)</p>"},{"location":"parameters/#command","title":"command","text":"<p>This parameter specifies the command that will be called during the execution of a program. There is typically a default set for this that you only need to change if you'd like parallelization. For example, VASP workflows use <code>vasp_std &gt; vasp.out</code> by default but you can override this to use <code>mpirun</code>.</p> yamltomlpython <pre><code>command: mpirun -n 8 vasp_std &gt; vasp.out\n</code></pre> <pre><code>command = \"mpirun -n 8 vasp_std &gt; vasp.out\"\n</code></pre> <pre><code>command = \"mpirun -n 8 vasp_std &gt; vasp.out\"\n</code></pre>"},{"location":"parameters/#composition","title":"composition","text":"<p>The composition input can be anything compatible with the <code>Composition</code> toolkit class. Note that compositions are sensitive to atom counts / multiplicity. There is a difference between giving <code>Ca2N</code> and <code>Ca4N2</code> in several workflows. Accepted inputs include:</p> <p>a string (recommended)</p> yamltomlpython <pre><code>composition: Ca2NF\n</code></pre> <pre><code>composition = \"Ca2NF\"\n</code></pre> <pre><code>composition = \"Ca2NF\"\n</code></pre> <p>a dictionary that gives the composition</p> yamltomlpython <pre><code>composition:\n    Ca: 2\n    N: 1\n    F: 1\n</code></pre> <pre><code>[composition]\nCa = 2\nN = 1\nF = 1\n</code></pre> <pre><code>composition={\n    \"Ca\": 2, \n    \"N\": 1, \n    \"F\": 1,\n}\n</code></pre> <p>a <code>Composition</code> object (best for advanced logic)</p> python <pre><code>from simmate.toolkit import Compositon\n\ncomposition = Composition(\"Ca2NF\")\n</code></pre>"},{"location":"parameters/#compress_output","title":"compress_output","text":"<p>This parameter determines whether to compress the <code>directory</code> to a zip file at the end of the run. After compression, it will also delete the directory. The default is False.</p> yamltomlpython <pre><code>compress_output: true\n</code></pre> <pre><code>compress_output = true\n</code></pre> <pre><code>compress_output = True\n</code></pre>"},{"location":"parameters/#convergence_cutoff","title":"convergence_cutoff","text":"<p>For evolutionary searches, the search will be considered converged when the best structure is not changing by this amount (in eV). In order to officially signal the end of the search, the best structure must survive within this convergence limit for a specific number of new individuals -- this is controlled by the <code>best_survival_cutoff</code>. The default of 1meV is typically sufficient and does not need to be changed. More often, users should update <code>best_survival_cutoff</code> instead.</p> yamltomlpython <pre><code>convergence_cutoff: 0.005\n</code></pre> <pre><code>convergence_cutoff = 0.005\n</code></pre> <pre><code>convergence_cutoff = 0.005\n</code></pre>"},{"location":"parameters/#cores","title":"cores","text":"<p>This parameter is exclusive to the BadELF workflows in the warrenapp. It specifies the number of computer cores that the BadELF algorithm can utilize. Note that this refers specifically to cores and not threads.</p> yamltomlpython <pre><code>cores: 10\n</code></pre> <pre><code>cores = 10\n</code></pre> <pre><code>cores = 10\n</code></pre>"},{"location":"parameters/#copy_previous_directory","title":"copy_previous_directory","text":"<p>This parameter determines whether to copy the directory from the previous calculation (if one exists) and use it as a starting point for the new calculation. This is only possible if you provided an input that points to a previous calculation. For instance, <code>structure</code> would need to use a database-like input:</p> yamltomlpython <pre><code>structure:\n    database_table: Relaxation\n    database_id: 123\ncopy_previous_directory: true\n</code></pre> <pre><code>copy_previous_directory: true\n\n[structure]\ndatabase_table = \"Relaxation\"\ndatabase_id = 123\n</code></pre> <pre><code>structure = {\"database_table\": \"Relaxation\", \"database_id\": 123}\ncopy_previous_directory=True\n</code></pre>"},{"location":"parameters/#diffusion_analysis_id","title":"diffusion_analysis_id","text":"<p>(advanced users only) This is the entry id from the <code>DiffusionAnalysis</code> table to link the results to. This is set automatically by higher-level workflows and rarely (if ever) set by the user.</p>"},{"location":"parameters/#directory","title":"directory","text":"<p>This is the directory where everything will be run -- either as a relative or full path. This is passed to the utilities function <code>simmate.ulitities.get_directory</code>, which generates a unique folder name if not provided (such as <code>simmate-task-12390u243</code>). This will be converted into a <code>pathlib.Path</code> object. Accepted inputs include:</p> <p>leave as default (recommended)</p> <p>a string</p> yamltomlpython <pre><code>directory: my-new-folder-00\n</code></pre> <pre><code>directory = \"my-new-folder-00\"\n</code></pre> <pre><code>directory = \"my-new-folder-00\"\n</code></pre> <p>a <code>pathlib.Path</code> (best for advanced logic)</p> python <pre><code>from pathlib import Path\n\ndirectory = Path(\"my-new-folder-00\")\n</code></pre>"},{"location":"parameters/#directory_new","title":"directory_new","text":"<p>Exclusive to the <code>restart.simmate.automatic</code> workflow, this is the folder where the workflow will be continued. It follows the same rules/inputs as the <code>directory</code> parameter.</p>"},{"location":"parameters/#directory_old","title":"directory_old","text":"<p>Exclusive to the <code>restart.simmate.automatic</code> workflow, this is the original folder that should be used as the starting point. It follows the same rules/inputs as the <code>directory</code> parameter.</p>"},{"location":"parameters/#electride_connection_cutoff","title":"electride_connection_cutoff","text":"<p>Exclusive to the badelf workflows in the simmate app. This is the ELF value cutoff to be allowed when determining the dimensionality of an electride. The default is 0, but a good option might be 0.5 which is the normal cutoff for what is considered an electride.</p> yamltomlpython <pre><code>electride_connection_cutoff: 0.5\n</code></pre> <pre><code>electride_connection_cutoff = 0.5\n</code></pre> <pre><code>electride_connection_cutoff = 0.5\n</code></pre>"},{"location":"parameters/#electride_finder_cutoff","title":"electride_finder_cutoff","text":"<p>Exclusive to the badelf workflows in the simmate app. This is the minimum ELF value that the algorithm will consider an electride. Any maxima in the ELF below this will not be considered an electride site during the algorithm.</p> yamltomlpython <pre><code>electride_finder_cutoff: 0.5\n</code></pre> <pre><code>electride_finder_cutoff = 0.5\n</code></pre> <pre><code>electride_finder_cutoff = 0.5\n</code></pre>"},{"location":"parameters/#find_electrides","title":"find_electrides","text":"<p>Exclusive to the badelf workflows in the simmate app. This parameter indicates whether the algorithm should search for electrides. Reasons to set this as false may be that the user knows there is no electride character in the structure of interest or if the user has manually placed electride sites.</p> yamltomlpython <pre><code>find_electrides: true\n</code></pre> <pre><code>find_electrides = true\n</code></pre> <pre><code>find_electrides = True\n</code></pre>"},{"location":"parameters/#fitness_field","title":"fitness_field","text":"<p>(advanced users only) For evolutionary searches, this is the value that should be optimized. Specifically, it should minimize this value (lower value = better fitness). The default is <code>energy_per_atom</code>, but you may want to set this to a custom column in a custom database table.</p>"},{"location":"parameters/#input_parameters","title":"input_parameters","text":"<p>(experimental feature) Exclusive to <code>customized.vasp.user-config</code>. This is a list of parameters to pass to <code>workflow_base</code>.</p>"},{"location":"parameters/#is_restart","title":"is_restart","text":"<p>(experimental feature) This parameter indicates whether the calculation is a restarted workflow run. The default is False. If set to true, the workflow will go through the given directory (which must be provided) and determine where to resume.</p> yamltomlpython <pre><code>directory: my-old-calc-folder\nis_restart: true\n</code></pre> <pre><code>directory = \"my-old-calc-folder\"\nis_restart = true\n</code></pre> <pre><code>directory = \"my-old-calc-folder\"\nis_restart = True\n</code></pre>"},{"location":"parameters/#max_atoms","title":"max_atoms","text":"<p>For workflows that involve generating a supercell or random structure, this will be the maximum number of sites to allow in the generated structure(s). For example, an evolutionary search may set this to 10 atoms to limit the compositions &amp; stoichiometries that are explored.</p> yamltomlpython <pre><code>max_atoms: 10\n</code></pre> <pre><code>max_atoms = 10\n</code></pre> <pre><code>max_atoms = 10\n</code></pre>"},{"location":"parameters/#max_path_length","title":"max_path_length","text":"<p>For diffusion workflows, this is the maximum length allowed for a single path.</p> yamltomlpython <pre><code>max_path_length: 3.5\n</code></pre> <pre><code>max_path_length = 3.5\n</code></pre> <pre><code>max_path_length = 3.5\n</code></pre>"},{"location":"parameters/#max_stoich_factor","title":"max_stoich_factor","text":"<p>This is the maximum stoichiometric ratio that will be analyzed. In a binary system evolutionary search, this will only look at non-reduced compositions up to the max_stoich_factor. For example, this means Ca2N and max factor of 4 would only look up to Ca8N4 and skip any compositions with more atoms (e.g. Ca10N5 is skipped)</p> yamltomlpython <pre><code>max_stoich_factor: 5\n</code></pre> <pre><code>max_stoich_factor = 5\n</code></pre> <pre><code>max_stoich_factor = 5\n</code></pre>"},{"location":"parameters/#max_structures","title":"max_structures","text":"<p>For workflows that generate new structures (and potentially run calculations on them), this will be the maximum number of structures allowed. The workflow will end at this number of structures regardless of whether the calculation/search is converged or not.</p> yamltomlpython <pre><code>max_structures: 100\n</code></pre> <pre><code>max_structures = 100\n</code></pre> <pre><code>max_structures = 100\n</code></pre> <p>Warning</p> <p>In <code>structure-prediction</code> workflows, <code>min_structure_exact</code> takes priority  over this setting, so it is possible for your search to exceed your  maximum number of structures. If you want <code>max_structures</code> to have absolute control, you can set <code>min_structure_exact</code> to 0.</p>"},{"location":"parameters/#max_supercell_atoms","title":"max_supercell_atoms","text":"<p>For workflows that involve generating a supercell, this will be the maximum number of sites to allow in the generated structure(s). For example, NEB workflows would set this value to something like 100 atoms to limit their supercell image sizes.</p> yamltomlpython <pre><code>max_supercell_atoms: 100\n</code></pre> <pre><code>max_supercell_atoms = 100\n</code></pre> <pre><code>max_supercell_atoms = 100\n</code></pre>"},{"location":"parameters/#migrating_specie","title":"migrating_specie","text":"<p>This is the atomic species/element that will be moving in the analysis (typically NEB or MD diffusion calculations). Note, oxidation states (e.g. \"Ca2+\") can be used, but this requires your input structure to be oxidation-state decorated as well.</p> yamltomlpython <pre><code>migrating_specie: Li\n</code></pre> <pre><code>migrating_specie = \"Li\"\n</code></pre> <pre><code>migrating_specie = \"Li\"\n</code></pre>"},{"location":"parameters/#migration_hop","title":"migration_hop","text":"<p>(advanced users only) The atomic path that should be analyzed. Inputs are anything compatible with the <code>MigrationHop</code> class of the <code>simmate.toolkit.diffusion</code> module. This includes:</p> <ul> <li><code>MigrationHop</code> object</li> <li>a database entry in the <code>MigrationHop</code> table</li> </ul> <p>(TODO: if you'd like full examples, please ask our team to add them)</p>"},{"location":"parameters/#migration_images","title":"migration_images","text":"<p>The full set of images (including endpoint images) that should be analyzed. Inputs are anything compatible with the <code>MigrationImages</code> class of the <code>simmate.toolkit.diffusion</code> module, which is effectively a list of <code>structure</code> inputs. This includes:</p> <p><code>MigrationImages</code> object</p> <p>a list of <code>Structure</code> objects</p> <p>a list of filenames (cif or POSCAR)</p> yamltomlpython <pre><code>migration_images:\n    - image_01.cif\n    - image_02.cif\n    - image_03.cif\n    - image_04.cif\n    - image_05.cif\n</code></pre> <pre><code>migration_images = [\n    \"image_01.cif\",\n    \"image_02.cif\",\n    \"image_03.cif\",\n    \"image_04.cif\",\n    \"image_05.cif\",\n]\n</code></pre> <pre><code>migration_images = [\n    \"image_01.cif\",\n    \"image_02.cif\",\n    \"image_03.cif\",\n    \"image_04.cif\",\n    \"image_05.cif\",\n]\n</code></pre>"},{"location":"parameters/#min_atoms","title":"min_atoms","text":"<p>This is the opposite of <code>max_atoms</code> as this will be the minimum number of sites allowed in the generate structure(s). See <code>max_atoms</code> for details.</p>"},{"location":"parameters/#min_structures_exact","title":"min_structures_exact","text":"<p>(experimental) The minimum number of structures that must be calculated with exactly matching nsites as specified in the fixed-composition.</p>"},{"location":"parameters/#min_supercell_atoms","title":"min_supercell_atoms","text":"<p>This is the opposite of <code>max_supercell_atoms</code> as this will be the minimum number of sites allowed in the generated supercell structure.</p>"},{"location":"parameters/#min_supercell_vector_lengths","title":"min_supercell_vector_lengths","text":"<p>When generating a supercell, this is the minimum length for each lattice vector of the generated cell (in Angstroms). For workflows such as NEB, larger is better but more computationally expensive.</p> yamltomlpython <pre><code>min_supercell_vector_lengths: 7.5\n</code></pre> <pre><code>min_supercell_vector_lengths = 7.5\n</code></pre> <pre><code>min_supercell_vector_lengths = 7.5\n</code></pre>"},{"location":"parameters/#nfirst_generation","title":"nfirst_generation","text":"<p>For evolutionary searches, no mutations or \"child\" individuals will be scheduled until this number of individuals have been calculated. This ensures we have a good pool of candidates calculated before we start selecting parents and mutating them.</p> yamltomlpython <pre><code>nfirst_generation: 15\n</code></pre> <pre><code>nfirst_generation = 15\n</code></pre> <pre><code>nfirst_generation = 15\n</code></pre>"},{"location":"parameters/#nimages","title":"nimages","text":"<p>The number of images (or structures) to use in the analysis. This does NOT include the endpoint images (start/end structures). More is better, but computationally expensive. We recommend keeping this value odd in order to ensure there is an image at the midpoint.</p> yamltomlpython <pre><code>nimages: 5\n</code></pre> <pre><code>nimages = 5\n</code></pre> <pre><code>nimages = 5\n</code></pre> <p>Danger</p> <p>For apps such as VASP, your <code>command</code> parameter must use a number of cores that is divisible by <code>nimages</code>. For example, <code>nimages=3</code> and <code>command=\"mpirun -n 10 vasp_std &gt; vasp.out\"</code> will fail because 10 is not divisible by 3.</p>"},{"location":"parameters/#nsteadystate","title":"nsteadystate","text":"<p>This parameter sets the number of individual workflows to be scheduled at once, effectively setting the queue size of an evolutionary search. The number of workflows run in parallel is determined by the number of <code>Workers</code> started. However, the <code>nsteadystate</code> value sets the maximum number of parallel runs as the queue size will never exceed this value. This parameter is closely tied with <code>steadystate_sources</code>.</p> yamltomlpython <pre><code>nsteadystate: 50\n</code></pre> <pre><code>nsteadystate = 50\n</code></pre> <pre><code>nsteadystate = 50\n</code></pre>"},{"location":"parameters/#nsteps","title":"nsteps","text":"<p>This parameter sets the total number of steps for the calculation. For instance, in molecular dynamics workflows, the simulation will stop after this many steps.</p> yamltomlpython <pre><code>nsteps: 10000\n</code></pre> <pre><code>nsteps = 10000\n</code></pre> <pre><code>nsteps = 10000\n</code></pre>"},{"location":"parameters/#percolation_mode","title":"percolation_mode","text":"<p>This parameter sets the percolating type to detect. The default is \"&gt;1d\", which searches for percolating paths up to the <code>max_path_length</code>. Alternatively, this can be set to \"1d\" to stop unique pathway finding when 1D percolation is achieved.</p> yamltomlpython <pre><code>percolation_mode: 1d\n</code></pre> <pre><code>percolation_mode = \"1d\"\n</code></pre> <pre><code>percolation_mode = \"1d\"\n</code></pre>"},{"location":"parameters/#relax_bulk","title":"relax_bulk","text":"<p>This parameter determines whether the bulk structure (typically the input structure) should be relaxed before running the rest of the workflow.</p> yamltomlpython <pre><code>relax_bulk: false\n</code></pre> <pre><code>relax_bulk: false\n</code></pre> <pre><code>relax_bulk: false\n</code></pre>"},{"location":"parameters/#relax_endpoints","title":"relax_endpoints","text":"<p>This parameter determines whether the endpoint structures for an NEB diffusion pathway should be relaxed before running the rest of the workflow.</p> yamltomlpython <pre><code>relax_endpoints: false\n</code></pre> <pre><code>relax_endpoints: false\n</code></pre> <pre><code>relax_endpoints: false\n</code></pre>"},{"location":"parameters/#run_id","title":"run_id","text":"<p>This parameter is the id assigned to a specific workflow run/calculation. If not provided, this will be randomly generated. It is highly recommended to leave this at the default value. This id is based on unique-ids (UUID), so every id should be 100% unique and in a string format.</p> yamltomlpython <pre><code>run_id: my-unique-id-123\n</code></pre> <pre><code>run_id = \"my-unique-id-123\"\n</code></pre> <pre><code>run_id = \"my-unique-id-123\"\n</code></pre>"},{"location":"parameters/#search_id","title":"search_id","text":"<p>(advanced users only) This parameter is the evolutionary search that this individual is associated with. This allows us to determine which <code>Selector</code>, <code>Validator</code>, and <code>StopCondition</code> should be used when creating and evaluating the individual. When running a search, this is set automatically when submitting a new flow.</p>"},{"location":"parameters/#selector_kwargs","title":"selector_kwargs","text":"<p>(advanced users only) This parameter is a dictionary of extra conditions to use when initializing the selector class. <code>MySelector(**selector_kwargs)</code>. This is closely tied with the <code>selector_name</code> parameter.</p>"},{"location":"parameters/#selector_name","title":"selector_name","text":"<p>(experimental feature; advanced users only) This parameter is the base selector class that should be used. The class will be initialized using <code>MySelector(**selector_kwargs)</code>. The input should be given as a string.</p> <p>Warning</p> <p>Currently, we only support truncated selection, so this should be left at its default value.</p>"},{"location":"parameters/#singleshot_sources","title":"singleshot_sources","text":"<p>(experimental feature; advanced users only) This parameter is a list of structure sources that run once and never again. This includes generating input structures from known structures (from third-party databases), prototypes, or substituting known structures.</p> <p>In the current version of simmate, these features are not enabled and this input should be ignored.</p>"},{"location":"parameters/#sleep_step","title":"sleep_step","text":"<p>This parameter is the amount of time in seconds that the workflow will shutdown before restarting the cycle when there is a cycle within a workflow (such as iteratively checking the number of subworkflows submitted and updating results). For evolutionary searches, setting this to a larger value will save on computation resources and database load, so we recommend increasing it where possible.</p> yamltomlpython <pre><code>run_id: 180\n</code></pre> <pre><code>run_id = 180\n</code></pre> <pre><code>sleep_step = 180  # 3 minutes\n</code></pre>"},{"location":"parameters/#source","title":"source","text":"<p>(experimental feature; advanced users only) This parameter indicates where the input data (and other parameters) came from. The source could be a number of things including a third party id, a structure from a different Simmate database table, a transformation of another structure, a creation method, or a custom submission by the user.</p> <p>By default, this is a dictionary to account for the many different scenarios. Here are some examples of values used in this column:</p> python <pre><code># from a thirdparty database or separate table\nsource = {\n    \"database_table\": \"MatprojStructure\",\n    \"database_id\": \"mp-123\",\n}\n\n# from a random structure creator\nsource = {\"creator\": \"PyXtalStructure\"}\n\n# from a templated structure creator (e.g. substituition or prototypes)\nsource = {\n    \"creator\": \"PrototypeStructure\",\n    \"database_table\": \"AFLOWPrototypes\",\n    \"id\": 123,\n}\n\n# from a transformation\nsource = {\n    \"transformation\": \"MirrorMutation\",\n    \"database_table\":\"MatprojStructure\",\n    \"parent_ids\": [\"mp-12\", \"mp-34\"],\n}\n</code></pre> <p>Typically, the <code>source</code> is set automatically, and users do not need to update it.</p>"},{"location":"parameters/#standardize_structure","title":"standardize_structure","text":"<p>This parameter determines whether to standardize the structure during our setup(). This means running symmetry analysis on the structure to reduce the symmetry and convert it to some standardized form. There are three different forms to choose from and thus 3 different values that <code>standardize_structure</code> can be set to:</p> <ul> <li><code>primitive</code>: for the standard primitive unitcell</li> <li><code>conventional</code>: for the standard conventional unitcell</li> <li><code>primitive-LLL</code>: for the standard primitive unitcell that is then LLL-reduced</li> <li><code>False</code>: this is the default and will disable this feature</li> </ul> <p>We recommend using <code>primitive-LLL</code> when the smallest possible and most cubic unitcell is desired.</p> <p>We recommend using <code>primitive</code> when calculating band structures and ensuring we have a standardized high-symmetry path. Note,Existing band-structure workflows use this automatically.</p> <p>To control the tolerances used to symmetrize the structure, you can use the symmetry_precision and angle_tolerance attributes.</p> <p>By default, no standardization is applied.</p> yamltomlpython <pre><code>standardize_structure: primitive-LLL\n</code></pre> <pre><code>standardize_structure = \"primitive-LLL\"\n</code></pre> <pre><code>standardize_structure = \"primitive-LLL\"\n</code></pre>"},{"location":"parameters/#steadystate_source_id","title":"steadystate_source_id","text":"<p>(advanced users only) This parameter is the structure source that this individual is associated with. This allows us to determine how the new individual should be created. When running a search, this is set automatically when submitting a new flow.</p>"},{"location":"parameters/#steadystate_sources","title":"steadystate_sources","text":"<p>(experimental feature; advanced users only) This parameter is a dictionary of sources that will be scheduled at a \"steady-state\", meaning there will always be a set number of individuals scheduled/running for this type of structure source. This should be defined as a dictionary where each is <code>{\"source_name\": percent}</code>. The percent determines the number of steady stage calculations that will be running for this at any given time. It will be a percent of the <code>nsteadystate</code> parameter, which sets the total number of individuals to be scheduled/running. For example, if <code>nsteadystate=40</code> and we add a source of <code>{\"RandomSymStructure\": 0.30, ...}</code>, this means 0.25*40=10 randomly-created individuals will be running/submitted at all times. The source can be from either the <code>toolkit.creator</code> or <code>toolkit.transformations</code> modules.</p> yamlyamlpython <pre><code>singleshot_sources:\n    RandomSymStructure: 0.30\n    from_ase.Heredity: 0.30\n    from_ase.SoftMutation: 0.10\n    from_ase.MirrorMutation: 0.10\n    from_ase.LatticeStrain: 0.05\n    from_ase.RotationalMutation: 0.05\n    from_ase.AtomicPermutation: 0.05\n    from_ase.CoordinatePerturbation: 0.05\n</code></pre> <pre><code>[singleshot_sources]\n\"RandomSymStructure\": 0.30\n\"from_ase.Heredity\": 0.30\n\"from_ase.SoftMutation\": 0.10\n\"from_ase.MirrorMutation\": 0.10\n\"from_ase.LatticeStrain\": 0.05\n\"from_ase.RotationalMutation\": 0.05\n\"from_ase.AtomicPermutation\": 0.05\n\"from_ase.CoordinatePerturbation\": 0.05\n</code></pre> <pre><code>singleshot_sources = {\n    \"RandomSymStructure\": 0.30,\n    \"from_ase.Heredity\": 0.30,\n    \"from_ase.SoftMutation\": 0.10,\n    \"from_ase.MirrorMutation\": 0.10,\n    \"from_ase.LatticeStrain\": 0.05,\n    \"from_ase.RotationalMutation\": 0.05,\n    \"from_ase.AtomicPermutation\": 0.05,\n    \"from_ase.CoordinatePerturbation\": 0.05,\n}\n</code></pre> <p>Note: if your percent values do not sum to 1, they will be rescaled. When calculating <code>percent*nsteadystate</code>, the value will be rounded to the nearest integer.</p> <p>We are moving towards accepting kwargs or class objects as well, but this is not yet allowed. For example, anything other than <code>percent</code> would be treated as a kwarg:</p> toml <pre><code>[singleshot_sources.RandomSymStructure]\npercent: 0.30\nspacegroups_exclude: [1, 2, 3]\nsite_generation_method: \"MyCustomMethod\"\n</code></pre>"},{"location":"parameters/#structure","title":"structure","text":"<p>The crystal structure to be used for the analysis. The input can be anything compatible with the <code>Structure</code> toolkit class. Accepted inputs include:</p> <p>a filename (cif or poscar) (recommended)</p> yamltomlpython <pre><code>structure: NaCl.cif\n</code></pre> <pre><code>structure = NaCl.cif\n</code></pre> <pre><code>structure=\"NaCl.cif\"\n</code></pre> <p>a dictionary that points to a database entry. </p> yamltomlpython <pre><code># example 1\nstructure:\n    database_table: MatprojStructure\n    database_id: mp-123\n\n# example 2\nstructure:\n    database_table: StaticEnergy\n    database_id: 50\n\n# example 3\nstructure:\n    database_table: Relaxation\n    database_id: 50\n    structure_field: structure_final\n</code></pre> <pre><code># example 1\n[structure]\ndatabase_table: MatprojStructure\ndatabase_id: mp-123\n\n# example 2\n[structure]\ndatabase_table: StaticEnergy\ndatabase_id: 50\n\n# example 3\n[structure]\ndatabase_table: Relaxation\ndatabase_id: 50\nstructure_field: structure_final\n</code></pre> <pre><code># example 1\nstructure={\n    \"database_table\": \"MatprojStructure\",\n    \"database_id\": \"mp-123\",\n}\n\n# example 2\nstructure={\n    \"database_table\": \"StaticEnergy\",\n    \"database_id\": 50,\n}\n\n# example 3\nstructure={\n    \"database_table\": \"Relaxation\",\n    \"database_id\": 50,\n    \"structure_field\": \"structure_final\",\n}\n</code></pre> <p>Note</p> <p>instead of <code>database_id</code>, you can also use the <code>run_id</code> or <code>directory</code> to indicate which entry to load. Further, if the database table is linked to multiple structures (e.g. relaxations have a <code>structure_start</code> and <code>structure_final</code>), then you can also add the <code>structure_field</code> to specify which to choose. </p> <p>a <code>Structure</code> object (best for advanced logic)</p> python <pre><code>from simmate.toolkit import Structure\n\nstructure = Structure(\n    lattice=[\n        [2.846, 2.846, 0.000],\n        [2.846, 0.000, 2.846],\n        [0.000, 2.846, 2.846],\n    ],\n    species=[\"Na\", \"Cl\"],\n    coords=[\n        [0.5, 0.5, 0.5],\n        [0.0, 0.0, 0.0],\n    ],\n    coords_are_cartesian=False,\n)\n</code></pre> <p>a <code>Structure</code> database object</p> python <pre><code>structure = ExampleTable.objects.get(id=123)\n</code></pre> <p>json/dictionary serialization from pymatgen</p>"},{"location":"parameters/#subworkflow_kwargs","title":"subworkflow_kwargs","text":"<p>This parameter is a dictionary of parameters to pass to each subworkflow run. For example, the workflow will be ran as <code>subworkflow.run(**subworkflow_kwargs)</code>. Note, many workflows that use this argument will automatically pass information that is unique to each call (such as <code>structure</code>).</p> yamltomlpython <pre><code>subworkflow_kwargs:\n    command: mpirun -n 4 vasp_std &gt; vasp.out\n    compress_output: true\n</code></pre> <pre><code>[subworkflow_kwargs]\ncommand = \"mpirun -n 4 vasp_std &gt; vasp.out\"\ncompress_output = true\n</code></pre> <pre><code>subworkflow_kwargs=dict(\n    command=\"mpirun -n 4 vasp_std &gt; vasp.out\",\n    compress_output=True,\n)\n</code></pre>"},{"location":"parameters/#subworkflow_name","title":"subworkflow_name","text":"<p>This parameter is the name of workflow that used to evaluate structures generated. Any workflow that is registered and accessible via the <code>get_workflow</code> utility can be used instead. If you wish to submit extra arguments to each workflow run, you can use the <code>subworkflow_kwargs</code> parameter.</p> yamltomlpython <pre><code>subworkflow_name: relaxation.vasp.staged\n</code></pre> <pre><code>subworkflow_name = \"relaxation.vasp.staged\"\n</code></pre> <pre><code>subworkflow_name = \"relaxation.vasp.staged\"\n</code></pre>"},{"location":"parameters/#supercell_end","title":"supercell_end","text":"<p>This parameter is the endpoint image supercell to use. This is really just a <code>structure</code> parameter under a different name, so everything about the <code>structure</code> parameter also applies here.</p>"},{"location":"parameters/#supercell_start","title":"supercell_start","text":"<p>This parameter is the starting image supercell to use. This is really just a <code>structure</code> parameter under a different name, so everything about the <code>structure</code> parameter also applies here.</p>"},{"location":"parameters/#symmetry_precision","title":"symmetry_precision","text":"<p>If standardize_structure=True, then this is the cutoff value used to determine if the sites are symmetrically equivalent. (in Angstroms)</p> yamlpython <pre><code>symmetry_precision: 0.1\n</code></pre> <pre><code>symmetry_precision = 0.1\n</code></pre>"},{"location":"parameters/#tags","title":"tags","text":"<p>When submitting workflows via the <code>run_cloud</code> command, tags are 'labels' that help control which workers are allowed to pick up and run the submitted workflow. Workers should be started with matching tags in order for these scheduled flows to run.</p> <p>When no tags are set, the following default tags will be used for a Simmate workflow:</p> <ul> <li> <code>simmate</code> (this is the default worker tag as well)</li> <li> the workflow's type name</li> <li> the workflow's app name</li> <li> the full workflow name</li> </ul> <p>For example, the <code>static-energy.vasp.matproj</code> would have the following tags: <pre><code>- simmate\n- static-energy\n- vasp\n- static-energy.vasp.matproj\n</code></pre></p> <p>To override these default tags, use the following:</p> yamltomlpython <pre><code>tags:\n    - my-tag-01\n    - my-tag-02\n</code></pre> <pre><code>tags = [\"my-tag-01\", \"my-tag-02\"]\n</code></pre> <pre><code>tags = [\"my-tag-01\", \"my-tag-02\"]\n</code></pre> <p>Warning</p> <p>When you have a workflow that is submitting many smaller workflows (such as  <code>structure-prediction</code> workflows), make sure you set the tags in the <code>subworkflow_kwargs</code> settings: <pre><code>subworkflow_kwargs:\n    tags:\n        - my-tag-01\n        - my-tag-02\n</code></pre></p> <p>Bug</p> <p>Filtering tags does not always work as expected in SQLite3 because a worker with  <code>my-tag</code> will incorrectly grab jobs like <code>my-tag-01</code> and <code>my-tag-02</code>. This issue is known by both Django and SQLite3. Simmate addresses this issue by requiring all tags to be 7 characters long AND fully lowercase when using SQLite3.</p>"},{"location":"parameters/#temperature_end","title":"temperature_end","text":"<p>For molecular dynamics simulations, this is the temperature to end the simulation at (in Kelvin). This temperature will be reached through a linear transition from the <code>temperature_start</code> parameter.</p> yamlpython <pre><code>temperature_end: 1000\n</code></pre> <pre><code>temperature_end = 1000\n</code></pre>"},{"location":"parameters/#temperature_start","title":"temperature_start","text":"<p>For molecular dynamics simulations, this is the temperature to begin the simulation at (in Kelvin).</p> yamltomlpython <pre><code>temperature_start: 250\n</code></pre> <pre><code>temperature_start = 250\n</code></pre> <pre><code>temperature_start = 250\n</code></pre>"},{"location":"parameters/#time_step","title":"time_step","text":"<p>For molecular dynamics simulations, this is time time between each ionic step (in femtoseconds).</p> yamltomlpython <pre><code>time_step: 1.5\n</code></pre> <pre><code>time_step = 1.5\n</code></pre> <pre><code>time_step = 1.5\n</code></pre>"},{"location":"parameters/#updated_settings","title":"updated_settings","text":"<p>(experimental feature) Unique to <code>customized.vasp.user-config</code>. This is a list of parameters to update the <code>workflow_base</code> with. This often involves updating the base class attributes.</p>"},{"location":"parameters/#vacancy_mode","title":"vacancy_mode","text":"<p>For NEB and diffusion workfows, this determines whether vacancy or interstitial diffusion is analyzed. Default of True corresponds to vacancy-based diffusion.</p> yamltomlpython <pre><code>vacancy_mode: false\n</code></pre> <pre><code>vacancy_mode = false\n</code></pre> <pre><code>vacancy_mode = False\n</code></pre>"},{"location":"parameters/#validator_kwargs","title":"validator_kwargs","text":"<p>(advanced users only) This parameter is a dictionary of extra conditions to use when initializing the validator class. <code>MyValidator(**validator_kwargs)</code>. This is closely tied with the <code>validator_name</code> parameter.</p>"},{"location":"parameters/#validator_name","title":"validator_name","text":"<p>(experimental feature; advanced users only) This parameter is the base validator class that should be used. The class will be initialized using <code>MyValidator(**validator_kwargs)</code>. The input should be given as a string.</p> <p>Warning</p> <p>Currently, we only support <code>CrystallNNFingerprint</code> validation, so this should be left at its default value.</p>"},{"location":"parameters/#workflow_base","title":"workflow_base","text":"<p>(experimental feature) Unique to <code>customized.vasp.user-config</code>. This is the base workflow to use when updating critical settings.</p>"},{"location":"parameters/#write_summary_files","title":"write_summary_files","text":"<p>This parameter determines whether or not to write output files. For some workflows, writing output files can cause excessive load on the database and possibly make the calculation IO bound. In cases such as this, you can set this to <code>False</code>.</p> yamltomlpython <pre><code>write_summary_files: false\n</code></pre> <pre><code>write_summary_files = false\n</code></pre> <pre><code>write_summary_files = False\n</code></pre> <p>Tip</p> <p>Beginners can often ignore this setting. This is typically only relevant in a setup where you have many computational resources and have many evolutionary searches (&gt;10) running at the same time.</p>"},{"location":"apps/badelf/","title":"The BadELF App","text":"<p>Note</p> <p>The current application maintainer is Sam Weaver</p> <p>The BadELF algorithm and application are based on the following research:</p> <ul> <li>\"Counting Electrons in Electrides\" (JACS 2023)</li> </ul>"},{"location":"apps/badelf/#about-badelf","title":"About BadELF","text":"<p>BadELF is a method that combines Bader Charge Analysis and the Electron Localization Function (ELF) to predict oxidation states and perform population analysis of electrides. It uses Bader segmentation of the ELF to detect electride electrons and Voronoi segmentation of the ELF to identify atoms.</p>"},{"location":"apps/badelf/#about-this-app","title":"About this app","text":"<p>The BadELF application provides workflows and utilities to streamline BadELF analysis.</p>"},{"location":"apps/badelf/#installation","title":"Installation","text":"<ol> <li> <p>Make sure you have Simmate installed and have reset your database.</p> </li> <li> <p>Register the badelf app with simmate by adding <code>- simmate.apps.BadelfConfig</code> to <code>~/simmate/my_env-apps.yaml</code></p> </li> <li> <p>Update your database to include custom tables from the warrenapp <pre><code>simmate database update\n</code></pre></p> </li> </ol>"},{"location":"apps/badelf/#basic-use","title":"Basic Use","text":"<p>BadELF requires outputs from VASP calculations (e.g. the CHGCAR, ELFCAR, etc.). You can either (a) generate these on your own or (b) run a simmate workflow that does it for you. </p>"},{"location":"apps/badelf/#a-from-vasp-outputs","title":"(a) from VASP outputs","text":"<p>The BadELF algorithm can be run in a folder with VASP results. Please ensure your VASP settings generate a CHGCAR and ELFCAR with matching grid sizes. </p> <p>Create a yaml file: <pre><code># inside input.yaml\nworkflow_name: bad-elf.badelf.badelf\n\n# all parameters below are optional\ndirectory: /path/to/folder\ncores: 4\nfind_electrides: true\nmin_elf: 0.5\nalgorithm: badelf\nelf_connection_cutoff: 0\ncheck_for_covalency: true\n</code></pre></p> <p>And run the workflow: <pre><code>simmate workflows run input.yaml\n</code></pre></p>"},{"location":"apps/badelf/#b-from-structure","title":"(b) from structure","text":"<p>If you would prefer to have Simmate handle the VASP calculation, workflows are available that will first run the required DFT and then BadELF. </p> <p>These workflows are stored in the <code>Warren Lab</code> app, which contains our lab's preferred VASP settings. Refer to the <code>Warren Lab</code> app for more details and to view the available workflows.</p>"},{"location":"apps/bader_henkelman/","title":"Overview of Bader App","text":""},{"location":"apps/bader_henkelman/#introduction-to-bader","title":"Introduction to Bader","text":"<p>Bader Charge Analysis, often known as \"Bader\", is a technique for partitioning charge density to forecast oxidation states. This module is specifically tailored for the Henkelman Group's code that performs this analysis.</p> <p>You can access the open-source code here.</p>"},{"location":"apps/bader_henkelman/#about-the-bader-app","title":"About the Bader App","text":"<p>The Bader app utilizes Simmate tools to build workflows and utilities based on the Bader code from the Henkelman Lab. </p> <p>Typically, other workflows oversee the execution of the workflows registered in this app. For example, workflows in the <code>Warren Lab</code> app combine Bader, VASP, and rational settings. Hence, beginners are recommended to start with other apps.</p>"},{"location":"apps/bader_henkelman/#installation-guide-ubuntu-2204","title":"Installation Guide (Ubuntu 22.04)","text":""},{"location":"apps/bader_henkelman/#installing-the-bader-command","title":"Installing the <code>bader</code> command","text":"<ol> <li>Download the <code>Linux x86-64</code> version from the Henkelman website here</li> <li>Unzip the downloaded file. It should contain a single executable named \"bader\".</li> <li>Move the <code>bader</code> executable to a directory of your preference. For example, <code>~/jacksund/bader/bader</code> (inside a folder named bader in my home directory)</li> <li>Run <code>nano ~/.bashrc</code> to modify your bash and add this line at the end: <pre><code>export PATH=/home/jacksund/bader/:$PATH\n</code></pre></li> <li>Restart your terminal and test the command <code>bader --help</code></li> </ol>"},{"location":"apps/bader_henkelman/#installing-chgsumpl-and-additional-scripts","title":"Installing <code>chgsum.pl</code> and additional scripts","text":"<ol> <li>Download the scripts from VTST-tools</li> <li>Unzip the folder (<code>vtstscripts-1033</code>) and move it to the directory where your bader executable is located.</li> <li>Run <code>nano ~/.bashrc</code> to modify your bash and add this line at the end: <pre><code>export PATH=/home/jacksund/bader/vtstscripts-1033:$PATH\n</code></pre></li> <li>Restart your terminal and you're ready to perform Bader analyses with Simmate!</li> </ol>"},{"location":"apps/bader_henkelman/#useful-resources","title":"Useful Resources","text":"<ul> <li>Bader Website (includes documentation and guide)</li> </ul>"},{"location":"apps/clease/","title":"The CLEASE Software","text":"<p>Danger</p> <p>Be aware that this software is currently in its early testing phase. It is not advised to use it outside of the Warren Lab.</p>"},{"location":"apps/clease/#overview","title":"Overview","text":"<p>As stated on the official webpage: The Cluster Expansion in Atomic Simulation Environment (CLEASE) is a software package that simplifies the intricate setup and construction process of cluster expansion (CE). It provides a comprehensive set of tools for defining CE parameters, generating training structures, fitting effective cluster interaction (ECI) values, and running Monte Carlo simulations.</p> <p>This software is open-source and available for free.</p>"},{"location":"apps/clease/#helpful-resources","title":"Helpful Resources","text":"<ul> <li>GitLab Repository</li> <li>Official Documentation</li> <li>Published Paper</li> </ul>"},{"location":"apps/deepmd/","title":"DeePMD Application","text":"<p>Danger</p> <p>Be aware that this application is currently undergoing initial testing and is not advised for use outside of the Warren Lab.</p>"},{"location":"apps/deepmd/#overview","title":"Overview","text":"<p>The Deep Potential for Molecular Dynamics (DeePMD) is a software package designed to build machine-learned interatomic potentials using energy and force-field data. When trained on Density Functional Theory (DFT) data, these machine-learned potentials demonstrate significant efficiency in performing molecular dynamics simulations.</p> <p>This software is open-source and available for free.</p>"},{"location":"apps/deepmd/#useful-resources","title":"Useful Resources","text":"<ul> <li>GitHub Repository</li> <li>Official Documentation</li> <li>Research Paper</li> </ul>"},{"location":"apps/evolution/","title":"Evolutionary Searches","text":"<p>Danger</p> <p>This application is currently undergoing preliminary testing and is not intended for use outside of the Warren Lab.</p> <p>Danger</p> <p> Ensure you are using a cloud database if your workers are operating on separate machines or an HPC cluster. </p> <p>The default database backend (sqlite) is not designed for parallel connections from different computers. Your calculations will be slower and more prone to errors with sqlite.</p> <p>If you encounter the error <code>database is locked</code>, it means you have exceeded the capabilities of sqlite.</p> <p>Warning</p> <p>A functioning VASP installation is required, as with previous tutorials.</p>"},{"location":"apps/evolution/#quick-tutorial","title":"Quick Tutorial","text":""},{"location":"apps/evolution/#fixed-composition-search","title":"Fixed-Composition Search","text":"<p>1) Familiarize yourself with the input options and default settings of the <code>structure-prediction.toolkit.fixed-composition</code> workflow. <pre><code>simmate workflows explore\n</code></pre></p> <p>2) (Optional) If you wish to include prototype structures or known materials at the start of your search, ensure you have loaded the data into your cloud database.  This was discussed in a previous tutorial.</p> <p>3) Create your input <code>yaml</code> file (e.g. <code>my_search.yaml</code>). Refer to the full example for best practices.</p> Basic InputBest-Practice Input <pre><code>workflow_name: structure-prediction.toolkit.fixed-composition\ncomposition: Ca8N4F8\n</code></pre> <pre><code>workflow_name: structure-prediction.toolkit.fixed-composition\ncomposition: Ca8N4F8\n\nsubworkflow_kwargs:  # (1)\n    command: mpirun -n 8 vasp_std &gt; vasp.out  # (2)\n    compress_output: true  # (3)\n    # see 'simmate workflows explore' on `relaxation.vasp.staged`\n    # for other optional subworkflow_kwargs\n\nsleep_step: 300  # (4)\nnsteadystate: 100  # (5)\n\n# see 'simmate workflows explore' output for other optional inputs\n</code></pre> <ol> <li>The default input parameter <code>subworkflow: relaxation.vasp.staged</code> is used to analyze each structure. You can modify this workflow's behavior with the <code>subworkflow_kwargs</code> setting.</li> <li>This is the command each relaxation step will use.</li> <li>Since you probably won't be reviewing the output files of each calculation, and these results can consume a lot of disk space, this setting will convert completed calculations to <code>zip</code> files to save space.</li> <li>The status of individual structures and the writing of output files are checked in cycles. If your average relaxation takes 30 minutes, you don't need to check/update every 60 seconds (the default). Longer sleep steps help reduce database load.</li> <li>By default, <code>nsteadystate</code> is set to 40, meaning the search will maintain 40 total calculations in the queue at all times, regardless of the number of available workers. The number of workers (not <code>nsteadystate</code>) controls the number of parallel calculations. Only change this value if you want more than 40 calculations to run in parallel, which requires &gt;40 running workers. Therefore, you can increase this value if needed, but do NOT decrease <code>nsteadystate</code>.</li> </ol> <p>4) Submit the workflow settings file to start scheduling jobs. <pre><code>simmate workflows run my_search.yaml\n</code></pre></p> <p>5) Start workers by submitting the <code>start-worker</code> command to a cluster's queue (e.g., SLURM) or wherever you want jobs to run. Submit as many workers as the number of workflows you want to run in parallel:</p> Basic WorkerBest-Practice Worker <pre><code>simmate engine start-worker\n</code></pre> <pre><code># (1)\nsimmate engine start-worker --close-on-empty-queue --nitems-max 10\n</code></pre> <ol> <li><code>--close-on-empty-queue</code> shuts down the worker when the queue is empty, freeing up computational resources. <code>--nitems-max</code> limits the number of workflows each worker will run. Short-lived workers help maintain the health of a cluster and allow other SLURM jobs to cycle through the queue, preventing resource hogging.</li> </ol> <p>6) Monitor the output and log files for any issues. Important error information can also be accessed via the command line: <pre><code>simmate engine stats\nsimmate engine error-summary\n</code></pre></p> <p>7) Submit new workers or cancel stale workers as needed.</p>"},{"location":"apps/evolution/#binary-system-search","title":"Binary-System Search","text":"<ol> <li> <p>Follow the steps from the <code>fixed-composition</code> search above. The only differences here are the input yaml and follow-up searches. Ensure you read and understand the best practices as well.</p> </li> <li> <p>Create your input yaml file: <pre><code>workflow_name: structure-prediction.toolkit.chemical-system\nchemical_system: Ca-N\n</code></pre></p> </li> <li> <p>Submit the search <pre><code>simmate workflows run my_search.yaml\n</code></pre></p> </li> <li> <p>Submit workers as needed <pre><code>simmate engine start-worker\n</code></pre></p> </li> </ol>"},{"location":"apps/overview/","title":"Overview","text":""},{"location":"apps/overview/#understanding-apps","title":"Understanding Apps","text":"<p>Apps are essentially codes or programs designed to perform specific analyses. They can be based on external software or custom-built using Simmate for a particular technique.</p> <p>For instance, VASP is a program capable of running a variety of density functional theory (DFT) calculations. However, since it's not written in Python, we require some \"helper\" code to execute VASP commands, create input files, and extract data from the outputs.</p> <p>Similarly, Simmate includes an <code>evo_search</code> \"app\" that encompasses all the functionalities required to execute the evolutionary structure prediction algorithm.</p>"},{"location":"apps/overview/#code-structure","title":"Code Structure","text":"<p>All apps adhere to the same folder structure:</p> <pre><code>\u251c\u2500\u2500 example_app\n\u2502   \u251c\u2500\u2500 database\n\u2502   \u251c\u2500\u2500 error_handlers\n\u2502   \u251c\u2500\u2500 inputs\n\u2502   \u251c\u2500\u2500 outputs\n\u2502   \u251c\u2500\u2500 configuration\n\u2502   \u251c\u2500\u2500 tasks\n\u2502   \u251c\u2500\u2500 website\n\u2502   \u2514\u2500\u2500 workflows\n</code></pre> <p>Here's a logical breakdown of what each module contains:</p> <ul> <li><code>configuration</code>: Assists in installing the program and configuring common settings.</li> <li><code>inputs</code> &amp; <code>outputs</code>: Automates file generation and data loading into Python.</li> <li><code>error_handlers</code>: Helps rectify common calculation errors that cause the program to fail.</li> <li><code>tasks</code>: Defines how the program is set up, executed, and processed. It integrates all the <code>inputs</code>, <code>outputs</code>, and <code>error-handler</code> functions. A single task can be seen as a single call to the program (i.e., a single calculation).</li> <li><code>database</code>: Contains all the datatables for storing our results.</li> <li><code>workflows</code>: Combines <code>tasks</code> and <code>database</code> to set up individual tasks and manage the saving of results to our database.</li> <li><code>website</code>: Allows us to submit workflows and view results via our website interface.</li> </ul> <p>Note</p> <p>Beginners are advised to start with the <code>workflows</code> module as it integrates all other modules.</p>"},{"location":"apps/warren_lab/","title":"The Warren Lab Application","text":"<p>Note</p> <p>The current maintainer of this application is Sam Weaver</p>"},{"location":"apps/warren_lab/#overview","title":"Overview","text":"<p>The Warren Lab App is a product of Scott Warren's Materials Discovery Lab at the University of North Carolina (Chapel Hill). Our lab focuses on electrides, fluoride-ion batteries, and 2D materials. </p> <p>The <code>Warren Lab</code> application incorporates workflows for our lab's preferred DFT settings and common analyses. Registering this app will introduce numerous new workflow presets that enhance several other apps (VASP, Bader, BadELF, etc.).</p>"},{"location":"apps/warren_lab/#useful-resources","title":"Useful Resources","text":"<ul> <li>Scott Warren (UNC contact page)</li> <li>Lab Website</li> </ul>"},{"location":"apps/warren_lab/#installation","title":"Installation","text":"<ol> <li> <p>Ensure that Simmate is installed and that you have a functioning database.</p> </li> <li> <p>Incorporate the following apps into <code>~/simmate/my_env-apps.yaml</code>: <pre><code># Include app dependencies.\n# IMPORTANT: Some of these may already be present by default. Avoid adding duplicates\n- simmate.apps.configs.VaspConfig\n- simmate.apps.configs.BaderConfig\n- simmate.apps.configs.BadelfConfig\n# Include the Warren Lab app.\n- simmate.workflows.configs.WarrenLabConfig\n</code></pre></p> </li> <li> <p>Update your database to incorporate custom tables: <pre><code>simmate database update\n</code></pre></p> </li> </ol>"},{"location":"apps/warren_lab/#included-workflows","title":"Included Workflows","text":""},{"location":"apps/warren_lab/#vasp-relaxation","title":"VASP (Relaxation)","text":"<pre><code>relaxation.vasp.warren-lab-hse\nrelaxation.vasp.warren-lab-hse-with-wavecar\nrelaxation.vasp.warren-lab-hsesol\nrelaxation.vasp.warren-lab-pbe\nrelaxation.vasp.warren-lab-pbe-metal\nrelaxation.vasp.warren-lab-pbe-with-wavecar\nrelaxation.vasp.warren-lab-pbesol\nrelaxation.vasp.warren-lab-scan\n</code></pre>"},{"location":"apps/warren_lab/#vasp-static-energy","title":"VASP (Static Energy)","text":"<pre><code>static-energy.vasp.warren-lab-hse\nstatic-energy.vasp.warren-lab-hsesol\nstatic-energy.vasp.warren-lab-pbe\nstatic-energy.vasp.warren-lab-pbe-metal\nstatic-energy.vasp.warren-lab-pbesol\nstatic-energy.vasp.warren-lab-prebadelf-hse\nstatic-energy.vasp.warren-lab-prebadelf-pbesol\nstatic-energy.vasp.warren-lab-scan\n</code></pre>"},{"location":"apps/warren_lab/#badelf","title":"BadELF","text":"<pre><code>bad-elf-analysis.badelf.badelf-pbesol\n</code></pre>"},{"location":"apps/vasp/error_handlers/","title":"Error handlers","text":""},{"location":"apps/vasp/error_handlers/#vasp-error-handlers","title":"VASP Error Handlers","text":"<p>This module provides error handlers designed to resolve issues that may arise during VASP workflow executions. It is a refactored version of error handlers originally used by Custodian, serving as a direct alternative to the <code>custodian.vasp.handlers</code> module. A significant distinction is that our error handlers are divided into smaller units, enhancing visibility of the error and its corresponding solution.</p>"},{"location":"apps/vasp/inputs/","title":"Inputs","text":""},{"location":"apps/vasp/inputs/#vasp-input-files","title":"VASP Input Files","text":"<p>This module assists in generating and interpreting VASP input files. It is a derivative and restructured version of classes utilized by PyMatGen. Specifically, it serves as a direct substitute for the <code>pymatgen.io.vasp.inputs</code> module.</p>"},{"location":"apps/vasp/installation/","title":"VASP Installation","text":"<p>You can find the official guides on the VASP wiki here. This includes detailed guides for installing VASP on your personal computer, such as VASP 6 on Ubuntu 22.04.</p>"},{"location":"apps/vasp/installation/#vasp-544-on-ubuntu-2204","title":"VASP 5.4.4 on Ubuntu 22.04","text":"<p>This guide is tailored for the Warren Lab as it requires specific build files shared within the team.</p> <p>To ensure compatibility, we need to manually build all VASP dependencies (for instance, Ubuntu uses gcc v11 but VASP requires v9). For the Warren Lab, we have bundled everything into a single zip file to simplify the setup process. Copy /media/synology/software/vasp/vasp.zip from WarWulf to your computer, for example, your home directory (e.g. <code>/home/jacksund/</code>). This file is only 172.1MB, but will expand to over 9GB once VASP is fully installed.</p> <p>The VASP directory contains folders (1) to (9) for each build step. These are the 9 programs that need to be built from source. Source files (.tar.gz) are provided in each directory. Each folder also contains an \"install\" directory where the program will be installed. You'll also find another folder (usually the program name) that contains the unzipped contents of the .tar.gz.</p> <p>Ensure the necessary build tools are installed and up to date: <pre><code>sudo apt update\nsudo apt upgrade\nsudo apt install gcc make m4 g++ gfortran\n</code></pre></p> <ol> <li>Install <code>gmp</code></li> </ol> <pre><code>cd ~/vasp/01_gmp\ntar xvzf *.tar.gz\ncd gmp-6.2.1\n./configure --prefix=/home/jacksund/vasp/01_gmp/install\nlscpu\nmake -j 2\nmake install\nmake check # (optional) to confirm successful install\n</code></pre> <ol> <li> <p>Install <code>mpfr</code> <pre><code>cd ~/vasp/02_mpfr\ntar xvzf *.tar.gz\ncd mpfr-4.1.0\n./configure --prefix=/home/jacksund/vasp/02_mpfr/install --with-gmp=/home/jacksund/vasp/01_gmp/install\nmake -j 2\nmake install\nmake check\n</code></pre></p> </li> <li> <p>Install <code>mpc</code> <pre><code>cd ~/vasp/03_mpc\ntar xvzf *.tar.gz\ncd mpc-1.2.1\n./configure --prefix=/home/jacksund/vasp/03_mpc/install --with-gmp=/home/jacksund/vasp/01_gmp/install --with-mpfr=/home/jacksund/vasp/02_mpfr/install\nmake -j 2\nmake install\nmake check\n</code></pre></p> </li> <li> <p>Install <code>gcc</code> <pre><code>cd ~/vasp/04_gcc\ntar xvzf *.tar.gz\ncd gcc-9.5.0\nmkdir build\ncd build\n../configure --prefix=/home/jacksund/vasp/04_gcc/install --with-gmp=/home/jacksund/vasp/01_gmp/install --with-mpfr=/home/jacksund/vasp/02_mpfr/install --with-mpc=/home/jacksund/vasp/03_mpc/install --disable-multilib\nmake -j 2  # this command takes roughly 1hr\nmake install\n\nnano ~/.bashrc\n# ADD TO BOTTOM OF FILE\n#\n# export PATH=/home/jacksund/vasp/04_gcc/install/bin:$PATH\n# export LD_LIBRARY_PATH=/home/jacksund/vasp/04_gcc/install/lib64:$LD_LIBRARY_PATH\n\nsource ~/.bashrc\n</code></pre></p> </li> <li> <p>Install <code>openmpi</code> <pre><code>cd ~/vasp/05_openmpi\ntar xvzf *.tar.gz\ncd openmpi-4.1.4\nmkdir build\ncd build\n../configure --prefix=/home/jacksund/vasp/05_openmpi/install\nmake -j 2\nmake install\n\nnano ~/.bashrc\n# ADD TO BOTTOM OF FILE\n#\n# export PATH=/home/jacksund/vasp/05_openmpi/install/bin:$PATH\n# export LD_LIBRARY_PATH=/vasp/05_openmpi/install/lib:$LD_LIBRARY_PATH\n\nsource ~/.bashrc\nmpirun --help\n</code></pre></p> </li> <li> <p>Install <code>fftw</code> <pre><code>cd ~/vasp/06_fftw\ntar xvzf *.tar.gz\ncd fftw-3.3.10\nmkdir build\ncd build\n../configure --prefix=/home/jacksund/vasp/06_fftw/install\nmake -j 2\nmake install\n\nnano ~/.bashrc\n# ADD TO BOTTOM OF FILE\n#\n# export PATH=/home/jacksund/vasp/06_fftw/install/bin:$PATH\n\nsource ~/.bashrc\n</code></pre></p> </li> <li> <p>Install <code>lapack</code> (blas, cblas, lapacke, lapack) <pre><code>cd ~/vasp/07_lapack\ntar xvzf *.tar.gz\ncd lapack-3.10.1\nmv make.inc.example make.inc\nmake all\nmkdir /home/jacksund/vasp/07_lapack/install\ncp *.a /home/jacksund/vasp/07_lapack/install\ncd ../install\ncp librefblas.a libblas.a\n</code></pre></p> </li> <li> <p>Install <code>scalapack</code> <pre><code>cd ~/vasp/08_scalapack\ntar xvzf *.tar.gz\ncd scalapack-2.2.0\nmv SLmake.inc.example SLmake.inc\n\nnano SLmake.inc\n# Edit these lines: (leave then uncommented)\n#\n# BLASLIB = -L/home/jacksund/vasp/07_lapack/install -lblas\n# LAPACKLIB = -L/home/jacksund/vasp/07_lapack/install -llapack\n# LIBS = $(LAPACKLIB) $(BLASLIB)\n\nmake all\ncp libscalapack.a ../../07_lapack/install\n</code></pre></p> </li> <li> <p>Install <code>vasp</code> <pre><code>cd ~/vasp/09_vasp\n\nnano makefile.include\n# Edit these lines: (leave then uncommented)\n#\n# LIBDIR     = /home/jacksund/vasp/07_lapack/install/\n# FFTW       ?= /home/jacksund/vasp/06_fftw/install\n\nmake std\n\nnano ~/.bashrc\n# ADD TO BOTTOM OF FILE\n#\n# export PATH=/home/jacksund/vasp/09_vasp/bin/:$PATH\n\nsource ~/.bashrc\n</code></pre></p> </li> </ol> <p>You can now use commands like <code>mpirun -n 4 vasp_std</code>!!! If you try this immediately, you'll see the \"error\" (VASP fails because no input files are present).. <pre><code>Error reading item 'VCAIMAGES' from file INCAR.\nError reading item 'VCAIMAGES' from file INCAR.\nError reading item 'VCAIMAGES' from file INCAR.\nError reading item 'VCAIMAGES' from file INCAR.\n</code></pre></p>"},{"location":"apps/vasp/neb/","title":"Nudged Elastic Band (NEB)","text":"<p>NEB is a technique used to determine the energy barrier of a specific migration pathway, such as atomic diffusion.</p>"},{"location":"apps/vasp/neb/#components-of-nudged-elastic-band","title":"Components of Nudged Elastic Band","text":"<p>** Note: Steps 3-5 are executed for each individual pathway **</p> <ol> <li> <p>Relax the initial bulk structure</p> </li> <li> <p>Identify all symmetrically unique pathways</p> </li> <li> <p>Relax the start/end supercell images      (or only one if they are equivalent)</p> </li> <li> <p>Interpolate the start/end supercell images and relax these     using IDPP.</p> </li> <li> <p>Relax all images using NEB</p> </li> </ol>"},{"location":"apps/vasp/neb/#potential-inputs","title":"Potential Inputs","text":"<p>Depending on the workflow, a user may wish to begin at a different step. For instance, they might have a specific start/end structure they want to use instead of automatically detecting unique pathways. Hence, we divide this workflow into several smaller ones. We need to consider the following input scenarios:</p> <ol> <li> <p><code>all_paths</code>: A bulk crystal and diffusing species are provided, and a full analysis of all paths is requested.</p> </li> <li> <p><code>single_path</code>: A bulk crystal, diffusing species, and pathway index are provided. The index specifies the particular pathway to be extracted from DistinctPathFinder. For instance, if DistinctPathFinder finds 5 unique paths, you can specify \"I want to run the 4th pathway in this list (i.e., the 4th shortest pathway).\"</p> </li> <li> <p><code>from_startend_sites</code>: A bulk crystal and start/end sites for the diffusing ion are provided, and that specific pathway should be analyzed. This would allow for a diffusion pathway that goes from a site at (0,0,0) to (1,1,1), for example.</p> </li> <li> <p><code>from_endpoints</code>: Two endpoint supercell structures are provided, and the specific pathway (from interpolated structures) should be analyzed.</p> </li> <li> <p><code>from_images</code>: A series of images are provided and analyzed.</p> </li> </ol> <p>In addition to these five scenarios, we also consider...</p> <ul> <li>Vacancy vs. interstitial diffusion</li> <li>Plugins for CI-NEB (currently not implemented)</li> </ul>"},{"location":"apps/vasp/neb/#useful-links","title":"Useful Links","text":"<ul> <li>Atomate NEB yaml</li> <li>Atomate NEB py</li> <li>pymatgen-diffusion neb</li> <li>pymatgen-diffusion pathfinder </li> <li>VASP's guide for NEB</li> <li>Plug-in guide for CI-NEB</li> </ul>"},{"location":"apps/vasp/outputs/","title":"Outputs","text":""},{"location":"apps/vasp/outputs/#vasp-output-files","title":"VASP Output Files","text":"<p>This module assists in generating and interpreting VASP output files. It is a derivative and restructured version of classes utilized by PyMatGen. Specifically, it serves as a direct substitute for the <code>pymatgen.io.vasp.outputs</code> module.</p>"},{"location":"apps/vasp/overview/","title":"VASP Application","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a proprietary software designed for atomic scale materials modelling based on first principles. A license is required to use this software.</p>"},{"location":"apps/vasp/overview/#useful-resources","title":"Useful Resources","text":"<ul> <li>Official Website</li> <li>User Manual</li> <li>Tutorials</li> </ul>"},{"location":"apps/vasp/workflows/","title":"Workflows","text":""},{"location":"apps/vasp/workflows/#vasp-workflow-library","title":"VASP Workflow Library","text":"<p>This module outlines the configurations for standard VASP workflows.</p> <p>Numerous workflows are essentially a division and restructuring of classes utilized by PyMatGen and Atomate. Specifically, this serves as a direct substitute to the <code>pymatgen.io.vasp.sets</code> module and the <code>atomate.vasp.workflows</code> module. Instead of constructing these tasks from numerous lower-level functions, we simplify these classes into a single <code>VaspWorkflow</code> class for easier interaction.</p>"},{"location":"full_guides/overview/","title":"Comprehensive Guides &amp; API Reference for Simmate","text":""},{"location":"full_guides/overview/#before-you-begin","title":"Before you begin","text":"<p>Before diving in, ensure you've either completed our introductory tutorials or have a robust understanding of Python.</p>"},{"location":"full_guides/overview/#guide-and-code-structure","title":"Guide and Code Structure","text":"<p>The structure of our guides and code may not always match. We've found that separating the two helps newcomers navigate Simmate without having to grasp all its components at once.</p>"},{"location":"full_guides/overview/#documentation","title":"Documentation","text":"<p>Our guides are arranged in ascending order of complexity, mirroring the typical user journey with Simmate. Users generally start with high-level features (the website interface) and progressively delve into lower-level features (the toolkit and Python objects). The documentation mirrors this progression.</p> <pre><code>graph LR\n  A[Website] --&gt; B[Workflows];\n  B --&gt; C[Database];\n  C --&gt; D[Toolkit];\n  D --&gt; E[Extras];</code></pre> <p>Tip</p> <p>Advanced topics are situated at the end of each section. Unlike the  getting-started guides, you do not need to complete a section before  proceeding to the next one.</p>"},{"location":"full_guides/overview/#python-modules","title":"Python Modules","text":"<p><code>simmate</code> is the base module, housing all the code that our package operates on. Each subfolder (or Python \"module\") provides detailed information about its contents.</p> <p>These modules include:</p> <ul> <li><code>apps</code>: Runs specific analyses or third-party programs (e.g., VASP, which performs DFT calculations)</li> <li><code>command_line</code>: Provides common functions as terminal commands</li> <li><code>configuration</code>: Contains default Simmate settings and methods for modifying them</li> <li><code>database</code>: Sets up data table structures and methods for accessing these tables</li> <li><code>engine</code>: Offers tools for running calculations and managing errors</li> <li><code>file_converters</code>: Includes methods for converting between file types (e.g., POSCAR to CIF)</li> <li><code>toolkit</code>: Houses core methods and classes for Simmate (e.g., the <code>Structure</code> class)</li> <li><code>utilities</code>: Contains simple functions used across other modules</li> <li><code>visualization</code>: Provides methods for visualizing structures and data</li> <li><code>website</code>: Powers the simmate.org website</li> <li><code>workflows</code>: Contains tools defining each calculation type (e.g., a structure optimization)</li> </ul> <p>Additionally, there's one extra file:</p> <ul> <li><code>conftest</code>: Runs Simmate tests and is intended solely for contributing developers</li> </ul>"},{"location":"full_guides/toolkit/","title":"Simmate Toolkit Overview","text":"<p>Danger</p> <p>Please note that many classes within this module are still under development. We recommend using pymatgen and ase for toolkit functionality until Simmate reaches v1.0.0. For developers, this implies that numerous classes are currently undocumented and untested. This approach enables us to expedite our testing process without the need to reformat tests and rewrite guides. We include this developmental code in our main branch because some higher-level functions (e.g., workflows) depend on these features. However, we ensure that these higher-level functions are thoroughly tested and documented to counterbalance the ongoing changes at the lower level.</p> <p>The toolkit module aims to extend pymatgen and ase. It encompasses low-level classes and functions, such as the <code>Structure</code> class and its associated methods. The toolkit module is entirely Python-based and does not rely on third-party DFT programs. For those, please refer to the <code>simmate.apps</code> module.</p> <p>The <code>Structure</code> and <code>Composition</code> classes are the most frequently used classes from this toolkit. You can import these classes using the following code:</p> <pre><code>from simmate.toolkit import Structure, Composition\n</code></pre>"},{"location":"full_guides/toolkit/#submodules-overview","title":"Submodules Overview","text":"<ul> <li><code>base_data_types</code>: Defines core classes for materials science, including <code>Structure</code> and <code>Composition</code> classes. For ease of use, these classes can be directly imported from <code>simmate.toolkit</code>, as shown above.</li> <li><code>creators</code>: Aids in the creation of structures, lattices, and periodic sites.</li> <li><code>featurizers</code>: Transforms properties into numerical descriptors for machine learning applications.</li> <li><code>structure_prediction</code>: Predicts crystal structures using an evolutionary algorithm.</li> <li><code>symmetry</code>: Provides tools and metadata for symmetry analyses, such as space groups and Wyckoff sites.</li> <li><code>transformations</code>: Offers methods to transform or \"mutate\" <code>Structures</code>.</li> <li><code>validators</code>: Provides methods to verify if a structure meets specific criteria.</li> </ul>"},{"location":"full_guides/database/basic_use/","title":"Basic Database Access","text":""},{"location":"full_guides/database/basic_use/#overview","title":"Overview","text":"<p>The process of accessing and analyzing data typically involves these steps:</p> <ol> <li>Establish a connection to your database</li> <li>Load a specific database table</li> <li>Apply filters to the data</li> <li>Convert the data to a desired format</li> <li>Modify the data using <code>simmate.toolkit</code> or pandas.Dataframe</li> </ol> <p>The following sections will guide you through each of these steps. Here's an example of what your final script might look like:</p> <pre><code># Connect to your database\nfrom simmate.database import connect\n\n# Load a specific database table\nfrom simmate.database.third_parties import MatprojStructure\n\n# Filter data\nresults = MatprojStructure.objects.filter(\n    nsites=3,\n    is_gap_direct=False,\n    spacegroup=166,\n).all()\n\n# Convert data to a desired format\nstructures = results.to_toolkit()\ndataframe = results.to_dataframe()\n\n# Modify data\nfor structure in structures:\n    # run your analysis/modifications here!\n</code></pre>"},{"location":"full_guides/database/basic_use/#connecting-to-your-database","title":"Connecting to Your Database","text":"<p>Before importing any submodules, you must configure Django settings for interactive use. Here's how to do it:</p> <pre><code># connect to the database\nfrom simmate.database import connect\n\n# now you can import tables in this module\nfrom simmate.database.workflow_results import MITStaticEnergy\n</code></pre> <p>If you forget the <code>connect</code> step, you'll encounter this error:</p> <pre><code>ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not\nconfigured. You must either define the environment variable DJANGO_SETTINGS_MODULE \nor call settings.configure() before accessing settings.\n</code></pre>"},{"location":"full_guides/database/basic_use/#loading-your-database-table","title":"Loading Your Database Table","text":"<p>The name of your table depends on the source you're accessing. To see the available sources (Materials Project, OQMD, Jarvis, COD), explore the contents of the database/third_parties module.</p> <p>For example, to load a table from the Materials Project, use: <pre><code>from simmate.database.third_parties import MatprojStructure\n</code></pre></p> <p>If you're accessing data from a specific workflow, you can access the table in two ways. Besides loading from the <code>workflow_results</code> module, most workflows have a <code>database_table</code> attribute that allows you to access the table:</p> <pre><code>########## METHOD 1 ########\n\nfrom simmate.workflows.static_energy import mit_workflow\n\ntable = mit_workflow.database_table\n\n\n######## METHOD 2 ########\n\nfrom simmate.database import connect\nfrom simmate.database.workflow_results import MITStaticEnergy\n\n# The line below shows that these tables are the same! Therefore, use\n# whichever method you prefer.\nassert table == MITStaticEnergy\n</code></pre>"},{"location":"full_guides/database/basic_use/#querying-and-filtering-data","title":"Querying and Filtering Data","text":"<p>Simmate uses Django's methods for querying a table, leveraging its Object-Relational Mapper (ORM) to make complex queries to our database. Below are some common queries. For a full list of query methods, refer to Django's query page.</p> <p>Access all rows of the database table via the <code>objects</code> attribute: <pre><code>MITStaticEnergy.objects.all()\n</code></pre></p> <p>Print all columns of the database table using the <code>show_columns</code> methods: <pre><code>MITStaticEnergy.show_columns()\n</code></pre></p> <p>Filter rows with exact-value matches in a column: <pre><code>MITStaticEnergy.objects.filter(\n    nsites=3,\n    is_gap_direct=False,\n    spacegroup=166,\n).all()\n</code></pre></p> <p>Filter rows based on conditions by chaining the column name with two underscores. Supported conditions are listed here, but the most commonly used ones are:</p> <ul> <li><code>contains</code> = contains text, case-sensitive query</li> <li><code>icontains</code>= contains text, case-insensitive query</li> <li><code>gt</code> = greater than</li> <li><code>gte</code> =  greater than or equal to</li> <li><code>lt</code> = less than</li> <li><code>lte</code> = less than or equal to</li> <li><code>range</code> = provides upper and lower bound of values</li> <li><code>isnull</code> = returns <code>True</code> if the entry does not exist</li> </ul> <p>Here's an example query with conditional filters: <pre><code>MITStaticEnergy.objects.filter(\n    nsites__gte=3,  # greater or equal to 3 sites\n    energy__isnull=False,  # the structure DOES have an energy\n    density__range=(1,5),  # density is between 1 and 5\n    elements__icontains='\"C\"',  # the structure includes the element Carbon\n    spacegroup__number=167,  # the spacegroup number is 167\n).all()\n</code></pre></p> <p>Note: For the filtering condition <code>elements__icontains</code>, we used quotations when querying for carbon: <code>'\"C\"'</code>. This is to avoid accidentally grabbing Ca, Cs, Ce, Cl, etc. This is necessary when using SQLite (the default database backend). If you're using Postgres, you can use the cleaner version <code>elements__contains=\"C\"</code>.</p>"},{"location":"full_guides/database/basic_use/#converting-data-to-desired-format","title":"Converting Data to Desired Format","text":"<p>By default, Django returns your query results as a <code>queryset</code> (or <code>SearchResults</code> in simmate), which is a list of database objects. It's often more useful to convert them to a pandas dataframe or to toolkit objects. <pre><code># Gives a pandas dataframe.\ndf = MITStaticEnergy.objects.filter(...).to_dataframe()\n\n# Gives a list of toolkit Structure objects\ndf = MITStaticEnergy.objects.filter(...).to_toolkit()\n\n# '...' are the set of filters selected from above.\n</code></pre></p>"},{"location":"full_guides/database/basic_use/#modifying-data","title":"Modifying Data","text":"<p>For information on how to modify and analyze data, refer to the pandas and <code>simmate.toolkit</code> documentation.</p>"},{"location":"full_guides/database/contributing_data/","title":"Contributing data","text":""},{"location":"full_guides/database/contributing_data/#contributing-your-data-with-simmate","title":"Contributing Your Data with Simmate","text":"<p>Warning</p> <p>This module is intended for the Simmate development team or third-party contributors who wish to integrate their own data. Users should use the <code>load_remote_archive</code> method to access data. For more information, refer to the database documentation.</p> <p>This module enables the import of data from various databases into Simmate using third-party codes. This data can then be used to build archives that users can access.</p>"},{"location":"full_guides/database/contributing_data/#benefits-of-integrating-your-data-with-simmate","title":"Benefits of Integrating Your Data with Simmate","text":"<p>When deciding whether to contribute your database to Simmate, consider the following:</p> <ol> <li>Will you benefit from converting data into a Simmate format?</li> <li>Will you benefit from distributing an archive? (private or public)</li> </ol> <p>The following sections will provide answers to these questions.</p>"},{"location":"full_guides/database/contributing_data/#converting-data-into-a-simmate-format","title":"Converting Data into a Simmate Format","text":"<p>Whether your data is open-source or proprietary, the benefits of using Simmate's <code>database</code> module remain the same. It...</p> <ul> <li>automatically constructs an API and ORM for your data</li> <li>significantly reduces the file size of your archives</li> </ul> <p>By providing raw data (such as a structure or energy), Simmate will automatically expand your data into the most useful columns, and you can then use our ORM to quickly query data. For example, Simmate can use an <code>energy</code> column/field to create columns for <code>energy_above_hull</code>, <code>formation_energy</code>, <code>decomposes_to</code>, and more -- then you can filter through your data using these new columns. Refer to the \"Querying Data\" section in the <code>simmate.database</code> module for examples of this query language.</p> <p>Simmate can efficiently compress your data to a small format by using the concepts of \"raw data\" vs \"secondary columns\" (columns that can be quickly recalculated using the raw data). To see just how compact, check out the file sizes for archives of current providers:</p> Provider Number of Structures Av. Sites per Structure Archive Size JARVIS 55,712 ~10 8.0 MB Materials Project 137,885 ~30 45.2 MB COD 471,664 ~248 1.16 GB OQMD 1,013,521 ~7 79.2 MB AFLOW n/a n/a n/a <p>(Note, COD experiences poor compression because Simmate has not yet optimized storage for disordered structures.)</p> <p>These small file sizes will make it much easier for downloading and sharing your data. This can have major savings on your database server as well.</p>"},{"location":"full_guides/database/contributing_data/#hosting-distributing-the-archive","title":"Hosting &amp; Distributing the Archive","text":"<p>Here is where being a private vs. open-source provider becomes important. Simmate allows you to decide how others access your data.</p> <p>If your data can only be accessible to your own team members or subscribers, then you can manage the distribution of the data (via a CDN, dropbox, etc.). Simmate does not require that you distribute your data freely, although we do encourage open-source data. Either way, you can benefit from...</p> <ul> <li>reducing the load on your own web APIs</li> </ul> <p>Server load can be reduced because, in Simmate, users download your archive once and then have the data stored locally for as long as they'd like. New users often want to download a large portion of a database (or all of it), and also do so repeatedly as they learn about APIs. Therefore, using Simmate archives upfront can save your team from these large and often-repeated queries.</p> <p>If you are fine with making your data freely available, you can further benefit by...</p> <ul> <li>avoiding the setup of your own server and instead use Simmate's for free</li> <li>exposing your data to the Simmate user base</li> </ul> <p>Providers that permit redistribution are welcome to use our CDN for their archives. This only requires contacting our team and making this request. Further, once your archive is configured, all Simmate users will be able to easily access your data.</p>"},{"location":"full_guides/database/contributing_data/#how-to-integrate-your-data-or-a-new-provider","title":"How to Integrate Your Data or a New Provider","text":"<p>Tip</p> <p>If you want to avoid this guide, you can just contact our team! Open a github issue to get our attention. In most cases, we only need a CSV or JSON file of your data (in any data format you'd like), and we can handle the rest for you. If you'd like to contribute the data on your own, keep reading.</p> <p>The end goal for each provider is to allow a user do the following: <pre><code>from simmate.database import connect\nfrom simmate.database.third_parties import ExampleProviderData\n\nExampleProviderData.load_remote_archive()\n\nsearch_results = ExampleProviderData.objects.filter(...).all()\n# plus all to_dataframe / to_toolkit features discussed elsewhere\n</code></pre></p> <p>The key part that providers must understand is the <code>load_remote_archive</code> method. This method...</p> <ol> <li>loads an archive of available data (as a <code>zip</code> file from a CDN)</li> <li>unpacks the data into the Simmate format</li> <li>saves everything to the user's database (by default this is <code>~/simmate/database.sqlite3</code>).</li> </ol> <p>This guide serves to make the first step work! Specifically, providers must make the archive that <code>load_remote_archive</code> will load in step 1 and make it downloadable by a CDN or API endpoint. It is up to the provider whether they personally distribute the archive or allow Simmate to distribute it for them.</p>"},{"location":"full_guides/database/contributing_data/#outline-of-steps","title":"Outline of Steps","text":"<p>To illustrate how this is done, we will walk through the required steps:</p> <ol> <li>Define a Simmate table</li> <li>Download data into the Simmate format (i.e. populate the Simmate table)</li> <li>Compress the data to archive file</li> <li>Make the archive available via a CDN</li> <li>Link the CDN to the Simmate table</li> </ol> <p>Tip</p> <p>These steps involve contributing changes to Simmate's code, so we recommend opening a github issue before starting too. That way, our team can help you through this process. If you are new to Github and contributing, be sure to read our tutorial for contributors too.</p>"},{"location":"full_guides/database/contributing_data/#step-1-define-a-simmate-table","title":"Step 1: Define a Simmate Table","text":"<p>To host data, Simmate must first know what kind of data you are going to host. We do this by adding a new file to the <code>simmate.database.third_party</code> module. You can view this folder on github here.</p> <p>Start by defining a <code>DatabaseTable</code> with any custom columns / database mix-ins. You can scroll through the other providers to see how tables are made. Good examples to view are for JARVIS and Materials Project.</p> <p>Here is a template with useful comments to get you started:</p> <pre><code># Start by deciding which base data types you can include. Here, we include a\n# crystal structure and an energy, so we use the Structure and Thermodynamics\n# mix-ins.\nfrom simmate.database.base_data_types import (\n    table_column, \n    Structure, \n    Thermodynamics,\n)\n\nclass ExampleProviderData(Structure, Thermodynamics):\n\n    # ----- Table columns + Required settings -----\n\n    # This Meta class tells Simmate where to store the table within our database.\n    # All providers with have the exact same thing here. \n    class Meta:\n        app_label = \"data_explorer\"\n\n    # By default, the ID column is an IntegerField, but if your data uses a string\n    # like \"mp-1234\" to denote structures, you can update this column to\n    # accept a string instead.\n    id = table_column.CharField(max_length=25, primary_key=True)\n\n    # Write the name of your team here!\n    source = \"The Example Provider Project\"\n\n    # We have many alerts to let users know they should cite you. Add the DOI\n    # that you'd like them to cite here.\n    source_doi = \"https://doi.org/...\"\n\n    # If you have any custom fields that you'd like to add, list them off here.\n    # All data types supported by Django are also supported by Simmate. You can\n    # view those options here:\n    #   https://docs.djangoproject.com/en/4.0/ref/models/fields/\n    custom_column_01 = table_column.FloatField(blank=True, null=True)\n    custom_column_02 = table_column.BooleanField(blank=True, null=True)\n\n    # Leave this as None for now. We will update this attribute in a later step.\n    remote_archive_link = None\n\n\n    # ----- Extra optionalfeatures -----\n\n    # (OPTIONAL) Define the \"raw data\" for your table. This is required if \n    # you'd like to use the `to_archive` method. Fields from the mix-in \n    # will automatically be added.\n    archive_fields = [\n        \"custom_column_01\",\n        \"custom_column_02\",\n    ]\n\n    # (OPTIONAL) Define how you would like data to be accessible in the REST \n    # API from the website server.\n    api_filters = {\n        \"custom_column_01\": [\"range\"],\n        \"custom_column_02\": [\"range\"],\n    }\n\n    # (OPTIONAL) if you host your data on a separate website, you can specify \n    # how to access that structure here. This is important if you want users\n    # to switch to your site for aquiring additional data. \n    @property\n    def external_link(self) -&gt; str:\n        return f\"https://www.exampleprovider.com/structure/{self.id}\"\n</code></pre> <p>Before moving on, make sure your table was configured properly by doing the following:</p> <pre><code># in the command line\nsimmate database reset\n</code></pre> <pre><code># in python\n\nfrom simmate.database import connect\nfrom simmate.database.third_parties import ExampleProviderData\n\n# This will show you all the columns for your table\nExampleProviderData.show_columns()\n\n# this will show you exactly what the table looks like\nmy_table = ExampleProviderData.objects.to_dataframe()\n</code></pre>"},{"location":"full_guides/database/contributing_data/#step-2-download-data-into-the-simmate-format","title":"Step 2: Download Data into the Simmate Format","text":"<p>Now that Simmate knows what to expect, we can load your data into the database. This can be done in several ways. It is entirely up to you which method to use, but here are our recommended options:</p> <ol> <li> <p>JSON or CSV file. If all of your data can be provided via a dump file, then we can use that! This is typically the easiest for a provider's server. For an example of this, see the COD implementation, which uses a download of CIF files.</p> </li> <li> <p>A custom python package. Feel free to add an optional dependency if your team has already put a lot of work into loading data using a python package. A great example of this is the MPRester class in pymatgen, which we use to pull Material Project data. (JARVIS, AFLOW, OQMD currently use this option too).</p> </li> <li> <p>REST API or GraphQL. If you have a web API, we can easily pull data using the python <code>requests</code> package. Note, in many cases, a REST API is an inefficient way to pull data - as it involves querying a database thousands of times (once for each page of structures) -- potentially crashing the server. In cases like that, we actually prefer a download file (option 1, shown above).</p> </li> <li> <p>OPTIMADE endpoint. This is a standardized REST API endpoint that many databases are using now. The huge upside here is that each database will have a matching API -- so once your team has an OPTIMADE endpoint, we can pull data into Simmate with ease. There's no need to build a 2nd implementation. The downside is the same as option 3: OPTIMADE doesn't have a good way to pull data in bulk. Their team is currently working on this though.</p> </li> <li> <p>Web scraping. As an absolute last resort, we can use <code>requests</code> to scrape webpages for data (or <code>selenium</code> in even more extreme cases.). This requires the most work from our team and is also the least efficient way to grab data. Therefore, scraping should always be avoided if possible.</p> </li> </ol> <p>With your data in hand, you will now add a file that saves data to the local simmate database on your computer. This file can be added to the <code>for_providers</code> module (here). However, if you want your data and its access to remain private, you can also keep this file out of Simmate's source-code. It's up to you, but we encourage providers to host their file in the Simmate repo -- so we can give feedback and so future providers can use it as an example/guide. </p> <p>Either way, here is a template of how that file will look like:</p> <pre><code>from django.db import transaction\nfrom rich.progress import track\n\nfrom simmate.toolkit import Structure\nfrom simmate.database.third_parties import ExampleProviderData\n\n# If you want to use a custom package to load your data, be sure to let our team\n# know how to install it.\ntry:\n    from my_package.db import get_my_data\nexcept:\n    raise ModuleNotFoundError(\n        \"You must install my_package with `conda install -c conda-forge my_package`\"\n    )\n\n\n# We make this an \"atomic transaction\", which means if any error is encountered\n# while saving results to the database, then the database will be reset to its\n# original state. Adding this decorator is optional\n@transaction.atomic\ndef load_all_structures():\n\n    # Use whichever method you chose above to load all of your data!\n    # Here' we are pretending to use a function that loads all data into a \n    # python dictionary, but this can vary.\n    data = get_my_data()\n\n    # Now iterate through all the data -- which is a list of dictionaries.\n    # We use rich.progress.track to monitor progress.\n    for entry in track(data):\n\n        # The structure is in the atoms field as a dictionary. We pull this data\n        # out and convert it to a toolkit Structure object. Note, this class\n        # is currently a subclass of pymatgen.Structure, so it supports reading\n        # from different file formats (like CIF or POSCAR) as well.\n        structure = Structure(\n            lattice=entry[\"atoms\"][\"lattice_mat\"],\n            species=entry[\"atoms\"][\"elements\"],\n            coords=entry[\"atoms\"][\"coords\"],\n            coords_are_cartesian=entry[\"atoms\"][\"cartesian\"],\n        )\n\n        # Now that we have a structure object, we can feed that and all\n        # other data to the from_toolkit() method. This will create a database\n        # object in the Simmate format. Note the data we pass here is based on\n        # the ExampleProviderData we defined in the other file.\n        structure_db = ExampleProviderData.from_toolkit(\n            id=entry[\"my_id\"],\n            structure=structure,  # required by Structure mix-in\n            energy=entry[\"my_final_energy\"],  # required by Thermodynamics mix-in\n            custom_column_01=entry[\"my_custom_column_01\"],\n            # The get method is useful if not all entries have a given field.\n            custom_column_02=entry.get(\"my_custom_column_02\"),\n        )\n\n        # and save it to our database!\n        structure_db.save()\n</code></pre> <p>Try running this on your dataset (or a subset of data if you want to quickly test things). When it finishes, you can ensure data was loaded properly by running:</p> <pre><code># in python\n\nfrom simmate.database import connect\nfrom simmate.database.third_parties import ExampleProviderData\n\n# Check that the number of rows matches your source data.\ntotal_entries = ExampleProviderData.objects.count()\n\n# View the data!\n# The [:100] limits this to your first 100 results\nmy_table = ExampleProviderData.objects.to_dataframe()[:100]\n</code></pre> <p>And that's it for writing new code! All that's left is making your data available for others.</p>"},{"location":"full_guides/database/contributing_data/#step-3-compress-the-data-to-archive-file","title":"Step 3: Compress the Data to Archive File","text":"<p>This will be the easiest step yet. We need to make a <code>zip</code> file for users to download, which can be done in one line:</p> <pre><code>ExampleProviderData.objects.to_archive()\n</code></pre> <p>You'll find a file named <code>ExampleProviderData-2022-01-25.zip</code> (but with the current date) in your working directory. The date is for timestamp and versioning your archives. Because archives are a snapshot of databases that may be dynamically changing/going, this timestamp helps users know which version they are on. You can practice reloading this data into your database too:</p> <ol> <li>Make a copy of your database file in <code>~/simmate/</code> so you don't lose your work</li> <li>In the terminal, reset your database with <code>simmate database reset</code></li> <li>In python, try reloading your data with <code>ExampleProviderData.load_archive()</code></li> <li>Try viewing your data again with <code>ExampleProviderData.objects.to_dataframe()</code></li> </ol>"},{"location":"full_guides/database/contributing_data/#step-4-make-the-archive-available-via-a-cdn","title":"Step 4: Make the Archive Available via a CDN","text":"<p>Users will now need the archive file you made to access your data. So you must decide: how should this <code>zip</code> file be downloaded by users? </p> <p>If you give Simmate approval, we can host your archive file on our own servers. Otherwise you must host your own. The only requirement for your host server is that the <code>zip</code> file can be downloaded from a URL.</p> <p>While we encourage open-source databases, if you consider your dataset private or commercial, Simmate does not require any payment or involvement for how this CDN is hosted and maintained. Thus, you can manage access to this URL via a subscription or any other method. However, Simmate's CDNs are reserved for archives that are freely distributed.</p> <p>Note: when uploading new versions of your archive, you should keep the outdated archive either available via its previous URL or, at a minimum, available upon request from users.</p>"},{"location":"full_guides/database/contributing_data/#step-5-link-the-cdn-to-the-simmate-table","title":"Step 5: Link the CDN to the Simmate Table","text":"<p>In Step 1, we left one attribute as None in our code: <code>remote_archive_link</code>. As a final step, you need to take the URL that you're host your <code>zip</code> file at and paste it here. For example, that line will become:</p> <pre><code>remote_archive_link = \"https://archives.simmate.org/ExampleProviderData-2022-01-25.zip\"\n</code></pre> <p>That's it! Let's test out everything again. Note, we are now using <code>load_remote_archive</code> in this process -- which will load your <code>zip</code> file from the URL.</p> <ol> <li>Make a copy of your database file in <code>~/simmate/</code> so you don't lose your work</li> <li>In the terminal, reset your database with <code>simmate database reset</code></li> <li>In python, try reloading your data with <code>ExampleProviderData.load_remote_archive()</code></li> <li>Try viewing your data again with <code>ExampleProviderData.objects.to_dataframe()</code></li> </ol> <p>If you've made it this far, thank you for contributing!!! Your data is now easily accessible to all Simmate users, which we hope facilitates its use and even lessen the load on your own servers. Congrats!</p>"},{"location":"full_guides/database/custom_tables/","title":"Custom Database Tables Creation","text":"<p>This module offers key components for data storage. When creating new and custom tables, these classes should be inherited.</p>"},{"location":"full_guides/database/custom_tables/#table-types","title":"Table Types","text":"<p>Data storage tables range from low-level to high-level. High-level tables inherit basic functionality from low-level tables and the data types stored in them, resulting in tables with enhanced functionality.  </p> <p>At the lowest level...</p> <ul> <li><code>base.DatabaseTable</code> : All tables inherit from this base type, which defines common functionality (like the <code>show_columns</code> method)</li> </ul> <p>At a higher level, tables inherit the <code>base.DatabaseTable</code> to create more specialized tables. These tables contain additional columns specific to the data stored in each. The new columns in each table are created using a feature called a mixin.  These mixins create the following tables:</p> <ul> <li><code>calculation</code> : Stores information about a specific calculation run (corrections, timestamps, etc.)</li> <li><code>structure</code> : Stores a periodic crystal structure</li> <li><code>symmetry</code> : NOT a mixin. Defines symmetry relationships for <code>structure</code> to reference</li> <li><code>forces</code> : Stores site forces and lattice stresses</li> <li><code>thermodynamics</code> : Stores energy and stability information</li> <li><code>density_of_states</code>: Stores results of a density of states calculation</li> <li><code>band_structure</code>: Stores results of a band structure calculation</li> </ul> <p>At the highest level, several lower-level tables can be combined via their mixins. This allows for the creation of tables that can store complex calculations:</p> <ul> <li><code>static_energy</code> : Stores results of single point energy calculation</li> <li><code>relaxation</code> : Stores all steps of a structure geometry optimization</li> <li><code>nudged_elastic_band</code> : Stores all results from trajectory calculations</li> <li><code>dynamics</code> : Stores all steps of a molecular dynamics simulation</li> <li><code>calculation_nested</code> : A special type of calculation that involves running a workflow made of smaller workflows</li> </ul>"},{"location":"full_guides/database/custom_tables/#custom-table-creation","title":"Custom Table Creation","text":"<p>To create a custom table, follow these steps:</p> <ol> <li>Define the lower-level tables and data types used to create your new table</li> <li>Register the new table to your database</li> <li>Save data to your new table</li> </ol> <p>All classes in this module are abstract and are primarily used as mix-ins. Each class will contain details on its specific use, but when combining multiple types, you can do the following:</p> <pre><code>from simmate.database.base_data_types import (\n    table_column,\n    Structure,\n    Thermodynamics,\n)\n\n# Inherit from all the types you'd like to store data on. All of the columns\n# defined in each of these types will be incorporated into your table.\nclass MyCustomTable(Structure, Thermodynamics):\n\n    # ----- Table columns -----\n\n    # Add any custom columns you'd like.\n    # These follow the types supported by Django.\n\n    # This custom field will be required and must be supplied at creation\n    custom_column_01 = table_column.IntegerField()\n\n    # This column we say is allowed to be empty. This is often needed if you\n    # create an entry to start a calculation and then fill in data after a\n    # it completes.\n    custom_column_02 = table_column.FloatField(null=True, blank=True)\n\n    # ----- Extra features -----\n\n    # If you are not using the `Calculation` mix-in, you'll have to specify\n    # which app this table is associated with. To determine what you set here,\n    # you should have completed the advanced simmate tutorials (08-09).\n    class Meta:\n        app_label = \"my_custom_app\"\n\n    # (OPTIONAL) Define the \"raw data\" for your table. This is required if \n    # you'd like to use the `to_archive` method. Fields from the mix-in \n    # will automatically be added.\n    archive_fields = [\n        \"custom_column_01\",\n        \"custom_column_02\",\n    ]\n\n    # (OPTIONAL) Define how you would like data to be accessible in the REST \n    # API from the website server.\n    api_filters = {\n        \"custom_column_01\": [\"range\"],\n        \"custom_column_02\": [\"range\"],\n    }\n</code></pre> <p>Warning</p> <p>Unless you are contributing to Simmate's source code, defining a new table does NOT automatically register it to your database. To do this, you must follow along with our custom workflows guides.</p>"},{"location":"full_guides/database/custom_tables/#data-loading","title":"Data Loading","text":"<p>Once your table is created and registered, you can use the <code>from_toolkit</code> method to create and save your data to the database. Note, the information you pass to this method is entirely dependent on what you inherit from and define above.</p> <pre><code>from my.example.project import MyCustomTable\n\nnew_row = MyCustomTable.from_toolkit(\n    # Because we inherited from Structure, we must provide structure\n    structure=new_structure,  // provide a ToolkitStructure here\n    # \n    # All tables can optionally include a source too.\n    source=\"made by jacksund\",\n    #\n    # Because we inherited from Thermodynamics, we must provide energy\n    energy=-5.432,\n    #\n    # Our custom fields can also be added\n    custom_column_01=1234,\n    custom_column_02=3.14159,\n)\n\nnew_row.save()\n</code></pre>"},{"location":"full_guides/database/custom_tables/#column-updating","title":"Column Updating","text":"<p>To modify a row, you can load it from your database, update the column, and then resave. Note, there are many more ways to do this, so consult the Django documentation for advanced usage.</p> <pre><code>from my.example.project import MyCustomTable\n\nmy_row = MyCustomTable.objects.get(id=1)\n\nmy_row.custom_column_01 = 4321\n\nmy_row.save()\n</code></pre>"},{"location":"full_guides/database/notes/","title":"Notes","text":"<p>&lt;&lt; _register_calc &gt;&gt; from_run_context from_toolkit</p> <p>&lt;&lt; _update_database_with_results &gt;&gt;  This function retrieves from _register_calc and includes the following methods: - update_database_from_results - update_from_results     - update_from_toolkit         - from_toolkit(as_dict=True)     - update_from_directory         - from_directory(as_dict=True)             - from_vasp_directory(as_dict=True) ---&gt; Note: Unexpected as_dict                 - from_vasp_run(as_dict=True)         - update_from_toolkit()             - from_toolkit(as_dict=True)</p> <p>&lt;&lt; load_completed_calc &gt;&gt; This function includes the following methods: - from_toolkit - from_directory     - from_vasp_directory         - from_vasp_run</p>"},{"location":"full_guides/database/overview/","title":"Simmate Database Module","text":"<p>This module provides tools for defining and managing your database.</p> <p>If you're new to this, make sure you've gone through our database tutorial.</p> <p>The module includes the following submodules:</p> <ul> <li><code>base_data_types</code> : Fundamental mix-ins for creating new tables</li> <li><code>workflow_results</code> : A set of result tables for <code>simmate.workflows</code></li> <li><code>prototypes</code> : Tables that hold prototype structures</li> <li><code>third_parties</code> : Tables derived from external sources (like Materials Project)</li> </ul> <p>In addition, this module includes an extra file:</p> <ul> <li><code>connect</code>: This file sets up the database and installed apps (in other words, it configures Django)</li> </ul>"},{"location":"full_guides/database/third_party_data/","title":"Accessing Third-party Data","text":"<p>This module simplifies the process of downloading data from external providers and storing it in your local database.</p> <p>Please be aware that this data is NOT provided by the Simmate team. The providers are separate entities and should be properly credited. All data is subject to the respective provider's terms and conditions.</p> <p>We currently support data from these providers:</p> <ul> <li> COD (Crystallography Open Database)</li> <li> JARVIS (Joint Automated Repository for Various Integrated Simulations)</li> <li> Materials Project</li> <li> OQMD (Open Quantum Materials Database)</li> </ul> <p>We have also configured the following provider, but are still awaiting permission to redistribute their data:</p> <ul> <li> AFLOW (Automatic FLOW for Materials Discovery)</li> </ul> <p>Tip</p> <p>If you're interested in making your data accessible via Simmate, please refer to the Contributing data module. We welcome contributions of any size! The <code>for_providers</code> module provides more information on the benefits of contributing and how to package your data.</p>"},{"location":"full_guides/database/third_party_data/#downloading-data","title":"Downloading Data","text":"<p>Before proceeding, make sure you've completed our introductory tutorial on downloading data from these providers. We use <code>MatprojStructure</code> as an example, but the same procedure applies to all other tables in this module.</p> <p>WARNING: The initial loading of the data archive can be time-consuming. We suggest running this process overnight. Once completed, we recommend backing up your database by duplicating your ~/simmate/my_env-database.sqlite3 file to avoid repeating this lengthy process.</p> <p>To download all data into your database:</p> <pre><code>simmate database load-remote-archives\n</code></pre> <p>Or in Python, you can download a specific table:</p> <pre><code>from simmate.database.third_parties import MatprojStructure\n\n# This process can take &gt;1 hour for some providers. You can\n# add `parallel=True` to expedite this process, but exercise caution when \n# parallelizing with SQLite (the default backend). We recommend \n# avoiding the use of parallel=True, and instead running\n# this line overnight.\nMatprojStructure.load_remote_archive()\n\n# Remember to cite the provider if you use their data!\nMatprojStructure.source_doi\n</code></pre>"},{"location":"full_guides/database/third_party_data/#updating-energy-fields","title":"Updating Energy Fields","text":"<p>Some database providers offer calculated energy, which can be used to update stability information:</p> <pre><code># updates ALL chemical systems.\n# This process can take over an hour for some providers. Consider running \n# this overnight along with your call to load_remote_archive.\nMatprojStructure.update_all_stabilities()\n\n# updates ONE chemical system\n# Use this if you need to quickly update a specific system\nMatprojStructure.update_chemical_system_stabilities(\"Y-C-F\")\n</code></pre>"},{"location":"full_guides/database/third_party_data/#alternative-options","title":"Alternative Options","text":"<p>This module can be used as an alternative or in addition to the following codes:</p> <ul> <li>MPContribs</li> <li>matminer.data_retrieval</li> <li>pymatgen.ext</li> <li>OPTIMADE APIs</li> </ul> <p>Unlike alternatives that query external APIs and load data into memory, this module stores data locally for quick loading in future Python sessions. This method ensures data stability (i.e., no unexpected changes in your source data) and fast loading, which is especially useful for high-throughput studies.</p>"},{"location":"full_guides/database/workflow_data/","title":"Accessing Workflow Data","text":"<p>This module, akin to the <code>simmate.workflows</code> module, organizes all database tables related to workflows and groups them by application for convenient access.</p>"},{"location":"full_guides/database/workflow_data/#retrieving-results","title":"Retrieving Results","text":"<p>The beginner tutorials offer guidance on running workflows and fetching their results. Here's a brief summary:</p> <pre><code>from simmate.workflows.static_energy import mit_workflow\n\n# Runs the workflow and returns a status\nstatus = mit_workflow.run(structure=...)\n\n# Gives the DatabaseTable where ALL results are stored\nmit_workflow.database_table\n</code></pre> <p>You can also directly connect to a table like this...</p> <pre><code># Sets up connection to the database\nfrom simmate.database import connect\n\nfrom simmate.database.workflow_results import MITStaticEnergy\n\n# NOTE: MITStaticEnergy here is the same as database_table in the previous codeblock.\n# These are just two different ways of accessing it.\nMITStaticEnergy.objects.filter(...)\n</code></pre>"},{"location":"full_guides/extras/command_line/","title":"The Simmate Command-line Interface","text":"<p>This module introduces the <code>simmate</code> command and its related sub-commands.</p> <p>It's crucial to understand that most commands in this module serve as wrappers for more fundamental functions, which explains the minimal code here. For example, the <code>simmate database reset</code> command is merely a wrapper for the Python code below:</p> <pre><code>from simmate.database import connect\nfrom simmate.database.utilities import reset_database\n\nreset_database()\n</code></pre> <p>We've built our command-line using Click instead of Argparse. We recommend familiarizing yourself with Click's documentation before contributing to this module.</p> <p>WARNING: We recommend using the command-line directly for interaction and usage, rather than referring to the online documentation provided here. This recommendation stems from our use of typer, which provides a more efficient overview of options compared to the API docs shown here.</p>"},{"location":"full_guides/extras/utilities/","title":"Utilities","text":"<p>This module comprises shared functions used throughout Simmate. These functions execute a range of tasks, including fetching the name of the active conda environment, creating a new directory, and compressing a folder into a zip file.</p> <p>Typically, you won't need to interact with the functions in this module unless you're developing new features or contributing to our codebase.</p>"},{"location":"full_guides/extras/visualization/","title":"Visualization","text":""},{"location":"full_guides/extras/visualization/#simmate-visualization","title":"Simmate Visualization","text":"<p>This module is specifically designed to create plots and 3D models for data and structure visualization. It functions as an extension of the <code>pymatgen.ext</code> module.</p> <p>NOTE: Currently, this module solely produces 3D models for <code>Structure</code> objects utilizing Blender. We are actively planning and developing additional features.</p> <p>As we work on improving these tools, we recommend using either VESTA or OVITO for crystal structure visualization.</p>"},{"location":"full_guides/extras/contributing/creating_and_submitting_changes/","title":"Changes Submission Guide","text":"<ol> <li> <p>Prior to making any changes, please engage with our team to discuss your proposed alterations. This ensures everyone is informed about ongoing work and allows for constructive feedback. You can submit feature requests, bug reports, and other issues on our Issues page. For general ideas and queries, please use our Discussions page.</p> </li> <li> <p>Ensure your fork is up-to-date with our main code at <code>jacksund/simmate</code>. This is essential if it's been a while since your last update. If you've just finished the previous section, you're good to go.</p> </li> <li> <p>Access the Simmate directory in Spyder, as detailed in step 8 of the previous section.</p> </li> <li> <p>Open the file you wish to edit in Spyder and make your changes.</p> </li> <li> <p>Don't forget to save all file changes in Spyder.</p> </li> <li> <p>Simmate demands clean, readable code. We utilize the <code>black</code> formatter and isort for managing imports. Execute these commands in the <code>~/Documents/github/simmate</code> directory: <pre><code>isort .\nblack .\n</code></pre></p> </li> <li> <p>Simmate employs pytest test cases to ensure new changes don't interfere with existing features. Run these tests to validate your changes. Execute this command in the <code>~/Documents/github/simmate</code> directory: <pre><code># you can optionally run tests in parallel \n# with a command such as \"pytest -n 4\"\npytest\n</code></pre></p> </li> <li> <p>If all tests are successful, your changes are ready for submission to Simmate!</p> </li> <li> <p>Use GitKraken to review your changes. If the changes are satisfactory, <code>stage</code> and <code>commit</code> them to the <code>main</code> branch of your repo (<code>yourname/simmate</code>). You can add an emoji to your commit message to show you've read the tutorial.</p> </li> <li> <p>Open a pull-request to merge your changes into our main code (<code>jacksund/simmate</code>). We'll review your changes and merge them if they meet our standards.</p> </li> </ol> <p>Tip</p> <p>You can also format files while coding with Spyder. Go to <code>Tools</code> -&gt; <code>Preferences</code> -&gt; <code>Completion and Linting</code> -&gt; <code>Code Style and Formatting</code> &gt; select <code>black</code> from the code formatting dropdown. To format an open file in Spyder, use the <code>Ctrl+Shift+I</code> shortcut.</p> <p>Note</p> <p>Currently, Spyder does not have a plugin for pytest. We're eagerly awaiting the development of this feature along with their Unittest plugin.</p>"},{"location":"full_guides/extras/contributing/extra/","title":"Additional Guidelines and Suggestions","text":""},{"location":"full_guides/extras/contributing/extra/#source-code-search","title":"Source Code Search","text":"<p>When you've made significant changes to a method, you might need to find all its instances in Simmate. You can use Spyder's <code>Find</code> window for this. Here's how to set it up:</p> <ol> <li>In Spyder, go to the <code>View</code> tab (at the top of the window) &gt; <code>Panes</code> &gt; select <code>Find</code>.</li> <li>The <code>Find</code> option should now appear in the top-right window of Spyder, alongside your <code>Help</code> window and <code>Variable Explorer</code>.</li> <li>In the <code>Find</code> window, set <code>Exclude</code> to the following to prevent these files from being searched: <pre><code>*.csv, *.dat, *.log, *.tmp, *.bak, *.orig, *.egg-info, *.svg, *.xml, OUTCAR, *.js, *.html\n</code></pre></li> <li>Set <code>Search in</code> to the <code>src/simmate</code> directory to limit the search to source code.</li> </ol>"},{"location":"full_guides/extras/contributing/extra/#command-line-git","title":"Command-Line Git","text":"<p>While we suggest using GitKraken, there might be times when you need to use the git command-line. Github offers detailed guides for this, but we've summarized the essentials here.</p> <p>To set up 2-factor-auth, follow these steps (according to these instructions):</p> <pre><code>1. Go to Profile &gt;&gt; Settings &gt;&gt; Account Security\n2. Click on \"Enable two-factor\" authentication\n3. Follow the prompts to finish setup (I used SMS and saved my codes to BitWarden)\n</code></pre> <p>To create your API token, follow these steps (according to these instructions):</p> <pre><code>1. Go to Profile &gt;&gt; Settings &gt;&gt; Developer Settings &gt;&gt; Personal Access tokens\n2. Generate a new token for 90 days with the \"repo\" scope and \"read:org\"\n3. Use this token as your password when running git commands\n</code></pre> <p>To set up permissions with git on the command-line, follow these steps (using this guide):</p> <pre><code>1. Make sure the GitHub CLI is installed (`conda install -c conda-forge gh`)\n2. Run `gh auth login` and follow the prompts to enter your personal token from above\n</code></pre> <p>Here are some frequently used commands... <pre><code># To clone a remote directory to your local disk\ngit clone &lt;GITHUB-URL&gt;\n\n# To pull a specific branch (main here) while in a git directory\ngit pull origin main\n\n# To discard all changes and reset your branch\ngit restore .\n</code></pre></p>"},{"location":"full_guides/extras/contributing/first_time_setup/","title":"Getting Started","text":"<p>Tip</p> <p>We recommend students and teachers to use their Github accounts with Github's free Student/Teacher packages. This includes Github Pro and other beneficial software. However, this step is not mandatory.</p> <ol> <li> <p>Fork the Simmate repository to your Github profile (e.g., <code>yourname/simmate</code>).</p> </li> <li> <p>Clone <code>yourname/simmate</code> to your local desktop. We recommend using GitKraken and cloning to a folder named <code>~/Documents/github/</code>. GitKraken is free for public repositories (including Simmate), but is also part of Github's free Student/Teacher packages. Their 6-minute beginner video provides a quick start guide.</p> </li> <li> <p>Navigate to the cloned Simmate repository: <pre><code>cd ~/Documents/github/simmate\n</code></pre></p> </li> <li> <p>Create your conda environment using our conda file. This will install Spyder and name your new environment <code>simmate_dev</code>. We highly recommend using Spyder as your IDE for consistency with the rest of the team. <pre><code>conda env update -f envs/conda/dev.yaml\nconda install -n simmate_dev -c conda-forge spyder -y\nconda activate simmate_dev\n</code></pre></p> </li> <li> <p>Install Simmate in development mode to your <code>simmate_dev</code> environment. <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>When resetting your database, refrain from using the prebuilt database. Pre-builts are only created for new releases, and the development database may differ from the latest release. <pre><code>simmate database reset --confirm-delete --no-use-prebuilt\n</code></pre></p> </li> <li> <p>Confirm everything is functioning correctly by running our tests <pre><code># you can optionally run tests in parallel \n# with a command such as \"pytest -n 4\"\npytest\n</code></pre></p> </li> <li> <p>In GitKraken, make sure you have the <code>main</code> branch of your repository (<code>yourname/simmate</code>) checked out.</p> </li> <li> <p>In Spyder, navigate to <code>Projects</code> &gt; <code>New Project...</code>. Select <code>existing directory</code>, choose your <code>~/Documents/github/simmate</code> directory, and then <code>create</code> your Project!</p> </li> <li> <p>You're now set to explore the source code and modify or add files! Continue to the next section for guidance on formatting, testing, and submitting your changes to our team.</p> </li> </ol>"},{"location":"full_guides/extras/contributing/maintainer_notes/","title":"Maintainer Guidelines","text":""},{"location":"full_guides/extras/contributing/maintainer_notes/#release-procedure","title":"Release Procedure","text":"<p>To generate a new release, adhere to these steps:</p> <ol> <li> <p>Modify the Simmate version number in <code>pyproject.toml</code> (link)</p> </li> <li> <p>Update the changelog with the new release and its release date.</p> </li> <li> <p>Confirm all tests pass using the pre-built database. If they don't, generate a new one using the commands below, rename your db file (e.g., <code>prebuild-2022-07-05.sqlite3</code>), compress the db file into a zip file, upload it to the Simmate CDN, and modify the <code>archive_filename</code> in <code>simmate.database.third_parties.utilites.load_default_sqlite3_build</code>. <pre><code>simmate database reset --confirm-delete --no-use-prebuilt\nsimmate database load-remote-archives\n</code></pre></p> </li> <li> <p>Generate a release on Github, which will automatically release to pypi.</p> </li> <li> <p>Wait for the autotick bot to initiate a pull request for the simmate feedstock. Check the status here (under \"Queued\").</p> </li> <li> <p>Review the autotick bot's changes before merging. If there were substantial changes, use grayskull to modify the version number, sha256, and dependencies.</p> </li> <li> <p>After merging, wait for the conda-forge channels to update their indexes (about 30 minutes). Then, test the conda install with: <pre><code># for a normal release\nconda create -n my_env -c conda-forge simmate -y\n\n# additionally, ensure spyder can also be installed in the same environment\nconda install -n my_env -c conda-forge spyder -y\n</code></pre></p> </li> </ol>"},{"location":"full_guides/extras/contributing/maintainer_notes/#full-test-suite","title":"Full Test Suite","text":"<p>Unit tests that require third-party programs (like VASP) are disabled by default. However, it's advisable to run a full test before new releases. To execute all unit tests that call programs like VASP:</p> <ol> <li> <p>Ensure you have the following prerequisites:</p> <ul> <li>A Linux environment with VASP &amp; Bader installed</li> <li>Dev version of Simmate installed</li> <li>The <code>main</code> branch of the official repo checked out</li> <li><code>simmate_dev</code> environment is active</li> <li>The base Simmate directory as the current working directory</li> <li>Clear any custom <code>~/simmate</code> configs (i.e., ensure default settings)</li> </ul> </li> <li> <p>Confirm the default test suite works: <pre><code>pytest\n</code></pre></p> </li> <li> <p>Reset your database, switch to the pre-built, and update it. This simulates the database of a new user: <pre><code>simmate database reset --confirm-delete --use-prebuilt\nsimmate database update\n</code></pre></p> </li> <li> <p>Open <code>pyproject.toml</code> and modify the following line to run the VASP tests: <pre><code># original line\naddopts = \"--no-migrations --durations=15 -m 'not blender and not vasp'\"\n\n# updated line\naddopts = \"--no-migrations --durations=15 -m 'not blender'\"\n</code></pre></p> </li> <li> <p>(Optional) By default, all VASP tests run using <code>mpirun -n 12 vasp_std &gt; vasp.out</code>. Modify this in <code>src/simmate/workflows/tests/test_all_workflow_runs.py</code> if needed.</p> </li> <li> <p>Run <code>pytest</code> again to pick up these tests. It's advisable to run specific tests and enable logging (<code>-s</code>) for monitoring: <pre><code># option 1\npytest\n\n# option 2(recommended)\npytest src/simmate/workflows/test/test_all_workflow_runs.py -s\n</code></pre></p> </li> <li> <p>If all tests pass, proceed with the new release. Discard your changes afterwards.</p> </li> </ol>"},{"location":"full_guides/extras/contributing/maintainer_notes/#website-css","title":"Website CSS","text":"<p>The Hyper theme, as outlined in our main docs here, must be built and hosted separately from any Simmate server due to licensing. To build/host the assets, adhere to these steps: </p> <ol> <li>Download the Hyper theme (private access): e.g., <code>Hyper_v4.6.0.zip</code></li> <li>Unpack the zip file and navigate to this directory: <pre><code>cd Hyper_v4.6.0/Bootstrap_5x/Hyper/\n</code></pre></li> <li>Install prerequisites into a new conda environment and activate it: <pre><code>conda create -n hyper -c conda-forge nodejs yarn git\nconda activate hyper\n</code></pre></li> <li>Install gulp using npm (conda install of gulp doesn't work): <pre><code>npm install gulp -g\n</code></pre></li> <li>In the main directory, install all Hyper dependencies using the <code>yarn.lock</code> file: <pre><code>yarn install\n</code></pre></li> <li>Edit themes/colors in the following files (e.g., change primary to <code>#0072ce</code>): <pre><code>/src/assests/scss/config/saas/\n&gt;&gt; go into each folder's _variables.scss\n</code></pre></li> <li>Build the assets: <pre><code>gulp build\n</code></pre></li> <li>Upload assets (in <code>dist</code> folder) to your CDN for serving.</li> </ol>"},{"location":"full_guides/extras/contributing/overview/","title":"Contributing to Simmate","text":"<p>This guide provides a step-by-step process for contributing new features to Simmate's code.</p>"},{"location":"full_guides/extras/contributing/overview/#understanding-core-dependencies","title":"Understanding Core Dependencies","text":"<p>As a newcomer to our community, you might initially engage in minor tasks such as correcting typos or adding small utilities/functions. However, if you aim to contribute more substantially, we highly recommend getting acquainted with our core dependencies. These include...</p> <ul> <li>Django for our database and website</li> <li>Prefect for our workflows and scheduler</li> <li>Dask for clusters and execution</li> </ul> <p>While you don't need to become an expert in these packages, a basic understanding of their functionalities will be advantageous.</p>"},{"location":"full_guides/extras/file_converters/overview/","title":"Overview","text":""},{"location":"full_guides/extras/file_converters/overview/#file-converters","title":"File Converters","text":"<p>This module offers functions for reading and writing a variety of file types. It's specifically designed to manage different crystal structure representations, such as POSCAR, CASTEP, and CIF file formats. As such, it includes functions for converting between these formats. Moreover, this module also includes functions for converting between different object types, specifically various Python classes.</p> <p>NOTE: Currently, this module only includes converters for the <code>structure</code> type. The <code>molecule</code> and <code>voxeldata</code> types are still under development.</p>"},{"location":"full_guides/extras/file_converters/structures/","title":"Structures","text":""},{"location":"full_guides/extras/file_converters/structures/#structure-converters","title":"Structure Converters","text":"<p>This module comprises converter classes for various structure file and object formats. All converters are directly linked for conversion into the <code>simmate.toolkit.base_data_types.structure.Structure</code> class. Simmate can automatically identify the file/object format you're using in most instances. Direct use of these converters is generally reserved for advanced usage or speed optimization:</p> <p>Here's an example of dynamic use:</p> <pre><code>from simmate.toolkit import Structure\n\nstructure1 = Structure.from_dynamic(\"example.cif\")\n\nstructure2 = Structure.from_dynamic(\"POSCAR\")\n\nstructure3 = Structure.from_dynamic(\n    {\"database_table\": \"MITStaticEnergy\", \"database_id\": 1}\n)\n</code></pre> <p>To convert between formats (for example, CIF to POSCAR), follow this two-step process:</p> <pre><code>from simmate.toolkit import Structure\n\n# STEP 1: convert to simmate\nstructure = Structure.from_dynamic(\"example.cif\")\n\n# STEP 2: convert to the desired format\nstructure.to(fmt=\"poscar\", filename=\"POSCAR\")\n</code></pre>"},{"location":"full_guides/extras/file_converters/structures/#additional-converters","title":"Additional Converters","text":"<p>This module does not include all the file-converters available in Simmate. You can find other converters in the <code>apps</code> module, where they are associated with a specific program. For example, the converter for POSCAR files is derived from the VASP software, so the POSCAR converter is located in the <code>vasp.inputs.poscar</code> module. Here's a list of other structure converters for your reference: </p> <ul> <li>POSCAR (<code>simmate.apps.vasp.inputs.poscar</code>)</li> </ul>"},{"location":"full_guides/extras/visualization/blender/","title":"Blender Setup","text":"<p>At present, the most direct method to utilize Blender with Simmate involves a manual installation of Blender, which Simmate can then access via the command line. This process may necessitate additional steps for users who only wish to visualize structures without directly using Blender. In the future, we may explore the possibility of employing a Blender developer to facilitate the use of Blender as a bpy module, even if it's a minimal build specifically for Simmate.</p>"},{"location":"full_guides/extras/visualization/blender/#previous-notes-on-building-bpy-from-source","title":"Previous Notes on Building bpy from Source","text":""},{"location":"full_guides/extras/visualization/blender/#building-a-custom-blender-bpy-module-on-ubuntu-1804","title":"Building a Custom Blender bpy Module (on Ubuntu 18.04)","text":"<p>Follow the instructions from https://wiki.blender.org/wiki/Building_Blender 1. Install all necessary dependencies to build the final package: <pre><code>sudo apt install git;\nsudo apt install build-essential;\nsnap install cmake --classic;\nmkdir ~/blender-git;\ncd ~/blender-git;\ngit clone https://git.blender.org/blender.git;\ncd blender;\ngit submodule update --init --recursive;\ngit submodule foreach git checkout master;\ngit submodule foreach git pull --rebase origin master;\ncd ~/blender-git/blender/build_files/build_environment;\n./install_deps.sh;\n</code></pre> 2. In the bpy_module.cmake file, set the portable setting to 'ON': <pre><code>nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake;\n</code></pre> Modify this line: <pre><code>set(WITH_INSTALL_PORTABLE    ON CACHE BOOL \"\" FORCE)\n</code></pre> 3. With the settings and dependencies prepared, build Blender as a Python module. The result will be in /blender-git/build_linux_bpy/bin/ <pre><code>cd ~/blender-git/blender;\nmake bpy;\n</code></pre> 4. Transfer the created module into the Anaconda environment and delete all build files. <pre><code>cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/;\nsudo rm -r /home/jacksund/blender-git;\n</code></pre> 5. You can now 'import bpy' in your Python environment! </p>"},{"location":"full_guides/extras/visualization/blender/#building-a-custom-blender-bpy-module-on-ubuntu-1910","title":"Building a Custom Blender bpy Module (on Ubuntu 19.10)","text":"<p>Follow the instructions from https://wiki.blender.org/wiki/Building_Blender/Linux/Ubuntu and troubleshoot with https://devtalk.blender.org/t/problem-with-running-blender-as-a-python-module/7367 1. Install all necessary dependencies to build the final package: <pre><code>sudo apt-get update;\nsudo apt-get install build-essential git subversion cmake libx11-dev libxxf86vm-dev libxcursor-dev libxi-dev libxrandr-dev libxinerama-dev;\nmkdir ~/blender-git;\ncd ~/blender-git;\ngit clone http://git.blender.org/blender.git;\ncd ~/blender-git/blender;\nmake update;\ncd ~/blender-git\n./blender/build_files/build_environment/install_deps.sh --with-all\n</code></pre> 2. In the bpy_module.cmake file, set the portable setting to 'ON': <pre><code>nano /home/jacksund/blender-git/blender/build_files/cmake/config/bpy_module.cmake;\n</code></pre> Modify this line: <pre><code>set(WITH_INSTALL_PORTABLE    ON CACHE BOOL \"\" FORCE)\n</code></pre> Add these lines:  <pre><code>set(WITH_MEM_JEMALLOC OFF CACHE BOOL \"\" FORCE)\nset(WITH_MOD_OCEANSIM        OFF CACHE BOOL \"\" FORCE)\n\nset(FFMPEG_LIBRARIES avformat;avcodec;avutil;avdevice;swscale;swresample;lzma;rt;theora;theoradec;theoraenc;vorbis;vorbisenc;vorbisfile;ogg;xvidcore;vpx;opus;mp3lame;x264;openjp2 CACHE STRING \"\" FORCE)\n\nset(PYTHON_VERSION 3.7 CACHE STRING \"\" FORCE)\nset(LLVM_VERSION 6.0 CACHE STRING \"\" FORCE)\n\nset(OPENCOLORIO_ROOT_DIR /opt/lib/ocio CACHE STRING \"\" FORCE)\nset(OPENIMAGEIO_ROOT_DIR /opt/lib/oiio CACHE STRING \"\" FORCE)\nset(OSL_ROOT_DIR /opt/lib/osl CACHE STRING \"\" FORCE)\nset(OPENSUBDIV_ROOT_DIR /opt/lib/osd CACHE STRING \"\" FORCE)\nset(OPENCOLLADA_ROOT_DIR /opt/lib/opencollada CACHE STRING \"\" FORCE)\nset(EMBREE_ROOT_DIR /opt/lib/embree CACHE STRING \"\" FORCE)\nset(OPENIMAGEDENOISE_ROOT_DIR /opt/lib/oidn CACHE STRING \"\" FORCE)\nset(ALEMBIC_ROOT_DIR /opt/lib/alembic CACHE STRING \"\" FORCE)\n</code></pre> 3. With the settings and dependencies prepared, build Blender as a Python module. The result will be in /blender-git/build_linux_bpy/bin/ <pre><code>cd ~/blender-git/blender;\nmake bpy;\n</code></pre> 4. Transfer the created module into the Anaconda environment and delete all build files. <pre><code>cp -r /home/jacksund/blender-git/build_linux_bpy/bin/* /home/jacksund/anaconda3/envs/jacks_env/lib/python3.7/site-packages/;\nsudo rm -r /home/jacksund/blender-git;\n</code></pre> 5. You can now 'import bpy' in your Python environment!</p>"},{"location":"full_guides/extras/visualization/overview/","title":"Overview","text":""},{"location":"full_guides/extras/visualization/overview/#simmate-visualization","title":"Simmate Visualization","text":"<p>This module aids in the creation of plots and 3D models for data and structure visualization. It functions as an extension of the <code>pymatgen.ext</code> module.</p> <p>NOTE: Currently, this module solely generates 3D models for <code>Structure</code> objects utilizing Blender. We are actively planning and developing additional features.</p> <p>In the meantime, we recommend using either VESTA or OVITO for crystal structure visualization while we enhance our tools.</p>"},{"location":"full_guides/website/overview/","title":"Simmate Website Module","text":"<p>The <code>simmate.website</code> module hosts everything for the website interface and API. Unlike most major projects, Simmate makes our website's source-code openly available to everyone! This means that you can host your own Simmate website on your own computer or even in production for your lab to privately use.</p>"},{"location":"full_guides/website/overview/#local-server-setup","title":"Local Server Setup","text":"<p>To set up Simmate on your local computer, execute the following command:</p> command line <pre><code># Initialize your database if not done already\nsimmate database reset\n\n# Run the website locally\nsimmate run-server\n</code></pre> <p>While this command is running, open your preferred browser (Chrome, Firefox, etc.) and navigate to http://127.0.0.1:8000/. This local version of our website at <code>simmate.org</code> operates on your desktop using your personal database.</p>"},{"location":"full_guides/website/overview/#production-server-setup","title":"Production Server Setup","text":"<p>To set up a production-ready server for your team, you have three options:</p> <ol> <li>Collaborate with the Simmate team and join our server</li> <li>Request our team to manage a server for you</li> <li>Set up and manage your own server</li> </ol> <p>For options 1 and 2, contact us at <code>simmate.team@gmail.com</code>.</p> <p>For option 3, follow our guide for setting up a server on DigitalOcean. Ensure you've completed the base Simmate tutorials, particularly the ones on setting up a cloud database and setting up computational resources.</p>"},{"location":"full_guides/website/overview/#third-party-sign-ins","title":"Third-Party Sign-Ins","text":"<p>Simmate supports sign-ins via third-party accounts such as Google and Github, thanks to the <code>django-allauth</code> package.</p> <p>By default, servers won't display these sign-in buttons. If you wish to enable third-party account logins, you'll need to configure this manually. Although <code>django-allauth</code> supports many account types (see their full list), Simmate currently only supports Github and Google. Guides for setting these up are provided below.</p>"},{"location":"full_guides/website/overview/#github-oauth","title":"Github OAuth","text":"<ol> <li>Create a new OAuth application here using the following information (replace <code>http://127.0.0.1:8000</code> with your cloud server link if applicable):</li> </ol> <pre><code>application name = My New Simmate Server (edit as desired)\nhomepage url = http://127.0.0.1:8000\nauthorization callback url = http://127.0.0.1:8000/accounts/github/login/callback/\n</code></pre> <ol> <li> <p>On the next page, select \"Generate a new client secret\" and copy this value.</p> </li> <li> <p>Set the environment variables on your local computer or production-ready server: <pre><code>GITHUB_SECRET = examplekey1234 (value from step 2)\nGITHUB_CLIENT_ID = exampleid1234 (value listed on Github as \"Client ID\")\n</code></pre></p> </li> </ol>"},{"location":"full_guides/website/overview/#google-oauth","title":"Google OAuth","text":"<ol> <li>Follow the django-allauth steps here to configure the Google API application.</li> <li>Set the environment variables on your local computer or production-ready server: <pre><code>GOOGLE_SECRET = examplekey1234\nGOOGLE_CLIENT_ID = exampleid1234\n</code></pre></li> </ol>"},{"location":"full_guides/website/overview/#css-and-js-assets","title":"CSS and JS Assets","text":"<p>Simmate doesn't distribute most source CSS and JavaScript files due to licensing restrictions on our third-party vendor assets. We use the Hyper theme from the CoderThemes team. You can contribute to Simmate's website using their Modern Dashboard template without needing to access any of the assets. Our templates load assets from a Simmate CDN:</p> <pre><code>&lt;!-- Normal asset loading with source code distribution --&gt;\n&lt;link href=\"assets/css/vendor/fullcalendar.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n\n&lt;!-- Simmate's CDN-based asset loading --&gt;\n&lt;link href=\"https://archives.simmate.org/assets/fullcalendar.min.css\" rel=\"stylesheet\" type=\"text/css\" /&gt;\n</code></pre> <p>If you need to modify the CSS or JS, please contact our team to discuss the best approach.</p>"},{"location":"full_guides/website/rest_api/","title":"Grasping the REST API","text":"<p>Warning</p> <p>This section is only for experts! If you're aiming to extract data from Simmate, we suggest using our Python client, as outlined in the database guides. The REST API is mainly for teams that cannot use Python or Simmate's code but still need to extract data. Be aware that data extraction via our REST API is heavily throttled, making it unsuitable for large data retrievals.</p> <p>\"REST API\" stands for \"Representational State Transfer (REST) Application Programming Interfaces (API)\". In simpler terms, it's a way to access databases through a website URL.</p>"},{"location":"full_guides/website/rest_api/#example-endpoint","title":"Example Endpoint","text":"<p>To get a better grasp of our API, let's examine some examples. We'll concentrate on the Materials Project database (at <code>/data/MatprojStructure/</code>), but keep in mind, nearly every URL within Simmate has REST API functionality! Our API endpoints are auto-generated from our database module, facilitating the introduction of new features and tables.</p>"},{"location":"full_guides/website/rest_api/#api-usage","title":"API Usage","text":"<p>Consider a typical URL and webpage: <pre><code>http://simmate.org/data/MatprojStructure/\n</code></pre></p> <p>This link takes you to a webpage where you can browse all Materials Project structures. However, this URL also serves as a REST API! To access it, simply add <code>?format=api</code> to the URL. Try this link: <pre><code>http://simmate.org/data/MatprojStructure/?format=api\n</code></pre></p> <p>Likewise, adding <code>?format=json</code> will return data in a JSON dictionary: <pre><code>http://simmate.org/data/MatprojStructure/?format=json\n</code></pre></p> <p>This also applies to individual entries. For example, to access all data for the structure with id <code>mp-1</code>, use... <pre><code>http://simmate.org/data/MatprojStructure/mp-1/?format=api\nhttp://simmate.org/data/MatprojStructure/mp-1/?format=json\n</code></pre></p> <p>The output should look like... <pre><code>{\n    \"id\": \"mp-1\",\n    \"structure\": \"...(hidden for clarity)\",\n    \"nsites\": 1,\n    \"nelements\": 1,\n    \"elements\": [\"Cs\"],\n    \"chemical_system\": \"Cs\",\n    \"density\": 1.9350390306525629,\n    \"density_atomic\": 0.00876794537479071,\n    \"volume\": 114.05180544066401,\n    \"volume_molar\": 68.68360262958124,\n    \"formula_full\": \"Cs1\",\n    \"formula_reduced\": \"Cs\",\n    \"formula_anonymous\": \"A\",\n    \"energy\": -0.85663276,\n    \"energy_per_atom\": -0.85663276,\n    \"energy_above_hull\": null,\n    \"is_stable\": null,\n    \"decomposes_to\": null,\n    \"formation_energy\": null,\n    \"formation_energy_per_atom\": null,\n    \"spacegroup\": 229,\n}\n</code></pre></p>"},{"location":"full_guides/website/rest_api/#filtering-results","title":"Filtering Results","text":"<p>Our URLs also support advanced filtering. For instance, to search for all structures with the spacegroup 229 in the Cr-N chemical system, the URL becomes... <pre><code>http://simmate.org/data/MatprojStructure/?chemical_system=Cr-N&amp;spacegroup__number=229\n</code></pre></p> <p>Conditions are specified by adding a question mark (<code>?</code>) to the URL, followed by <code>example_key=desired_value</code>. Additional conditions are separated by <code>&amp;</code>, resulting in <code>key1=value1&amp;key2=value2&amp;key3=value3</code>, etc. You can also add <code>format=api</code> to this!</p> <p>However, for complex or advanced cases, we recommend using the <code>simmate.database</code> module, as it provides more robust filtering capabilities.</p>"},{"location":"full_guides/website/rest_api/#pagination-of-results","title":"Pagination of Results","text":"<p>To avoid server overload, Simmate currently returns a maximum of 12 results at a time. Pagination is automatically managed using the <code>page=...</code> keyword in the URL. In the HTML, API, and JSON views, links to the next page of results are always provided. For instance, in the JSON view, the returned data includes <code>next</code> and <code>previous</code> URLs.</p>"},{"location":"full_guides/website/rest_api/#ordering-results","title":"Ordering Results","text":"<p>For API and JSON formats, you can manually determine the order of returned data by adding <code>ordering=example_column</code> to your URL. To reverse the order, use <code>ordering=-example_column</code> (note the \"<code>-</code>\" before the column name). For example:</p> <pre><code>http://simmate.org/data/MatprojStructure/?ordering=density_atomic\n</code></pre>"},{"location":"full_guides/website/rest_api/#advanced-api-usage","title":"Advanced API Usage","text":"<p>Simmate's REST API is built using the django-rest-framework Python package, and filtering is implemented using django-filter. </p> <p>Our endpoints are dynamically created for each request, thanks to our <code>SimmateAPIViewset</code> class in the <code>simmate.website.core_components</code> module, which automatically renders an API endpoint from a Simmate database table. The backend implementation of dynamic APIs is still experimental as we assess the pros and cons of this approach -- for example, it allows for quick Django configuration, but at the expense of increased CPU time per web request.</p>"},{"location":"full_guides/workflows/creating_new_workflows/","title":"Creating New Workflows","text":""},{"location":"full_guides/workflows/creating_new_workflows/#workflow-naming","title":"Workflow Naming","text":"<p>To create a workflow name, adhere to the Simmate conventions and run checks to ensure everything operates as expected:</p> <pre><code>from simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n    pass  # we will build the rest of workflow later\n\n# Assign long and complex names to a variable for easier access.\nmy_workflow = Example__Python__MyFavoriteSettings\n\n# Verify that our naming convention works as expected\nassert my_workflow.name_full == \"example.python.my-favorite-settings\"\nassert my_workflow.name_type == \"example\"\nassert my_workflow.name_app == \"python\"\nassert my_workflow.name_preset == \"my-favorite-settings\"\n</code></pre>"},{"location":"full_guides/workflows/creating_new_workflows/#basic-workflow","title":"Basic Workflow","text":"<p>To construct a Simmate workflow, you can use any Python code. The only requirement is that you place the code inside a <code>run_config</code> method of a new subclass for <code>Workflow</code>:</p> <pre><code>from simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False  # we don't have a database table yet\n\n    @staticmethod\n    def run_config(**kwargs):\n        print(\"This workflow doesn't do much\")\n        return 12345\n</code></pre> <p>Note</p> <p>Behind the scenes, the <code>run</code> method transforms our <code>run_config</code> into a workflow and performs additional setup tasks.</p> <p>Danger</p> <p>We added <code>**kwargs</code> to our function input. This is required for your workflow to run. Make sure you read the \"Default parameters\" section below to understand why.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#pythonic-workflow","title":"Pythonic Workflow","text":"<p>Here's a realistic example where we construct a Workflow that has input parameters and accesses class attributes/methods:</p> <pre><code>class Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False # we don't have a database table yet\n    example_constant = 12\n\n    @staticmethod\n    def squared(x):\n        return x ** 2\n\n    @classmethod\n    def run_config(cls, name, say_hello=True, **kwargs):\n        if say_hello:\n            print(f\"Hello and welcome, {name}!\")\n\n        # access class values and methods\n        x = cls.example_constant\n        example_calc = cls.squared(x)\n        print(f\"Our calculation gave a result of {example_calc}\")\n\n        # access extra arguments if needed\n        for key, value in kwargs.items():\n            print(\n                f\"An extra parameter for {key} was given \"\n                f\"with a value of {value}\"\n            )\n\n        return \"Success!\"\n</code></pre> <p>Danger</p> <p>The <code>**kwargs</code> is still crucial here. Make sure we are adding it at the end of our input parameters. (see the next section for why)</p>"},{"location":"full_guides/workflows/creating_new_workflows/#default-parameters-and-using-kwargs","title":"Default Parameters and Using kwargs","text":"<p>In the workflows above, we used <code>**kwargs</code> in each of our <code>run_config</code> methods. If you remove these, the workflow will fail. This is because Simmate automatically passes default parameters to the <code>run_config</code> method -- even if you didn't define them as inputs. </p> <p>We do this to allow all workflows to access key information about the run. These parameters are:</p> <ul> <li><code>run_id</code>: a unique id for tracking a calculation</li> <li><code>directory</code>: a unique folder name where the calculation will take place</li> <li><code>compress_output</code>: whether to compress the directory to a zip file when we're done</li> <li><code>source</code>: where the input of this calculation came from</li> </ul> <p>You can use any of these inputs to assist with your workflow. Alternatively, just add <code>**kwargs</code> to your function and ignore them.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#common-input-parameters","title":"Common Input Parameters","text":"<p>You often will use input parameters that correspond to <code>toolkit</code> objects, such as <code>Structure</code> or <code>Composition</code>. If you use the matching input parameter name, these will inherit all of their features -- such as loading from filename, a dictionary, or python object.</p> <p>For example, if you use a <code>structure</code> input variable, it behaves as described in the Parameters section.</p> <pre><code>from simmate.toolkit import Structure\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False  # we don't have a database table yet\n\n    @staticmethod\n    def run_config(structure, **kwargs):\n\n        # Even if we give a filename as an input, Simmate will convert it\n        # to a python object for us\n        assert type(structure) == Structure\n\n        # and you can interact with the structure object as usual\n        return structure.volume\n</code></pre> <p>Tip</p> <p>If you see a parameter in our documentation that has similar use to yours, make sure you use the same name. It can help with adding extra functionality.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#writing-output-files","title":"Writing Output Files","text":"<p>Of all the default parameters (described above), you'll likely get the most from using the <code>directory</code> input. It is important to note that <code>directory</code> is given as a <code>pathlib.Path</code> object. Just add the directory to your run_config() method and use the object that's provided.</p> <p>For example, this workflow will write an output file to <code>simmate-task-12345/my_output.txt</code> (where the <code>simmate-task-12345</code> folder is automatically set up by Simmate).</p> <pre><code>from simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False  # we don't have a database table yet\n\n    @staticmethod\n    def run_config(directory, **kwargs):\n\n        # We use the unique directory to write outputs!\n        # Recall that we have a pathlib.Path object.\n        output_file = directory / \"my_output.txt\"\n\n        with output_file.open(\"w\") as file:\n            file.write(\"Writing my output!\")\n\n        return \"Done!\"\n</code></pre>"},{"location":"full_guides/workflows/creating_new_workflows/#building-from-existing-workflows","title":"Building from Existing Workflows","text":"<p>For many apps, there are workflow classes that you can use as a starting point. For example, VASP users can inherit from the <code>VaspWorkflow</code> class, which includes many built-in features:</p> basic VASP examplefull-feature VASP exampleCustom INCAR modifier <pre><code>from simmate.apps.vasp.workflows.base import VaspWorkflow\n\nclass Relaxation__Vasp__MyExample1(VaspWorkflow):\n\n    functional = \"PBE\"\n    potcar_mappings = {\"Y\": \"Y_sv\", \"C\": \"C\"}\n\n    _incar = dict(\n        PREC=\"Normal\",\n        EDIFF=1e-4,\n        ENCUT=450,\n        NSW=100,\n        KSPACING=0.4,\n    )\n</code></pre> <pre><code>from simmate.apps.vasp.workflows.base import VaspWorkflow\nfrom simmate.apps.vasp.inputs import PBE_POTCAR_MAPPINGS\nfrom simmate.apps.vasp.error_handlers import (\n    Frozen,\n    NonConverging,\n    Unconverged,\n    Walltime,\n)\n\n\nclass Relaxation__Vasp__MyExample2(VaspWorkflow):\n\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS  # (1)\n\n    _incar = dict(\n        PREC=\"Normal\",  # (2)\n        EDIFF__per_atom=1e-5,  # (3)\n        ENCUT=450,\n        ISIF=3,\n        NSW=100,\n        IBRION=1,\n        POTIM=0.02,\n        LCHARG=False,\n        LWAVE=False,\n        KSPACING=0.4,\n        multiple_keywords__smart_ismear={  # (4)\n            \"metal\": dict(\n                ISMEAR=1,\n                SIGMA=0.06,\n            ),\n            \"non-metal\": dict(\n                ISMEAR=0,\n                SIGMA=0.05,\n            ),\n        },\n        # WARNING --&gt; see \"Custom Modifier\"\" tab for this to work\n        EXAMPLE__multiply_nsites=8,  # (5)\n    )\n\n    error_handlers = [  # (6)\n        Unconverged(),\n        NonConverging(),\n        Frozen(),\n        Walltime(),\n    ]\n</code></pre> <ol> <li>You can use pre-set mapping for all elements rather than define them yourself</li> <li>Settings that match the normal VASP input are the same for all structures regardless of composition.</li> <li>Settings can also be set based on the input structure using built-in tags like <code>__per_atom</code>. Note the two underscores (<code>__</code>) signals that we are using a input modifier.</li> <li>The type of smearing we use depends on if we have a metal, semiconductor, or insulator. So we need to decide this using a built-in keyword modifier named <code>smart_ismear</code>. Because this handles the setting of multiple INCAR values, the input begins with <code>multiple_keywords</code> instead of a parameter name.</li> <li>If you want to create your own logic for an input parameter, you can do that as well. Here we are showing a new modifier named <code>multiply_nsites</code>. This would set the incar value of EXAMPLE=16 for structure with 2 sites (2*8=16). Note, we define how this modifer works and register it in the \"Custom INCAR modifier\" tab. Make sure you include this code as well.</li> <li>These are some default error handlers to use, and there are many more error handlers available than what's shown. Note, the order of the handlers matters here. Only the first error handler triggered in this list will be used before restarting the job</li> </ol> <p>If you need to add advanced logic for one of your INCAR tags, you can register a keyword_modifier to the INCAR class like so: <pre><code># STEP 1: define the logic of your modifier as a function\n# Note that the function name must begin with \"keyword_modifier_\"\ndef keyword_modifier_multiply_nsites(structure, example_mod_input):\n    # add your advanced logic to determine the keyword value.\n    return structure.num_sites * example_mod_input\n\n# STEP 2: register modifier with the Incar class\nfrom simmate.apps.vasp.inputs import Incar\nIncar.add_keyword_modifier(keyword_modifier_multiply_nsites)\n\n# STEP 3: use your new modifier with any parameter you'd like\n_incar = dict(\n    \"NSW__multiply_nsites\": 2,\n    \"EXAMPLE__multiply_nsites\": 123,\n)\n</code></pre></p> <p>Danger</p> <p>Make sure this code is ran BEFORE you run the workflow. Registration is  reset every time a new python session starts. Therefore, we recommend  keeping your modifer in the same file that you define your workflow in.</p> <p>You can also use Python inheritance to borrow utilities and settings from an existing workflow:</p> <pre><code>from simmate.workflows.utilities import get_workflow\n\noriginal_workflow = get_workflow(\"static-energy.vasp.matproj\")\n\n\nclass StaticEnergy__Vasp__MyCustomPreset(original_workflow):\n\n    version = \"2022.07.04\"\n\n    _incar_updates = dict(\n        NPAR=1,\n        ENCUT=-1,\n    )\n</code></pre> <p>Tip</p> <p>To gain more insight to workflows like this, you should read through both the \"Creating S3 Workflows\" and related \"Apps\" sections for more  information.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#linking-a-database-table","title":"Linking a Database Table","text":"<p>Many workflows will want to store common types of data (such as static energy or relaxation data). If you want to use these tables automatically, you simply need to ensure your <code>name_type</code> matches what is available!</p> <p>For example, if we look at a static-energy calculation, you will see the <code>StaticEnergy</code> database table is automatically used because the name of our workflow starts with \"StaticEnergy\":</p> <pre><code>from simmate.database import connect\nfrom simmate.database.workflow_results import StaticEnergy\n\n# no work required! This line shows everything is setup and working\nassert StaticEnergy__Vasp__MyCustomPreset.database_table == StaticEnergy\n</code></pre> <p>If you want to build or use a custom database, you must first have a registered <code>DatabaseTable</code>, and then you can link the database table to your workflow directly. The only other requirement is that your database table uses the <code>Calculation</code> database mix-in: </p> <pre><code>from my_project.models import MyCustomTable\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n    database_table = MyCustomTable\n</code></pre>"},{"location":"full_guides/workflows/creating_new_workflows/#workflows-that-call-a-command","title":"Workflows that Call a Command","text":"<p>In many cases, you may have a workflow that runs a command or some external program and then reads the results from output files. An example of this would be an energy calculation using VASP. If your workflow involves calling another program, you should read about the <code>S3Workflow</code> which helps with writing input files, calling other programs, and handling errors.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#registering-your-workflow","title":"Registering Your Workflow","text":"<p>Registering your workflow so that you can access it in the UI requires you to build a \"simmate project\". This is covered in the getting-started tutorials.</p>"},{"location":"full_guides/workflows/creating_new_workflows/#running-your-custom-workflow","title":"Running Your Custom Workflow","text":"<p>Once you have your new workflow and registered it, you can run it as you would any other one.</p> yamlpython <pre><code>workflow_name: path/to/my/script.py:my_workflow_obj  # (1)\n\n# Example parameters from our \"Basic Workflow\" above\nname: Jack\nsay_hello: true\n</code></pre> <ol> <li>If your workflow is not regiestered, you need to provide the path to your python script (e.g. <code>my_script.py</code> file) and then the variable name that the workflow is stored as. The normal variable would be  <code>Example__Python__MyFavoriteSettings</code>, but in the python example, we set it to something shorter like <code>my_workflow</code> for convenience.</li> </ol> <pre><code># in the same file the workflow is defined in\n\n# These names can be long and unfriendly, so it can be nice to\n# link them to a variable name for easier access.\nmy_workflow = Example__Python__MyFavoriteSettings\n\n# Here we use parameters from our \"Basic Workflow\" above\nstate = my_workflow.run(\n    name=\"Jack\"\n    say_hello=True,\n)\nresult = state.result()\n</code></pre> <p>Example</p> <p>If you wrote your workflow in a file name <code>learning_simmate.py</code>, you could set the workflow_name to <code>learning_simmate.py:Example__Python__MyFavoriteSettings</code>.</p> <p>Make sure you read the \"common input parameters\" section above. These let us really take advantage of how we provide our input. For example, a <code>structure</code> parameter will automatically accept filenames or database entries:</p> yamlpython <pre><code>workflow_name: path/to/my/script.py:my_workflow_obj\n\n# Automatic features!\nstructure:\n    database_table: MatProjStructure\n    database_id: mp-123\n</code></pre> <pre><code># in the same file the workflow is defined in\nstate = my_workflow.run(\n    structure={\n        \"database_table\": \"MatProjStructure\",\n        \"database_id\": \"mp-123\",\n    }\n)\nresult = state.result()\n</code></pre> <p>Warning</p> <p>When switching from Python to YAML, make sure you adjust the input format of your parameters. This is especially important if you use python a <code>list</code> or <code>dict</code> for one of your input parameters. Further, if you have complex input parameters (e.g. nested lists, matricies, etc.), we recommend using a TOML input file instead.</p> listsdictionariesnested liststuplenested liststuple <p><pre><code># in python\nmy_parameter = [1,2,3]\n</code></pre> <pre><code># in yaml\nmy_parameter:\n    - 1\n    - 2\n    - 3\n</code></pre></p> <p><pre><code># in python\nmy_parameter = {\"a\": 123, \"b\": 456, \"c\": [\"apple\", \"orange\", \"grape\"]}\n</code></pre> <pre><code># in yaml\nmy_parameter:\n    a: 123\n    b: 456\n    c:\n        - apple\n        - orange\n        - grape\n</code></pre> <pre><code># in toml\n[my_parameter]\na = 123\nb = 456\nc = [\"apple\", \"orange\", \"grape\"]\n</code></pre></p> <p><pre><code># in python\nmy_parameter = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n]\n</code></pre> <pre><code># in yaml (we recommend switching to TOML!)\nmy_parameter:\n    - - 1\n      - 2\n      - 3\n    - - 4\n      - 5\n      - 6\n    - - 7\n      - 8\n      - 9\n</code></pre> <pre><code># in toml\nmy_parameter = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n]\n</code></pre></p> <p><pre><code># in python\nmy_parameter = (1,2,3)\n</code></pre> <pre><code># in yaml\nmy_parameter:\n    - 1\n    - 2\n    - 3\n# WARNING: This will return a list! Make sure you call \n#   `tuple(my_parameter)`\n# at the start of your workflow's `run_config` if you need a tuple.\n</code></pre> <pre><code># in toml\nmy_parameter = [1, 2, 3]\n# WARNING: This will return a list! Make sure you call \n#   `tuple(my_parameter)`\n# at the start of your workflow's `run_config` if you need a tuple.\n</code></pre></p> <p><pre><code># in python\nmy_parameter = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n]\n</code></pre> <pre><code># in yaml (we recommend switching to TOML!)\nmy_parameter:\n    - - 1\n      - 2\n      - 3\n    - - 4\n      - 5\n      - 6\n    - - 7\n      - 8\n      - 9\n</code></pre> <pre><code># in toml\nmy_parameter = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n]\n</code></pre></p> <p><pre><code># in python\nmy_parameter = (1,2,3)\n</code></pre> <pre><code># in yaml\nmy_parameter:\n    - 1\n    - 2\n    - 3\n# WARNING: This will return a list! Make sure you call \n#   `tuple(my_parameter)`\n# at the start of your workflow's `run_config` if you need a tuple.\n</code></pre> <pre><code># in toml\nmy_parameter = [1, 2, 3]\n# WARNING: This will return a list! Make sure you call \n#   `tuple(my_parameter)`\n# at the start of your workflow's `run_config` if you need a tuple.\n</code></pre></p>"},{"location":"full_guides/workflows/nested_workflows/","title":"Creating nested workflows","text":""},{"location":"full_guides/workflows/nested_workflows/#overview","title":"Overview","text":"<p>Workflows can incorporate any Python code, enabling them to invoke other workflows using the <code>run</code> or <code>run_cloud</code> methods. This functionality allows for the sequential execution of multiple workflows or their submission to a cluster for large-scale analyses.</p>"},{"location":"full_guides/workflows/nested_workflows/#running-a-workflow-repeatedly","title":"Running a Workflow Repeatedly","text":"<p>You can use the <code>run</code> method of a workflow within another workflow for repeated runs.</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False  # no database table yet\n\n    @staticmethod\n    def run_config(structure, **kwargs):\n\n        another_workflow = get_workflow(\"static-energy.vasp.mit\")\n\n        for n in range(10):\n            structure.perturb(0.05)  # in-place modification\n            state = another_workflow.run(structure=structure)\n            result = state.result()\n            # ... process the result\n</code></pre> <p>Note</p> <p>The <code>state.result()</code> call is used just like in a regular workflow run. The usage remains the same.</p>"},{"location":"full_guides/workflows/nested_workflows/#running-multiple-workflows","title":"Running Multiple Workflows","text":"<p>You can call a series of workflows on an input. The <code>run_config</code> accepts any Python code, so the workflow usage remains unchanged:</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False\n\n    @staticmethod\n    def run_config(structure, directory, **kwargs):\n\n        subworkflow_1 = get_workflow(\"static-energy.vasp.mit\")\n        subworkflow_1.run(structure=structure)\n\n        subworkflow_2 = get_workflow(\"population-analysis.vasp.elf-matproj\")\n        subworkflow_2.run(structure=structure)\n\n        subworkflow_3 = get_workflow(\"electronic-structure.vasp.matproj-full\")\n        subworkflow_3.run(structure=structure)      \n</code></pre>"},{"location":"full_guides/workflows/nested_workflows/#storing-all-runs-in-a-shared-directory","title":"Storing All Runs in a Shared Directory","text":"<p>To save the results of all workflows in a single location, manually set the directory for each subworkflow run:</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False\n\n    @staticmethod\n    def run_config(structure, directory, **kwargs):  \n        another_workflow = get_workflow(\"static-energy.vasp.mit\")\n        for n in range(10):\n            structure.perturb(0.05)\n\n            subdirectory = directory / f\"perturb_number_{n}\"\n\n            another_workflow.run(\n                structure=structure,\n                directory=subdirectory, # creates a subdirectory for this run\n            )\n</code></pre> <p>Danger</p> <p>Do NOT share a working directory when using <code>run_cloud</code>. This can lead to problems when resources are distributed across different computers and file systems. Refer to github #237 for more information.</p>"},{"location":"full_guides/workflows/nested_workflows/#transferring-results-between-runs","title":"Transferring Results Between Runs","text":"<p>You can pass the result from one subworkflow to the next subworkflow by interacting with the database object.</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False\n\n    @staticmethod\n    def run_config(structure, directory, **kwargs):\n\n        subworkflow_1 = get_workflow(\"relaxation.vasp.mit\")\n        state_1 = subworkflow_1.run(structure=structure)\n        result_1 = state_1.result()\n\n        subworkflow_2 = get_workflow(\"static-energy.vasp.mit\")\n        state_2 = subworkflow_2.run(\n            structure=result_1,  # use the final structure of the last calculation\n        )\n        result_2 = state_2.result()\n\n        if result_2.energy_per_atom &gt; 0:\n            print(\"Structure is very unstable even after relaxing!\")\n            structure_new = result_2.to_toolkit()\n            structure_new.scale_lattice(\n                volume=structure.volume * 1.2,\n            )\n            state_2 = subworkflow_2.run(\n                structure=structure_new,  # use the modified structure\n            )\n</code></pre>"},{"location":"full_guides/workflows/nested_workflows/#transferring-files-between-runs","title":"Transferring Files Between Runs","text":"<p>A workflow can require a file from a previous calculation as an input. This can be specified using the <code>use_previous_directory</code> attribute, which copies files from the previous directory to the current one.</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False\n    use_previous_directory = [\"filename1\", \"filename2\"]\n\n    @staticmethod\n    def run_config(structure, directory, previous_directory, **kwargs):\n\n        expected_file1 = directory / \"filename1\"\n        assert expected_file1.exists()\n\n        expected_file2 = directory / \"filename2\"\n        assert expected_file2.exists()\n</code></pre> <p>Workflows with <code>use_previous_directory</code> set to True or a list of filenames MUST provide either a <code>previous_directory</code> parameter or a database object from a previous calculation as the <code>structure</code> parameter.</p> <pre><code>workflow.run(previous_directory=\"path/to/my/folder\")\n</code></pre> <pre><code>status = setup_workflow.run()\nprevious_result = status.result()\n\nworkflow.run(structure=previous_result)\n</code></pre> <p>Tip</p> <p>File copying/passing should be used for large files and data chunks. Small data pieces should be passed between workflows using Python objects and the database.</p>"},{"location":"full_guides/workflows/nested_workflows/#submitting-parallel-workflows","title":"Submitting Parallel Workflows","text":"<p>For situations where you don't want to wait for each workflow run to finish or need to submit hundreds of independent workflow runs, use the <code>run_cloud</code> command instead of <code>run</code>.</p> <pre><code>from simmate.workflows.utilities import get_workflow\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    use_database = False\n\n    @staticmethod\n    def run_config(structure, **kwargs):\n\n        another_workflow = get_workflow(\"static-energy.vasp.mit\")\n\n        submitted_states = []\n\n        for n in range(10):\n            structure.perturb(0.05)\n\n            state = another_workflow.run_cloud(structure=structure)\n\n            submitted_states.append(state)\n\n        results = [state.result() for state in submitted_states]\n\n        for result in results:\n            print(result.energy_per_atom)\n</code></pre> <p>Danger</p> <p>Do NOT share a working directory when using <code>run_cloud</code>. This can lead to problems when resources are distributed across different computers and file systems. Refer to github #237 for more information.</p> <p>Tip</p> <p>The <code>state.result()</code> call to wait for each result is optional. You can even have a workflow that just submits runs and then shuts down without ever waiting on the results.</p>"},{"location":"full_guides/workflows/overview/","title":"Simmate Workflows Module","text":"<p>This module organizes all predefined workflows by application for easy access.</p> <p>While this module provides basic usage, you can find more information in <code>simmate.engine.workflow</code>.</p>"},{"location":"full_guides/workflows/overview/#basic-usage","title":"Basic Usage","text":"<p>The getting-started tutorials offer detailed instructions on running workflows and retrieving their results. Here's a brief summary:</p> <pre><code>from simmate.workflows.static_energy import StaticEnergy__Vasp__Matproj as workflow\n\n# Run the workflow and return a state\nstate = workflow.run(structure=\"my_structure.cif\")\nresult = state.result()\n\n# Access the DatabaseTable where ALL results are stored\nworkflow.database_table  # --&gt; returns all relaxation results\nworkflow.all_results  # --&gt; returns all results for this relaxation preset\ndf = workflow.all_results.to_dataframe()  # convert to pandas dataframe\n</code></pre> <p>For more details on interacting with workflows, refer to the <code>simmate.engine</code> module, specifically the <code>simmate.engine.workflow</code> module.</p>"},{"location":"full_guides/workflows/overview/#simplifying-import-paths","title":"Simplifying Import Paths","text":"<p>The import path above may seem lengthy and complex, especially for regular python users. To simplify this, you can use the <code>get_workflow</code> utility:</p> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.matproj\")\n</code></pre>"},{"location":"full_guides/workflows/overview/#overview-of-classes","title":"Overview of Classes","text":""},{"location":"full_guides/workflows/overview/#what-is-a-workflow","title":"What is a <code>Workflow</code>?","text":"<p>As explained in Simmate's getting-started tutorial, a <code>Workflow</code> consists of 4 stages:</p> <ul> <li><code>configure</code>: Sets the calculation settings (like VASP's INCAR settings)</li> <li><code>schedule</code>: Determines whether to run the workflow immediately or queue it for later (e.g., SLURM, PBS, or remote computers)</li> <li><code>execute</code>: Writes input files, runs the calculation (e.g., VASP), and checks the results for errors</li> <li><code>save</code>: Stores the results in our database</li> </ul> <p>The <code>configure</code> step defines a workflow. Pre-built workflows are already configured, but you can create a custom workflow for more flexibility. </p> <p>The <code>schedule</code> step is managed by Simmate. You only need to know that <code>run</code> executes the workflow on your local computer, while <code>run_cloud</code> schedules it to run remotely.</p> <p>The <code>execute</code> step is the core of the workflow. It can include various tasks. The most common type of workflow in Simmate, known as a \"S3Workflow\", is explained below.</p> <p>The <code>save</code> step takes the result of the <code>execute</code> step and saves it to a SQL database. This is automated for common workflows like relaxations, dynamics, or static-energy calculations. However, advanced users may want to customize their methods.</p> <p>All stages of a <code>Workflow</code> are executed through the <code>run</code> or <code>run_cloud</code> methods. In other words, <code>Workflow.run</code> = <code>configure</code> + <code>schedule</code> + <code>execute</code> + <code>save</code>.</p> <p>To start building custom workflows, complete the getting-started tutorials and review the <code>simmate.engine.workflow</code> documentation.</p>"},{"location":"full_guides/workflows/overview/#what-is-a-nestedworkflow","title":"What is a <code>NestedWorkflow</code>?","text":"<p>A \"nested\" workflow is a workflow composed of multiple other workflows. For instance, the <code>relaxation.vasp.staged</code> workflow includes a series of relaxations of increasing quality followed by a final energy calculation.</p>"},{"location":"full_guides/workflows/overview/#what-is-an-s3workflow","title":"What is an <code>S3Workflow</code>?","text":"<p>An <code>S3Workflow</code> involves writing input files, calling an external program, and reading the output files. The term \"S3\" stands for Supervised, Staged, and Shell call. Here's what each term means:</p> <ul> <li><code>Staged</code>: The calculation consists of three stages (each is a class method)<ol> <li><code>setup</code> = write input files</li> <li><code>run_command_and_monitor</code> = run &amp; monitor the program</li> <li><code>workup</code> = read the output files</li> </ol> </li> <li><code>Shell</code>: The program is called through the command-line (the actual <code>execution</code> call)</li> <li><code>Supervised</code>: Simmate monitors the shell command for errors in the background (occurs during the <code>execution</code> call)</li> </ul> <p>All stages of an S3 workflow are included in the <code>execute</code> step of a <code>Workflow</code>, where Simmate provides extensive built-in functionality.</p> <p>To build a custom S3 workflow, we recommend going through: 1. getting-started guides 2. <code>simmate.engine.workflow</code> documentation 3. <code>simmate.engine.s3_workflow</code> documentation</p>"},{"location":"full_guides/workflows/s3_workflows/","title":"Overview of Supervised-Staged-Shell Workflow","text":""},{"location":"full_guides/workflows/s3_workflows/#introduction-to-s3-supervised-staged-shell","title":"Introduction to S3: Supervised, Staged, Shell","text":"<p>The S3 workflow is designed to supervise a staged workflow that involves a shell command.</p> <p>A shell command is a single call to an external program. For example, the \"vasp_std &gt; vasp.out\" command is used to run a calculation in VASP. This call is considered a staged task, which includes three steps:</p> <ul> <li>Setup: Writing necessary input files for the program.</li> <li>Execute: Running the program by calling the command.</li> <li>Workup: Loading data from output files back into Python.</li> </ul> <p>Supervising the task involves monitoring the program during the execution stage. Simmate can check output files for common errors or issues while the program is running. If an error is detected, the program is stopped, the issue is fixed, and the program is restarted.</p> <pre><code>graph LR\n  A[Start] --&gt; B[setup];\n  B --&gt; C[execute];\n  C --&gt; D[workup];\n  D --&gt; E[Has errors?];\n  E --&gt;|Yes| B;\n  E --&gt;|No| F[Done!];</code></pre> <p>Warning</p> <p>The diagram is slightly misleading as the \"Has Errors?\" check also occurs while the execute step is still running. This allows for early error detection before the program completes its run.</p> <p>Running S3Workflows is similar to running normal workflows (e.g., using the <code>run</code> method), and the entire process of supervision, staging, and shell execution is automated.</p>"},{"location":"full_guides/workflows/s3_workflows/#implementing-s3workflows-with-common-programs","title":"Implementing S3Workflows with Common Programs","text":"<p>For commonly used material science programs, refer to their guides in the \"Third-party Software\" section. If your program is listed there, a subclass of <code>S3Workflow</code> is likely already available. For instance, VASP users can use <code>VaspWorkflow</code> to build workflows.</p>"},{"location":"full_guides/workflows/s3_workflows/#building-a-custom-s3workflow","title":"Building a Custom S3Workflow","text":"<p>Tip</p> <p>Before creating a custom <code>S3Workflow</code>, ensure you've read the section on S3Workflows for common programs like VASP and the guides on building a custom <code>Workflow</code>.</p>"},{"location":"full_guides/workflows/s3_workflows/#simple-command-call","title":"Simple Command Call","text":"<p>The simplest example of an S3Workflow is a single command call without any additional actions (no input files, no error handling, etc.). </p> <p>Unlike custom <code>Workflows</code> where we define a <code>run_config</code> method, <code>S3Workflows</code> have a pre-built <code>run_config</code> method that handles the different stages and monitoring of a workflow. </p> <p>For instance, let's use the <code>echo</code> command to print something:</p> <pre><code>from simmate.engine import S3Workflow\n\nclass Example__Echo__SayHello(S3Workflow):\n    use_database = False  # no custom table for now\n    monitor = False  # no error handling yet\n    command = \"echo Hello\"\n\n# behaves like a normal workflow\nstate = Example__Echo__SayHello.run()\nresult = state.result()\n</code></pre> <p>Tip</p> <p>Note the use of \"Echo\" in our workflow name. This helps users understand what commands or programs will be run when a workflow is executed.</p>"},{"location":"full_guides/workflows/s3_workflows/#custom-setup-and-workup","title":"Custom Setup and Workup","text":"<p>If you need to write input files or read output files, you'll need to update your <code>setup</code> and <code>workup</code> methods:</p> <pre><code>from simmate.engine import S3Workflow\n\nclass Example__Echo__SayHello(S3Workflow):\n\n    use_database = False  # no custom table for now\n    monitor = False  # no error handling yet\n\n    command = \"echo Hello &gt; output.txt\"  # adds \"Hello\" into a new file\n\n    @classmethod\n    def setup(cls, directory, custom_parameter, **kwargs):\n        # The directory given is a pathlib.Path object for the directory\n        # that the command will be called in\n\n        print(\"I'm setting things up!\")\n        print(f\"My new setting value is {cls.some_new_setting}\")\n        print(f\"My new parameter value is {custom_parmeter}\")\n\n        return  # no need to return anything. Nothing will be done with it.\n\n    @staticmethod\n    def workup(directory):\n        # The directory given is a pathlib.Path object for the directory\n        # that the command will be called in\n\n        # Simply check that we have a new file\n        output_file = directory / \"output.txt\"\n        assert output_file.exists()\n\n        print(\"I'm working things up!\")\n        return \"Done!\"\n\ntask = Example__Echo__SayHello()\nresult = task.run()\n</code></pre> <p>Note:</p> <ol> <li>Writing new <code>setup</code> or <code>workup</code> methods is optional. If you do...<ul> <li>Both <code>setup</code> and <code>workup</code> methods should be either a staticmethod or classmethod.</li> <li>Custom <code>setup</code> methods require the <code>directory</code> and <code>**kwargs</code> input parameters.</li> <li>Custom <code>workup</code> methods require the <code>directory</code> input parameter.</li> </ul> </li> <li>Setting/overwriting attributes is optional. You can also add new ones.</li> </ol> <p>S3Workflows for commonly used programs (like <code>VaspWorkflow</code> for VASP) often have custom <code>setup</code> and <code>workup</code> methods already defined. You can modify these as needed.</p> <p>For a comprehensive example of a subclass, refer to <code>simmate.apps.vasp.workflows.base.VaspWorkflow</code> and the tasks that use it like <code>simmate.apps.materials_project.workflows.relaxation.matproj</code>.</p>"},{"location":"full_guides/workflows/s3_workflows/#custom-error-handling","title":"Custom Error Handling","text":"<p>Custom error handling is currently under development. Please contact our team if you need this guide prioritized.</p>"},{"location":"full_guides/workflows/s3_workflows/#alternatives-to-s3workflow","title":"Alternatives to S3Workflow","text":"<p>For advanced users, the S3Workflow class combines the functionality of Prefect's ShellTask, a Custodian Job, and Custodian monitoring. When subclassing this, we can also incorporate functionality from <code>pymatgen.io.vasp.sets</code>. By merging these into one class, we simplify the process for users and task creation.</p>"},{"location":"full_guides/workflows/using_existing_workflows/","title":"Using Existing Workflows","text":""},{"location":"full_guides/workflows/using_existing_workflows/#finding-available-workflows","title":"Finding Available Workflows","text":"<p>To view all registered workflows, use the following command-line: <pre><code>simmate workflows list-all\n</code></pre></p> <p>For detailed information about a specific workflow, use the <code>explore</code> command: <pre><code>simmate workflows explore\n</code></pre></p> <p>Tip</p> <p>For a thorough understanding of the various workflows, refer to the \"Workflow Names\" section.    </p>"},{"location":"full_guides/workflows/using_existing_workflows/#loading-a-workflow","title":"Loading a Workflow","text":"<p>After identifying a workflow, you can load it as follows:</p> yamlpython <pre><code># in example.yaml\nworkflow_name: static-energy.vasp.matproj\n</code></pre> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.matproj\")\n</code></pre>"},{"location":"full_guides/workflows/using_existing_workflows/#understanding-parameters-options","title":"Understanding Parameters &amp; Options","text":"<p>Parameters are essential for using Simmate. We've dedicated a section in our documentation to them. Please familiarize yourself with this section for detailed parameter descriptions and examples.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#running-a-workflow-local","title":"Running a Workflow (Local)","text":"<p>To execute a workflow on your local machine, use the <code>run</code> method. Here's an example:</p> command linepython <p><pre><code># in example.yaml\nworkflow_name: static-energy.vasp.matproj\nstructure: NaCl.cif\ncommand: mpirun -n 4 vasp_std &gt; vasp.out\n</code></pre> <pre><code>simmate workflows run example.yaml\n</code></pre></p> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.matproj\")\n\nstate = workflow.run(\n    structure=\"NaCl.cif\", \n    command=\"mpirun -n 4 vasp_std &gt; vasp.out\",\n)\n</code></pre>"},{"location":"full_guides/workflows/using_existing_workflows/#running-a-workflow-cloud","title":"Running a Workflow (Cloud)","text":"<p>Workflows can also be executed on a remote cluster. It's important to understand the differences between local and cloud runs:</p> local (run)remote submission (run-cloud) <pre><code>graph TD\n  A[submit with 'run' command] --&gt; B[starts directly on your local computer &amp; right away];</code></pre> <pre><code>graph TD\n  A[submit with 'run-cloud' command] --&gt; B[adds job to scheduler queue];\n  B --&gt; C[waits for a worker to pick up job];\n  C --&gt; D[worker selects job from queue];\n  D --&gt; E[runs the job where the worker is];\n  F[launch a worker with 'start-worker' command] --&gt; D;</code></pre> <p>To schedule a workflow to run on a remote cluster, ensure your computational resources are configured. Then, use the <code>run_cloud</code> method:</p> command linepython <p><pre><code># in example.yaml\nworkflow_name: static-energy.vasp.matproj\nstructure: NaCl.cif\ncommand: mpirun -n 4 vasp_std &gt; vasp.out\n</code></pre> <pre><code>simmate workflows run-cloud example.yaml\n</code></pre></p> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.matproj\")\n\nstate = workflow.run_cloud(\n    structure=\"NaCl.cif\", \n    command=\"mpirun -n 4 vasp_std &gt; vasp.out\",\n)\n</code></pre> <p>Warning</p> <p>The <code>run-cloud</code> command/method only schedules the workflow. It won't  run until you add computational resources (or <code>Workers</code>). To do this, you must read through the \"Computational Resources\" documentation.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#accessing-results","title":"Accessing Results","text":"<p>There are several methods to view the results of a workflow run, and some may be more suitable than others.</p> <p>Tip</p> <p>If you plan to run multiple workflows and want the results in an Excel spreadsheet, then option 3 is for you!</p>"},{"location":"full_guides/workflows/using_existing_workflows/#option-1-output-files","title":"Option 1: Output Files","text":"<p>Navigate to the directory where the calculation was run. You'll find a few additional files in your output. One of them is <code>simmate_summary.yaml</code>, which  provides quick information.</p> <p>Simmate can also identify errors, correct them, and retry a calculation. If this  occurred during your workflow run, you'll see a file named  <code>simmate_corrections.csv</code> detailing the errors encountered and how they were resolved.</p> <p>Other workflows will also generate plots for you. For example,  <code>electronic-structure</code> workflows will calculate a band structure using Materials Project settings, and write an image of your final band structure to  <code>band_structure.png</code>. These extra files and plots vary for each workflow,  but they make checking your results quick and easy.</p> <p>Tip</p> <p>While the plots and summary files are useful for quick viewing, there is much  more information available in the database. Furthermore, you can also use python toolkit objects to run a custom analysis. These are covered in the  next two sections.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#option-2-python-objects","title":"Option 2: Python Objects","text":"<p>When running a workflow in python, a <code>State</code> object is returned. From this, you can access the results as <code>toolkit</code> objects. States allows you to check if the run completed successfully or not. Then final output of your workflow run can be accessed using <code>state.result()</code>. The <code>State</code> is based off of Prefect's state object, which you can read more about  here. We use <code>State</code>s because  the status of a run becomes important when we start scheduling runs to run remotely, and more importantly, it allows use to building in compatibility with other workflow engines like Prefect.</p> python (local)python (cloud) <pre><code>state = workflow.run(...)\n\nresult = state.result()\n</code></pre> <pre><code>state = workflow.run_cloud(...)\n\n# This will block and wait for the job to finish\nresult = state.result()\n</code></pre> <p>Tip</p> <p>This approach is best for users comfortable with python. If you want to use these features, we recommend reading through the <code>Toolkit</code> guides.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#option-3-the-database","title":"Option 3: The Database","text":"<p>You can also view results database through the <code>database_table</code> attribute  (if one is available). This returns a Simmate database object for results of  ALL runs of this workflow. But as an example:</p> python <pre><code>table = workflow.database_table\n\n# pandas dataframe that you can view in Spyder\ndf = table.objects.to_dataframe()\n\n# or grab a specific run result and convert to a toolkit object\nentry = table.objects.get(run_id=\"example-123456\")\nstructure = entry.to_toolkit()\n</code></pre> <p>You'll notice the table gives results for all runs of this type (e.g. all static-energies). To limit your results to just this specific workflow, you can use the <code>all_results</code> property:</p> python <pre><code>results = workflow.all_results \n\n# the line above is a shortcut for...\ntable = workflow.database_table\nresults = table.objects.filter(workflow_name=workflow.name_full)\n</code></pre> <p>Tip</p> <p>Guides for filtering and manipulating the data in this table is covered  in the <code>Database</code> guides.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#option-4-the-website-server","title":"Option 4: The Website Server","text":"<p>In the <code>simmate_summary.yaml</code> output file, there is the <code>_WEBSITE_URL_</code>. You can copy/paste this URL into your browser and view your results in an interactive format. Just make sure you are running your local server first:</p> <pre><code>simmate run-server\n</code></pre> <p>Then open the link given by <code>_WEBSITE_URL_</code>:</p> <pre><code>http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1\n</code></pre> <p>Note</p> <p>Remember that the server and your database are limited to your local computer. Trying to access a URL on a computer that doesn't share the same database file will not work -- so you may need to copy your database file from the cluster to your local computer. Or even better -- if you would like to access results through the internet, then you have to switch to a cloud database.</p>"},{"location":"full_guides/workflows/using_existing_workflows/#running-massively-parallel-workflows","title":"Running Massively Parallel Workflows","text":"<p>Some workflows submit many subworkflows. For example, evolutionary structure prediction does this by submitting hundreds of individual structure relaxations, analyzing the results, and submitting new structures based on the results.</p> <p>This is achieved by the workflow manually calling <code>run-cloud</code> on others (see section above on how run-cloud works). If you start multiple workers elsewhere, you can calculate these subworkflows in parallel:</p> <pre><code>graph TD\n  A[main workflow];\n  A --&gt; B[subworkflow];\n  B --&gt; C[schedule run 1] --&gt; G[scheduler];\n  B --&gt; D[schedule run 2] --&gt; G;\n  B --&gt; E[schedule run 3] --&gt; G;\n  B --&gt; F[schedule run 4] --&gt; G;\n  G --&gt; H[worker 1];\n  G --&gt; I[worker 2];\n  G --&gt; J[worker 3];</code></pre> <p>To run these types of workflows, you must:</p> <ol> <li>Start the main workflow with the <code>run</code> command</li> <li>Start at least one worker that will run the submitted calculations</li> </ol> <p>Tip</p> <p>Make sure you read through the \"Computational Resources\" documentation. There is also a  full walk-through example of a massively-parallel workflow in the getting started guides.</p> <p>Note</p> <p>The number of workers will determine how many jobs are run in parallel -- and this is only limited by the number of jobs queued. For example, if I submit 500 workflows with <code>run-cloud</code> but only start 100 workers, then only 100 workflows will be run at a time. Further, if I submit 25 workflows but have 100 workers, then that means 75 of our workflows will be sitting idle without any job to run.</p>"},{"location":"full_guides/workflows/using_prefect/","title":"Prefect Backend (Experimental)","text":"<p>When you choose Prefect as your workflow executor, Workflows are internally converted into Prefect Flows. This makes understanding Prefect beneficial for handling complex scenarios. For advanced usage or when developing new features, we recommend going through the Prefect tutorials available here.</p>"},{"location":"full_guides/workflows/using_prefect/#minimal-example-prefect-vs-simmate","title":"Minimal Example (Prefect vs. Simmate)","text":"<p>It's useful to understand the comparison between Prefect and Simmate workflows. For simple scenarios involving Python code, a Prefect workflow is defined as follows:</p> <pre><code>from prefect import flow\n\n@flow\ndef my_favorite_workflow():\n    print(\"This workflow doesn't do much\")\n    return 42\n\n# Execute your workflow\nstate = my_favorite_workflow()\nresult = state.result()\n</code></pre> <p>To convert this into a Simmate workflow, we only need to slightly adjust the format. Instead of using a <code>@flow</code> decorator, we use the <code>run_config</code> method of a new subclass:</p> <pre><code># NOTE: This example does not follow Simmate's naming convention, \n# which may cause some higher-level features to not function correctly. This will be corrected in a subsequent step.\n\nfrom simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n\n    @staticmethod\n    def run_config(**kwargs):\n        print(\"This workflow doesn't do much\")\n        return 42\n\n# Execute your workflow\nstate = MyFavoriteWorkflow.run()\nresult = state.result()\n</code></pre> <p>Internally, the <code>run</code> method converts our <code>run_config</code> into a Prefect workflow. Methods like <code>run_cloud</code> will now automatically use Prefect.</p>"},{"location":"full_guides/workflows/workflow_names/","title":"Guidelines for Naming Workflows","text":""},{"location":"full_guides/workflows/workflow_names/#structure-for-naming-workflows","title":"Structure for Naming Workflows","text":"<p>Every workflow name follows the <code>type.app.preset</code> format:</p> <p><code>type</code>: Defines the type of analysis the workflow carries out (relaxation, static-energy, dynamics, etc.)</p> <p><code>app</code>: Denotes the third-party software used by the workflow (vasp, abinit, qe, deepmd, etc.)</p> <p><code>preset</code>: Gives a unique identifier for the settings applied (matproj, quality00, my-test-settings, etc.)</p> <p>Example</p> <p>Take the workflow <code>static-energy.vasp.matproj</code> as an example...</p> <ul> <li>type = static energy (performs a single point energy calculation)</li> <li>app = vasp (uses VASP for energy calculation)</li> <li>preset = matproj  (implements \"Materials Project\" settings)</li> </ul>"},{"location":"full_guides/workflows/workflow_names/#workflow-class-name-vs-mini-name","title":"Workflow Class Name vs. Mini Name","text":"<p>The <code>type.app.preset</code> format is standard, but in Python, the workflow class name is represented as <code>Type__App__Preset</code>. All workflows follow this structure.</p> <p>Example</p> <p><code>static-energy.vasp.matproj</code> is represented as <code>StaticEnergy__VASP__MatProj</code> in Python.</p> <p>Note that when converting a workflow name in Python, periods should be replaced with double underscores (<code>__</code>) and phrases should be converted to pascal case. The placement of hyphens (<code>-</code>) is determined by capital letters.</p>"},{"location":"full_guides/workflows/workflow_names/#finding-a-workflow-on-the-website-interface","title":"Finding a Workflow on the Website Interface","text":"<p>You can use the naming conventions described above to find a workflow on the website interface:</p> Template URLExample <pre><code>https://simmate.org/workflows/{TYPE}/{APP}/{PRESET}\n</code></pre> <pre><code>https://simmate.org/workflows/static-energy/vasp/matproj\n</code></pre>"},{"location":"full_guides/workflows/adding_computational_resources/overview/","title":"Overview","text":"<p> This module is intended as a simpler alternative to Prefect. It offers a stable, quick-start solution, but lacks the scalability and comprehensive features of Prefect.</p> <p>This module functions as an SQL executor, offering a streamlined version of FireWorks and Prefect. The scheduler is directly integrated with the Django database, removing the need to navigate firewalls or intricate setups. Any worker with database access can operate effectively. However, this configuration slows down the executor as each task requires multiple database calls and write operations. This trade-off between speed and stability is acceptable, as many Simmate workflows take longer than a minute, and the speed reduction is less than a second.</p> <p>Here's how to implement it:</p> <pre><code>from simmate.engine.execution.executor import SimmateExecutor\n\n# EXAMPLE 1\nfuture = SimmateExecutor.submit(sum, [4, 3, 2, 1])\nassert future.result() == 10\n\n# EXAMPLE 2\nimport time\n\ndef test():\n    futures = [executor.submit(time.sleep, 5) for n in range(10)]\n    return executor.wait(futures)\n\ntest()\n\n# ----------------------------------------------------------------------------\n\nfrom simmate.engine.execution.worker import SimmateWorker\n\nworker = SimmateWorker(waittime_on_empty_queue=1, tags=[])  # nitems_max=1\nworker.start()\n\n# ----------------------------------------------------------------------------\n</code></pre>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/","title":"Integrate with Prefect and Dask","text":"<p> We do not recommend this tutorial for users at the moment. This tutorial is for Prefect v1, but much of Simmate now depends on Prefect v2. As we adjust to the new backend, parts of this tutorial may be broken and recommended procedures are subject to change.  </p> <p>In this tutorial, you will learn how to run workflows on distributed computational resources -- with full scheduling and monitoring.</p> <ol> <li>The quick tutorial</li> <li>The full tutorial<ul> <li>A review of concepts</li> <li>Should I set up my own cluster?</li> <li>Setting up your scheduler with Prefect</li> <li>Setting up your cluster with Dask</li> <li>Connecting others to your scheduler</li> </ul> </li> </ol> <p> For beginners, this will be the most difficult part of setting up Simmate -- but it is entirely optional. Be sure to read the section on Should I set up my own cluster?. There are many ways to set up your resources and caviats to each (especially if you are using university or federal supercomputers). While python experts should be able to learn Prefect and Dask quickly, we strongly urge beginners to get advice from our team. If you struggle to follow along with this tutorial, post a question or email us directly (simmate.team@gmail.com).</p> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#the-quick-tutorial","title":"The quick tutorial","text":"<p> prefect, dask, and dask_jobqueue will be already installed for you because they are dependencies of Simmate</p> <ol> <li>Be aware that you can share a cloud database without sharing computational resources. This flexibility is very important for many collaborations. </li> <li>Just like with your cloud database, designate a point-person to manage your private computational resources. Everyone else can skip to step 9.</li> <li>Either sign in to Prefect Cloud (recommended) or setup a Prefect Server.</li> <li>Connect to your server with the following steps (this is from Prefect's tutorial):<ul> <li>On Prefect Cloud's homepage, go to <code>User</code> -&gt; <code>Account Settings</code> -&gt; <code>API Keys</code> -&gt; <code>Create An API Key</code></li> <li>Copy the created key</li> <li>set your Prefect backend with the command <code>prefect backend cloud</code></li> <li>tell Prefect your key with the command <code>prefect auth login --key example_key_h123j2jfk</code></li> </ul> </li> <li>Register all Simmate workflows with Prefect using the command <code>simmate engine setup-cloud</code></li> <li>Test that Prefect is configured properly with the following steps (this will run the workflow locally):<ul> <li>run the command <code>prefect agent local start</code> (note that this will run endlessly and submit all workflows in parallel. use <code>crtl+C</code> to stop)</li> <li>in a separate terminal, rerun our workflow from tutorial 2 with <code>run-cloud</code> instead of <code>run</code> (so <code>simmate workflows run-cloud relaxation_mit POSCAR</code>)</li> </ul> </li> <li>Set up your computational resources using Dask (and if needed, Dask JobQueue). There are MANY options for this, which are covered in the <code>simmate.engine</code> module. Take the time to read the documentation here. But as an example, we'll set up a SLURM cluster and then link it to a Prefect agent. Note, if you want to run workflows with commands like <code>mpirun -n 18 vasp_std &gt; vasp.out</code>, then limit the Dask worker to one core while having the SLURM job request more cores. The resulting python script will look something like this: <pre><code># --------------------------------------------------------------------------------------\n\n# STEP 1: Configure our Dask Cluster.\n\nfrom dask_jobqueue import SLURMCluster\n\ncluster = SLURMCluster(\n    #\n    # General options\n    scheduler_options={\"port\": 8786},\n    local_directory=\"~\",\n    #\n    #\n    # Dask Worker Options\n    cores=1,\n    processes=1,\n    memory=\"4GB\",\n    # REQUIRED: Make sure you preload this script!\n    extra=[\"--preload simmate.configuration.dask.connect_to_database\"],  \n    #\n    #\n    # SLURM Job Options\n    job_cpu=18,\n    job_mem=\"50GB\",\n    job_extra=[\n        \"--output=slurm-%j.out\",\n        \"-N 1\",\n    ],\n    walltime=\"300-00:00:00\",\n    queue=\"p1\",  # this is the name of the SLURM queue/partition\n    env_extra=[\"module load vasp;\"],  # our workflow requires the vasp module to be loaded\n)\n\n# Scale the cluster to the number of SLURM jobs that you'd like\ncluster.scale(10)\n\n# --------------------------------------------------------------------------------------\n\n# STEP 2: Configure our Prefect Agent and start submitting workflows!\n\nfrom prefect.agent.local import LocalAgent\nfrom simmate.configuration.prefect.connect_to_dask import set_default_executor\n\n# We want Prefect to use our Dask Cluster to run all of the workflow tasks. To\n# tell Prefect to do this, we wrote a helper function that ships with Simmate.\nset_default_executor(cluster.scheduler.address)\n\n# Start our cluster! Our cluster's name is Warwulf, so we use that here.\nagent = LocalAgent(\n    name=\"WarWulf\",\n    labels=[\"WarWulf\"],\n)\n\n# Now we can start the Prefect Agent which will run and search for jobs and then\n# submit them to our Dask cluster.\nagent.start()\n# NOTE: this line will run endlessly unless you set a timelimit in the LocalAgent above\n\n# --------------------------------------------------------------------------------------\n</code></pre></li> <li>Test out your cluster by running <code>simmate workflows run-cloud relaxation_mit POSCAR</code> in a separate terminal (submit this a bunch if you'd like to). If you'd like to limit how many workflows of a given tag (e.g. \"WarWulf\" above) run in parallel, set the concurrency limit in Prefect cloud here.</li> <li>To let others use your cluster, simply add them to your Prefect Cloud and give them an API key. They just need to do the following:<ul> <li>set your Prefect backend with the command <code>prefect backend cloud</code></li> <li>tell Prefect your key with the command <code>prefect auth login --key example_key_h123j2jfk</code></li> <li>try submitting a workflow with <code>simmate workflows run-cloud relaxation_mit POSCAR</code></li> </ul> </li> </ol> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#the-full-tutorial","title":"The full tutorial","text":""},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#a-review-of-concepts","title":"A review of concepts","text":"<p>Recall from tutorial 2, there are 4 steps to a workflow: - <code>configure</code>: chooses our desired settings for the calculation (such as VASP's INCAR settings) - <code>schedule</code>: decides whether to run the workflow immediately or send off to a job queue (e.g. SLURM, PBS, or remote computers) - <code>execute</code>: writes our input files, runs the calculation (e.g. calling VASP), and checks the results for errors - <code>save</code>: saves the results to our database</p> <p>This tuturial will give an overview of how to modify the <code>schedule</code> and determine which computer <code>execute</code> is called on. Up until now, we have been using the default behavior for these two steps. But now we want to instead do the following: - <code>schedule</code>: submits the workflow to a scheduler queue of many other workflows - <code>execute</code>: run the calculation on a remote cluster</p> <p>A scheduler is something we submit workflows to and controls when to run them. As a bunch of workflows are submitted, our scheduler forms a queue and keeps track of which ones to run next. We will use Prefect as our scheduler. </p> <p>A cluster is a group of computational resources that actually run the workflows. So our scheduler will find whichever workflow should be ran next, and send it to our cluster to run. Clusters are often made up of \"workers\" -- where a worker is just a single resource and it works through one job at a time. For example, if we had a cluster made up of 10 desktop computers, each computer would run a workflow and once finished ask the scheduler for the next workflow to run. At any given time, 10 workflows will be running. We'll use Dask to set up our cluster, whether your resources are on the cloud, a supercomputer, or just simple desktops.</p> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#should-i-set-up-my-own-cluster","title":"Should I set up my own cluster?","text":"<p>Before we start... We understand that resource restrictions can be very stingent between labs and companies. So sharing resources is not possible, even in close collaborations. Simmate addresses this issue by making it easy to share a cloud database without sharing computational resources. In other words, you can contribute to a shared database without letting others see/access your computational resources.</p> <p>With that said, each team will likely need to handle their own computational resources, which can be any number of things: - a university or federal HPC cluster with SLURM, PBS, or some other queue system - a single node or even a Kubernetes cluster provided by a commercial service like DigitalOcean, GoogleCloud, etc. - a series of desktop computers that your lab shares - any combination of these resources</p> <p>The easiest way to use these resources is to sign on and run simmate directly on it. When this is done, the workflow runs directly on your resource and it will run there immediately. We've already see this with Tutorial 2 when we called <code>simmate workflows run ...</code>. Just use that on your new resource to start! (for help signing on to remote supercomputers and installing anaconda, be sure to ask the cluster's IT team).</p> <p>As another example, you can submit a workflow to a SLURM cluster with a submit.sh file like this: <pre><code>#!/bin/bash\n\n#SBATCH --output=slurm.out\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n\n# make sure you have you activated your conda enviornment \n# and required modules before submitting\n\nsimmate workflows run-cloud relaxation_mit POSCAR &gt; simmate.out\n</code></pre></p> <p> If you are only running a few workflows per day (&lt;10), we recommend you stick to running workflows in this way. That is, just calling <code>simmate workflows run</code>. Don't overcomplicate things. </p> <p>Alternatively, if your team is submitting hundreds or thousands of workflows at a time, then it would be extremely useful to monitor and orchestrate these workflows. To do this, we will use Prefect and Dask. Just like with our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources have been set up, the other users can connect using API key (which is essentially a username+password).</p> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#setting-up-your-scheduler-with-prefect","title":"Setting up your scheduler with Prefect","text":"<p>The first half of this section is just a replica of Prefect's tutorial, but rewritten in a condensed format. If you prefer, you can use their tutorial instead: Introduction to Prefect Orchestration</p> <p>With Prefect as our scheduler, we have two options on how to set this up: Prefect Cloud or Prefect Server. They give a guide on which to choose here, but to summarize: - Prefect Server is free and open-source, but it requires you set up a server and manage it independently - Prefect Cloud is free for your first 20,000 workflow tasks each month (here's their pricing page) and all set up for you</p> <p>Our team uses Prefect Cloud and recommends that beginners do the same. Unless you are running dozens of [evolutionary searches] per month, you won't come close to the 20,000 workflow task-run limit. This tutorial will continue with Prefect Cloud, but if you decide on Prefect Server, you'll have to set that up before continuing.</p> <p>Go ahead and sign in to Prefect Cloud (use your github account if you have one!). Everything will be empty to start, so we want to add all of Simmate's workflows to our interface here. To do this, we need an API key, which acts like a username and password. It's how we tell python that we now have a Prefect account.</p> <p>To get your API key, go to Prefect Cloud's homepage and navigate to <code>User</code> -&gt; <code>Account Settings</code> -&gt; <code>API Keys</code> -&gt; <code>Create An API Key</code>. Copy the created API key.</p> <p>On your computer, open a terminal and make sure you have your conda enviornment active. Prefect is already installed because the installation of Simmate did it for us. So run the command: <code>prefect backend cloud</code>. This just tells prefect that we decided to use their scheduler. Next, run the command <code>prefect auth login --key example_key_h123j2jfk</code> where you replace the end text with your copied API KEY. We can now use Simmate to access your cloud and submit workflows for us!</p> <p>To get started, we need to add all of our workflows to Prefect. This is done with <code>simmate engine setup-cloud</code>. After running this, you should see all of the workflow in your Prefect Cloud now! </p> <p>If you were to use the <code>simmate workflows run</code> command that we've been using, you'll notice it still runs directly on your computer. To instead submit it to Prefect Cloud, use the command  <code>simmate workflows run-cloud</code> instead. So for an example workflow, the full command would be... <code>simmate workflows run-cloud relaxation_mit POSCAR</code>. </p> <p>When you use <code>run-cloud</code>, the workflow run shows up in your Prefect Cloud, but it's not running. That is because we need a Prefect \"Agent\" -- an Agent simply checks for workflows that need to be ran and submits them to a cluster of computational resources. Let's start a Prefect Agent with all default settings (no cluster is attached yet), which means it will simply run the workflow on whichever computer the agent is on. To do this, run the command <code>prefect agent local start</code>. You'll see it pick up the workflow we submitted with <code>run-cloud</code> and run it. Even after the job completes, the agent will continue to run. So if you submit a new workflow, it will run it as well.</p> <p>We skimmed over a lot of the fundamentals for Prefect here, so we highly recommend going through Prefect's guides and tutorials. Spending a day or two on this will save you a lot of headache down the road.</p> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#setting-up-your-cluster-with-dask","title":"Setting up your cluster with Dask","text":"<p>Because there is such a diverse set of computational resources that teams can have, we can't cover all setup scenarios in this tutorial. Instead, we will go through some examples of submitting Simmate workflows to a Dask cluster. This will all be on your local computer. For switching to remote resources (and job queue clusters like SLURM), we can only point you to key tutorials and documentation on how to set up your cluster. You can ask our team which setup is the best fit for your team, and we'll try to guide you through the process. Setting up your cluster shouldn't take longer than a hour, so post a question if you're struggling!</p> <p>Here are some useful resources for setting up a cluster with Dask. We recommend going through these before trying to use Dask with Simmate: - Introduction to Dask Futures     - This is the best tutorial to start with! Then go through their example. Dask can do a lot, but Simmate only really uses this feature. If you understand how to use the <code>client.submit</code>, then you understand how Simmate is using Dask   - Introduction to Dask Jobqueue     - If you use a queue system like PBS, Slurm, MOAB, SGE, LSF, and HTCondor, then this will show you how to set up a cluster on your resource.</p> <p>Here's a simple example of using Dask to run a function that \"sleeps\" for 1 second <pre><code>import time\n\n# This loop will take 60 seconds to complete\nfor n in range(60):  # run this 60 times\n    time.sleep(1)  # sleeps 1 second\n\n# Now we switch to using Dask\nfrom dask.distributed import Client\n\nclient = Client()\n\n# Futures are basically our \"job_id\". They let us check the status and result\nfutures = []\nfor n in range(60):  # run this 60 times\n    # submits time.sleep(1) to Dask. pure=False tells Dask to rerun each instead\n    # of loading past results from each time.sleep(1). \n    future = client.submit(time.sleep, 1, pure=False) \n    futures.append(future)\n\n# now wait for all the jobs to finish\n# This will take much less than 60 seconds!\nresults = [future.result() for future in futures]\n</code></pre></p> <p>We'll now try using Dask to run our Simmate workflows.</p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-serial-one-item-at-time","title":"In serial (one item at time)","text":"<p>Let's start with how we've been submitting workflows: using <code>workflow.run</code>. This runs the workflow immediately and on your local computer. If we were to run a workflow of many structures, only one workflow would run at a time. Once a workflow completes, it moves on to the next one:</p> <pre><code>from simmate.workflows import example_workflow\n\nfor structure in structures:\n    result = example_workflow.run(structure=structure)\n</code></pre>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-parallel-many-workflows-at-once","title":"In parallel (many workflows at once)","text":"<p>To use your entire computer and all of its CPUs, we can set up a Dask cluster. There's one added thing you need to do though -- and that's make sure all of Dask workers are able to connect to the Simmate database. This example let's you submit a bunch of workflows and multiple workflows can run at the same time. You can do this with...</p> <pre><code># first setup Dask so that it can connect to Simmate\nfrom dask.distributed import Client\nclient = Client(preload=\"simmate.configuration.dask.connect_to_database\")\n\n# now submit your workflows and \nfutures = []\nfor structure in structures:\n    future = client.submit(example_workflow.run, structure=structure)\n\nresult = future.result()\n\n# wait for all workflows to finish\n# you can monitor progress at http://localhost:8787/status\n</code></pre>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#in-parallel-many-tasks-from-a-single-workflow-at-once","title":"In parallel (many tasks from a single workflow at once)","text":"<p>But what if you want to run a single workflow with all of it's tasks in parallel? To do that, we use...</p> <pre><code># first setup Dask so that it can connect to Simmate\nfrom dask.distributed import Client\nclient = Client(preload=\"simmate.configuration.dask.connect_to_database\")\n\n# Tell Prefect that we should submit each task to Dask\nfrom prefect.executors import DaskExecutor\nexample_workflow.executor = DaskExecutor(address=client.scheduler.address)\n\n# now run your workflow and wait for it to finish\nresult = example_workflow.run()\n</code></pre>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#connecting-your-dask-cluster-to-your-prefect-agent","title":"Connecting your Dask cluster to your Prefect Agent","text":"<p>Once you learned how to set up your Dask cluster, the next step is tell Prefect where it is. You can always write your python script as two steps:</p> <ol> <li>Configure and then start your Dask Cluster.</li> <li>Configure and then start your Prefect Agent, which will start submitting workflows!</li> </ol> <p>In the future, we hope to have more convenient methods through the command-line, but these features are not complete yet. </p> <p></p>"},{"location":"getting_started/_Integrate_with_Prefect_and_Dask/#connecting-others-to-your-scheduler","title":"Connecting others to your scheduler","text":"<p>Once you've set up your scheduler with Prefect and cluster with Dask, your team members simply need to connect to Prefect Cloud to submit their workflows. Generate an API key for them. Then they can complete step 9 of the quick tutorial (above).</p> <p>Just remember... <code>workflow.run()</code> in python and <code>simmate workflows run</code> in the command-line will still run the workflow locally and right away. <code>workflow.run_cloud()</code> in python and <code>simmate workflows run-cloud</code> in the command-line will submit your workflow to Prefect Cloud.</p>"},{"location":"getting_started/overview/","title":"Getting Started Tutorials","text":"<p>Welcome!  These tutorials will help introduce you to Simmate, whether you're new to coding or a python expert.</p>"},{"location":"getting_started/overview/#expected-duration","title":"Expected duration","text":"<p>Python Experts: Tutorials 1-4 will take under 1 hour total, while tutorials 5-8 (optional) will take 2-3 hours total</p> <p>Beginners: If you're new to coding, we suggest you dedicate one day (1-2 hrs) per tutorial in order to fully grasp the concepts introduced in each. Please note that tutorials 5-8 cover advanced topics and are entirely optional.</p>"},{"location":"getting_started/overview/#getting-help","title":"Getting help","text":"<p>We recommend creating a GitHub account before starting the tutorials, if you don't have one already. While not mandatory, having an account will enable you to ask questions on our forum and log in to our website.</p> <p>  Click the buttons below to get started! </p>"},{"location":"getting_started/wrap_up/","title":"You did it!","text":"<p>If you made it to this point, you're pretty much a Simmate expert! </p> <p>Any other guides and tutorials will be in the full-guides section. </p> <p>We hope you see the potential Simmate has to offer the larger materials community! With a powerful framework like Simmate in hand, anything is possible. In other words...</p> <p>\"The ceiling is the roof\" -Michael Jordan </p> <p>Have fun coding and always be sure to ask for help/feedback when you need it.</p> <p> </p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/","title":"Configuring Your Cluster and Workers","text":""},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#switching-from-run-to-run-cloud","title":"Switching from Run to Run-cloud","text":"<p>Transitioning a workflow to a schedule is simple. All you need to do is replace all your scripts and commands from the <code>run</code> method to the <code>run_cloud</code> method. For instance, if you're using the command line:</p> command linepython <pre><code>simmate workflows run-cloud my_settings.yaml\n</code></pre> <pre><code>workflow.run_cloud(...)\n</code></pre> <p>This action schedules your workflow, but it doesn't initiate it. It merely places it in the queue, waiting for a worker to execute it. The workflow will only run once a worker is started.</p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#initiating-a-single-flow-worker","title":"Initiating a \"single-flow\" worker","text":"<p>To run the workflow, start a worker with the following command: <pre><code>simmate engine start-singleflow-worker\n</code></pre></p> <p>Danger</p> <p>If you're using a cluster, the start-worker command should be executed within your submit script (for example, inside <code>submit.sh</code> for SLURM). Avoid running workers on the head node!  </p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#initiating-a-many-flow-worker","title":"Initiating a \"many-flow\" worker","text":"<p>When you initiate this \"singleflow\" worker, you'll observe that the Worker starts, executes one workflow, then shuts down. This method is recommended for HPC clusters as it adheres to best practices for resource sharing. It prevents a worker from monopolizing computational resources when there are no scheduled workflows to run! </p> <p>However, if you want more control over the number of workflows run or even to run a worker indefinitely, use the following command: <pre><code>simmate engine start-worker\n</code></pre></p> <p>If your team frequently runs many short workflows that take less than 5 minutes, constantly starting and stopping workers can be inconvenient (sometimes it can take simmate up to 10 seconds to set everything up). This can lead to significant overhead and wasted computation time. </p> <p>To mitigate this, you can run a worker that shuts down after executing 10 workflows or when the queue is empty: <pre><code>simmate engine start-worker --nitems-max 10 --close-on-empty-queue\n</code></pre></p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#initiating-multiple-workers-simultaneously","title":"Initiating Multiple Workers Simultaneously","text":"<p>If you need to initiate multiple workers at once, you can use the <code>start-cluster</code> command as well. <pre><code># starts 5 local workers\nsimmate engine start-cluster 5\n</code></pre></p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#managing-the-workflows-run-by-each-worker","title":"Managing the Workflows Run by Each Worker","text":"<p>Warning</p> <p>The comprehensive guide for custom workers is currently under development. Refer to the workflow \"tags\" in the full guides for more details.</p>"},{"location":"getting_started/add_computational_resources/adding_clusters_and_workers/#allowing-others-to-connect-to-your-scheduler","title":"Allowing Others to Connect to Your Scheduler","text":"<p>If others are connected to your database, they're all set! Other schedulers like Prefect or Dask may require additional setup.</p>"},{"location":"getting_started/add_computational_resources/checklist_for_workers/","title":"Worker Setup Checklist","text":"<p>Now that you're familiar with the terms scheduler, cluster, and worker, let's walk through the checklist to set everything up:</p> <ul> <li> Configure your scheduler</li> <li> Connect a cloud database</li> <li> Establish a connection to the scheduler</li> <li> Register all custom projects/apps</li> </ul>"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#1-the-scheduler","title":"1. The Scheduler","text":"<p>If you're using the \"SimmateExecutor\", you're all set! No additional setup is required. This is because the job submission queue is essentially a database table within the simmate database. Workers will queue this table and pick up the first result.</p>"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#2-connecting-to-a-cloud-database","title":"2. Connecting to a Cloud Database","text":"<p>As we've learned from previous tutorials, simmate, by default, writes results to a local file named <code>~/simmate/my_env-database.sqlite3</code>. Cloud databases, on the other hand, allow multiple computers to share data and access the database via an internet connection. Since SQLite3 (the default database engine) isn't designed for hundreds of connections and we often use separate computers to run workflows, it's advisable to set up a cloud database. Don't miss tutorial 08 where we guide you through this process!</p>"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#3-connecting-to-the-scheduler","title":"3. Connecting to the Scheduler","text":"<p>With the \"SimmateExecutor\", all you need is a connection to the cloud database. Ensure that ALL your computational resources are connected to the configured cloud database. If your workers aren't picking up any workflow submissions, it's likely due to an improper connection.</p>"},{"location":"getting_started/add_computational_resources/checklist_for_workers/#4-connecting-custom-projects","title":"4. Connecting Custom Projects","text":"<p>If you have custom database tables or code, it's crucial that (a) the cloud database is aware of these tables and (b) your remote resources can access your custom code. Therefore, your custom project/app should be installed and accessible on all your computation resources. Remember to <code>pip install your-cool-project</code> on all computers.</p> <p>Tip</p> <p>The SimmateExecutor uses cloudpickle when submitting tasks, so many custom workflows will function without this step. We're still figuring out the best way to guide users through this process. For now, we recommend testing your workflow even if you skip this step -- it often works. If it doesn't, the above text explains why.</p>"},{"location":"getting_started/add_computational_resources/intro_to_clusters/","title":"Introduction to Engine Concepts","text":""},{"location":"getting_started/add_computational_resources/intro_to_clusters/#overview","title":"Overview","text":"<p>In a previous tutorial, we discussed the four steps of a workflow: <code>configure</code>, <code>schedule</code>, <code>execute</code>, and <code>save</code>.</p> <p>This tutorial will focus on how to modify the <code>schedule</code> and determine the computer on which <code>execute</code> is called. Until now, we have been using the default behavior for these two steps. However, we will now explore how to:</p> <ul> <li><code>schedule</code>: Submit the workflow to a queue containing multiple workflows.</li> <li><code>execute</code>: Run the calculation on a remote cluster.</li> </ul>"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#visualizing-our-setup","title":"Visualizing Our Setup","text":"<p>The schematic below will aid in understanding the concepts discussed in this tutorial. Take a moment to familiarize yourself with the organization of our resources and refer back to this schematic as needed.</p> General SetupExample (The Warren Lab) <pre><code>graph TD;\n    A[user 1]--&gt;E[scheduler];\n    B[user 2]--&gt;E;\n    C[user 3]--&gt;E;\n    D[user 4]--&gt;E;\n    E--&gt;F[cluster 1];\n    E--&gt;G[cluster 2];\n    E--&gt;H[cluster 3];\n    F--&gt;I[worker 1];\n    F--&gt;J[worker 2];\n    F--&gt;K[worker 3];\n    G--&gt;L[worker 4];\n    G--&gt;M[worker 5];\n    G--&gt;N[worker 6];\n    H--&gt;O[worker 7];\n    H--&gt;P[worker 8];\n    H--&gt;Q[worker 9];</code></pre> <pre><code>graph TD;\n    A[Jack's submissions]--&gt;E[cloud database];\n    B[Scott's submissions]--&gt;E;\n    C[Lauren's submissions]--&gt;E;\n    D[Siona's submissions]--&gt;E;\n    E--&gt;F[WarWulf];\n    E--&gt;G[LongLeaf];\n    E--&gt;H[DogWood];\n    F--&gt;I[slurm job 1];\n    F--&gt;J[slurm job 2];\n    F--&gt;K[slurm job 3];\n    G--&gt;L[slurm job 4];\n    G--&gt;M[slurm job 5];\n    G--&gt;N[slurm job 6];\n    H--&gt;O[slurm job 7];\n    H--&gt;P[slurm job 8];\n    H--&gt;Q[slurm job 9];</code></pre>"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#what-is-a-scheduler","title":"What is a Scheduler?","text":"<p>A scheduler is a tool to which we submit workflows. It controls when and where workflows are run. </p> <p>The terms \"scheduler\" and \"executor\" are often used interchangeably. As workflows are submitted, the scheduler forms a queue and determines the order of execution. We can use the built-in <code>SimmateExecutor</code>, Dask, or Prefect as our scheduler. For this tutorial, we will use the <code>SimmateExecutor</code> as it is the default option and is already set up for us.</p>"},{"location":"getting_started/add_computational_resources/intro_to_clusters/#what-is-a-cluster","title":"What is a Cluster?","text":"<p>A cluster is a collection of computational resources that execute the workflows. The scheduler identifies the next workflow to be run and sends it to the cluster for execution. </p> <p>Clusters typically consist of workers -- individual resources that process one job at a time. For instance, if we have 10 computers (or slurm jobs) each running one workflow at a time, these computers collectively form our cluster. Each computer is a worker. At any given time, 10 workflows will be running, with each worker handling one. As we are using the <code>SimmateExectuor</code>, we will use <code>SimmateWorker</code>s to set up each worker and, consequently, our cluster. The setup for each worker is the same, regardless of whether your resources are on a cloud service, a supercomputer, or simple desktops.</p>"},{"location":"getting_started/add_computational_resources/quick_start/","title":"Utilizing Computational Resources","text":""},{"location":"getting_started/add_computational_resources/quick_start/#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Remember, it's possible to share a cloud database without sharing computational resources. This flexibility is crucial for many collaborations. </p> </li> <li> <p>As with your cloud database, assign a point-person to manage your private computational resources. All other users simply need to switch from <code>run</code> to <code>run_cloud</code>.</p> </li> <li> <p>If your computational resources are spread across different computers, ensure you've set up a cloud database (refer to the previous tutorial for guidance). If you plan to schedule and run everything on your local computer (or file system), you can skip this step.</p> </li> <li> <p>For remote resources, ensure all simmate installations are connected to the same database (i.e., your database connection file should be present on all resources).</p> </li> <li> <p>If you're using custom workflows, ensure you're using a simmate project and all resources have this app installed. If you don't have custom database tables, you can try proceeding without this step, but registering via an app is the only way to ensure the workflow will run correctly.</p> </li> <li> <p>Schedule your simmate workflows by switching from the <code>run</code> method to the <code>run_cloud</code> method. This workflow will be scheduled but won't run until a worker is started: <pre><code>simmate workflows run-cloud my_settings.yaml\n</code></pre></p> </li> <li> <p>Start a worker wherever you want to run the workflow with the following command.  If you're on a cluster, <code>start-worker</code> should be called within your submit script (e.g., inside <code>submit.sh</code> for SLURM). Avoid running workers on the head node. <pre><code>simmate engine start-singleflow-worker\n</code></pre></p> </li> <li> <p>This \"singleflow\" worker will start, run one workflow, then shut down. For more control over the number of workflows run or to run a worker indefinitely, use the command: <pre><code>simmate engine start-worker\n</code></pre></p> </li> <li> <p>Expand your cluster! Start workers wherever you want, and start as many as you need. Just ensure you follow steps 4 and 5 for every worker. If you need to start multiple workers simultaneously, you can use the <code>start-cluster</code> command as well. <pre><code># starts 5 local workers\nsimmate engine start-cluster 5\n</code></pre></p> </li> <li> <p>To control which workers run which workflows, use tags. Workers will only pick up submissions that have matching tags. <pre><code># when submitting\nsimmate workflows run-cloud ... -t my_tag -t small-job\n\n# when starting the worker (typically on a different computer)\nsimmate engine start-worker -t small-job\n</code></pre></p> </li> <li> <p>To allow others to use your cluster, simply connect them to the same database.</p> </li> </ol>"},{"location":"getting_started/add_computational_resources/should_you_setup/","title":"Should I Establish My Own Cluster?","text":"<p>Note</p> <p>We recognize that resource limitations can be stringent across labs and companies, making resource sharing unfeasible, even in close collaborations. Simmate addresses this by facilitating the sharing of a cloud database without the need to share computational resources. This means you can contribute to a shared database without exposing your computational resources to others.</p>"},{"location":"getting_started/add_computational_resources/should_you_setup/#basic-resource-utilization","title":"Basic Resource Utilization","text":"<p>Each team will likely manage their own computational resources, which could include:</p> <ul> <li>A university or federal HPC cluster with SLURM, PBS, or another queue system</li> <li>A single node or a Kubernetes cluster on a cloud provider</li> <li>A collection of shared desktop computers within your lab</li> <li>Any combination of these resources</li> </ul> <p>The simplest way to utilize these resources is to log in and execute a simmate workflow using the <code>run</code> method. This action runs the workflow directly on your resource. </p> <p>This was demonstrated in the \"Run a workflow\" tutorial when we executed <code>simmate workflows run ...</code>. On an HPC SLURM cluster, simmate would be run using a <code>submit.sh</code>:</p> <pre><code>#!/bin/bash\n\n#SBATCH --output=slurm.out\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n\nsimmate workflows run-yaml my_settings.yaml &gt; simmate.out\n</code></pre> <p>Tip</p> <p>If you're only executing a few workflows per day (&lt;10), we recommend sticking to this method of running workflows. In other words, just execute <code>simmate workflows run</code>. Avoid unnecessary complexity. Revisit tutorial 02 to refresh these concepts.</p>"},{"location":"getting_started/add_computational_resources/should_you_setup/#when-to-establish-workers","title":"When to Establish Workers","text":"<p>If your team is submitting hundreds or thousands of workflows simultaneously, it would be highly beneficial to monitor and manage these workflows using a scheduler and cluster. </p> <p>Similar to our cloud database in the previous tutorial, you only need ONE person to manage ALL of your computational resources. Once the resources are established, other users can connect using the database connection file (or an API key if you're using Prefect).</p> <p>If you're the designated person for your team, proceed with this tutorial. If not, consult with your point person! Utilizing your team's resources should be as simple as switching from the <code>run</code> to <code>run_cloud</code> method.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/","title":"Creating New Project Files","text":"<ol> <li> <p>To initiate a new project, navigate to your desired folder for code storage and execute... <pre><code>simmate start-project\n</code></pre></p> </li> <li> <p>Verify the creation of a new folder named <code>my_new_project</code>. Upon opening, you should find a series of new files: <pre><code>my_new_project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 example_app\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 apps.py\n    \u251c\u2500\u2500 models.py\n    \u251c\u2500\u2500 tests.py\n    \u251c\u2500\u2500 urls.py\n    \u251c\u2500\u2500 views.py\n    \u2514\u2500\u2500 workflows.py\n</code></pre></p> </li> <li> <p>Note the presence of a folder named <code>example_app</code>. This is where your code will reside. You can create as many app folders as needed, and in extreme cases, even nest apps within other apps.</p> </li> </ol>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#naming-your-project-and-app","title":"Naming Your Project and App","text":"<p>Danger</p> <p>Once you've chosen a name, stick with it. Altering your project or app name post-installation can result in <code>ModuleNotFound</code> errors.</p> <p>Naming the Project</p> <ol> <li> <p>Rename your folder from \"my_new_project\" to a name of your choice. Adhere to Python conventions by keeping your project name all lowercase and connected with underscores. For instance, <code>warren_lab</code> or <code>scotts_project</code> are suitable project names.</p> </li> <li> <p>Open the file <code>new_project_project/pyproject.toml</code> and update the name here as well. <pre><code>[project]\nname = \"my_simmate_project\"  # &lt;--- update with your new name\n</code></pre></p> </li> </ol> <p>Naming the App</p> <ol> <li> <p>Determine how your code should be imported. For instance, you may want your workflows to be loaded like so: <pre><code>from example_app.workflows import Example__Workflow__Settings\n</code></pre></p> </li> <li> <p>Use the first part of this (<code>example_app</code>) to rename the <code>example_app</code> folder. The Python conventions (described above) also apply here. For instance, <code>simmate_abinit</code> or <code>simmate_clease</code> are informative and memorable project names. Here's how they would work: <pre><code>from simmate_clease.workflows import ClusterExpansion__Clease__BasicSettings\n</code></pre></p> </li> <li> <p>Open the file <code>example_app/apps.py</code> and rename the class AND name property to match your app name. <pre><code>from django.apps import AppConfig\n\nclass SimmateCleaseConfig(AppConfig):  # don't forget the class name\n    name = \"simmate_clease\"\n</code></pre></p> </li> </ol> <p>Note</p> <p>While this file may seem trivial, it enables users to build complex apps that include many other apps / subapps. Beginners will likely never revisit this file.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#installing-your-app","title":"Installing Your App","text":"<ol> <li> <p>Open the <code>pyproject.toml</code> file. This file instructs Python on how to install your code. It doesn't require much to install a package . As your project expands and requires other programs to be installed, you'll note them here. For now, no changes are needed.</p> </li> <li> <p>While inside your new project folder, \"install\" the project to your conda environment in \"--editable\" (-e) mode. This allows you to make changes to your code, and Python will automatically incorporate your changes. <pre><code># replace \"my_new_project\" with the name of your project\ncd my_new_project\npip install -e .\n</code></pre></p> </li> <li> <p>Verify the installation by running these lines in Python. You may need to restart your terminal/Spyder for this to work.</p> </li> </ol> example 1example 2 <pre><code>import example_app\nfrom example_app.apps import ExampleAppConfig\n</code></pre> <pre><code>import simmate_clease\nfrom simmate_clease.apps import SimmateCleaseConfig\n</code></pre> <p>You now have an installed app! However, Simmate is still unaware of its existence. We need to inform Simmate to load it.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_app/#registering-your-app-with-simmate","title":"Registering Your App with Simmate","text":"<ol> <li> <p>Navigate to your Simmate configuration folder. Recall from earlier tutorials that this is where your database is stored, and it is located at... <pre><code># in your home directory\ncd ~/simmate/\n</code></pre></p> </li> <li> <p>Locate the file <code>~/simmate/my_env-settings.yaml</code>, which is named after your conda environment. Open it and you'll see we have apps already installed with Simmate: <pre><code>apps:\n    - simmate.workflows.configs.BaseWorkflowsConfig\n    - simmate.apps.VaspConfig\n    - simmate.apps.BaderConfig\n    - simmate.apps.EvoSearchConfig\n</code></pre></p> </li> <li> <p>In this section, add the following line: <pre><code>- example_app.apps.ExampleAppConfig\n</code></pre></p> </li> <li> <p>Ensure Simmate can locate and load your app in Python <pre><code>from simmate.configuration import settings\nprint(settings.apps)  # you should see your new app!\n</code></pre></p> </li> <li> <p>Ensure Simmate can configure your new app and its tables properly <pre><code>from simmate.database import connect\n</code></pre></p> </li> </ol>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/","title":"Creating Custom Database Tables","text":""},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#constructing-custom-tables","title":"Constructing Custom Tables","text":"<p>Your project includes example database tables that demonstrate how to construct basic ones. Although it may appear extremely simple, there's no additional work required! </p> <p>Remember the lesson on inheritance from the \"access the database\" tutorial. This is why creating new tables is straightforward. </p> <p>For instance, the following code...</p> <pre><code># This code is for illustrative purposes only. Do not run it.\n\nfrom simmate.database.base_data_types import (\n    table_column,\n    Structure,\n    Calculation,  # useful for tables used in workflows\n)\n\nclass MyCustomTable1(Structure, Calculation):\n    pass  # no additional requirements unless you want to add custom columns/features\n</code></pre> <p>... will generate a new database table with the following columns:</p> <pre><code>- created_at\n- updated_at\n- source\n- structure\n- nsites\n- nelements\n- elements\n- chemical_system\n- density\n- density_atomic\n- volume\n- volume_molar\n- formula_full\n- formula_reduced\n- formula_anonymous\n- spacegroup (points to an entry in the Spacegroup table)\n</code></pre> <p>However, we can't import these tables or load data into them just yet. We'll cover that next.</p> <p>Tip</p> <p>While we only demonstrate <code>Structure</code> and <code>Calculation</code> data here, there are many more <code>base_data_types</code> you can utilize. All types automatically generate features for you. Make sure to review our guides in the <code>simmate.database</code> module for more information. Advanced database tables may require further reading on the base data types.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#incorporating-tables-into-your-database","title":"Incorporating Tables into Your Database","text":"<ol> <li> <p>Open the <code>example_app/models.py</code> file and review the example tables.</p> </li> <li> <p>Add these tables to your database by updating your database with Simmate: <pre><code>simmate database update\n</code></pre></p> </li> <li> <p>Verify the output of your <code>update</code> command. You should see that your new app and tables have been added. <pre><code>Migrations for 'example_app':\n  example_app/migrations/0001_initial.py\n    - Create model MyCustomTable2\n    - Create model MyCustomTable1\n</code></pre></p> </li> <li> <p>Ensure you can view the new tables in your database. Remember, we need to connect to our database first. We'll start with <code>MyCustomTable2</code>:</p> </li> </ol> <pre><code>from simmate.database import connect\n\nfrom example_app.models import MyCustomTable2\n\nMyCustomTable2.objects.count()  # should output 0 as we haven't added data yet\n</code></pre> <p>Danger</p> <p>Whenever you modify the <code>models.py</code> file, make sure to run <code>simmate database update</code> for your changes to take effect in your database.</p> <p>Info</p> <p>In Django (which Simmate uses under the hood), a <code>DatabaseTable</code> is referred to as a <code>Model</code>. Therefore, a model and table can be considered the same. As we're using Django, the file name <code>models.py</code> must remain as is. That's where Django searches for your custom database tables.</p> <p>Tip</p> <p><code>simmate database reset</code> will also apply your changes to the database. Just ensure you select no for the prebuilt database.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#populating-your-new-table-with-data","title":"Populating Your New Table with Data","text":"<p>You can automatically populate this table using the <code>from_toolkit</code> method:</p> <pre><code>from simmate.database import connect\nfrom simmate.toolkit import Structure\n\nfrom example_app.models import MyCustomTable1\n\nnacl = Structure.from_file(\"NaCl.cif\")\n\nnew_entry = MyCustomTable1.from_toolkit(structure=nacl)\nnew_entry.save()\n</code></pre> <p>Tip</p> <p>Manually populating your table with data is often unnecessary. Instead, you can connect your database to a workflow, which will automatically fill it with data. We'll cover this in a later step.</p>"},{"location":"getting_started/custom_tables_and_apps/create_a_custom_table/#searching-your-new-data","title":"Searching Your New Data","text":"<p>You can filter results just like any other table. If you need a refresher, refer back to the earlier database tutorial.</p> <pre><code>df = MyCustomTable1.objects.filter(nsites__lte=10).to_dataframe()\n</code></pre>"},{"location":"getting_started/custom_tables_and_apps/quick_start/","title":"Constructing Custom Database Tables and Applications","text":""},{"location":"getting_started/custom_tables_and_apps/quick_start/#when-to-create-a-simmate-project","title":"When to Create a Simmate Project?","text":"<p>A custom Simmate project is necessary if you wish to construct a new database table or access your workflows via the website interface.</p> <p>There are several other reasons to create a project, such as:</p> <ul> <li> Utilizing a custom database table to store workflow results</li> <li> Accessing the workflow through the website interface</li> <li> Accessing the workflow from other scripts (and the <code>get_workflow</code> function)</li> <li> Organizing code into smaller files for easy import</li> <li> Sharing workflows within a team</li> <li> Allowing others to install your workflows after publishing a new paper</li> </ul> <p>All these tasks can be accomplished using Simmate \"projects\". Essentially, these projects are folders containing Python files arranged in a specific format (i.e., there are rules for file naming and content).</p>"},{"location":"getting_started/custom_tables_and_apps/quick_start/#is-this-equivalent-to-creating-a-new-package","title":"Is this Equivalent to Creating a New Package?","text":"<p>Yes, indeed -- Projects are essentially the creation of a new Python package. In fact, our <code>start-project</code> command functions like a \"cookie-cutter\" template.</p> <p>This has significant implications for code and research sharing. With a fully-functional and published Simmate project, you can share your code with other labs via Github and PyPi. This allows the entire Simmate community to install and use your custom workflows with Simmate. For them, the process is as simple as:</p> <ol> <li><code>pip install my_new_project</code></li> <li>Adding <code>example_app.apps.ExampleAppConfig</code> to their <code>~/simmate/my_env-apps.yaml</code></li> </ol> <p>Alternatively, you can request to merge your app into our Simmate repository, making it a default installation for all users. Whichever path you choose, your hard work will be more accessible to the community and new users!</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/","title":"Using App Workflows","text":""},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#organizing-app-workflows","title":"Organizing app workflows","text":"<p>Simmate automatically searches for a <code>workflows.py</code> (or <code>workflows</code> module) within your app and retrieves all Python classes within it, assuming they are workflows you want registered. However, if your scripts contain non-workflow classes or abstract base workflows, this can lead to unexpected errors. Therefore, you need to explicitly specify which workflows should be registered with your app.</p> <p>Note</p> <p>If you encounter <code>AttributeError: '...' object has no attribute 'name_full'</code> or <code>Exception: Make sure you are following Simmate naming conventions</code>, it's likely that your workflows are misorganized.</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#in-workflowspy-file","title":"in workflows.py file","text":"<p>Upon creating your project/app, you'll find a single <code>workflows.py</code> file with the following at the top:</p> <pre><code># -----------------------------------------------------------------------------\n# List all workflows you want registered\n# -----------------------------------------------------------------------------\n\n__all__ = [\n    \"Example__Python__MyExample1\",\n    \"Relaxation__Vasp__MyExample2\",\n]\n</code></pre> <p>You must explicitly list all workflows you want registered if you stick with a single <code>workflow.py</code> file format.</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#in-a-workflows-module","title":"in a workflows module","text":"<p>As your app expands, you might want to store your workflows in separate scripts or submodules. You can do this by replacing the <code>workflows.py</code> file with a <code>workflows</code> folder with the following structure:</p> <pre><code># rest of example_app is organized the same as before\nexample_app\n\u2514\u2500\u2500 workflows\n    \u251c\u2500\u2500 __init__.py   # &lt;-- file used for registration\n    \u251c\u2500\u2500 example_1.py\n    \u251c\u2500\u2500 example_2.py\n    \u251c\u2500\u2500 example_3.py\n    \u2514\u2500\u2500 example_submodule\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 example_4.py\n        \u2514\u2500\u2500 example_5.py\n</code></pre> <p>Here, <code>example_N.py</code> files can be named as you wish and do NOT require the <code>__all__</code> tag. This folder structure allows for better workflow organization.</p> <p>Instead of the <code>__all__</code> tag, we can list workflows to register in the <code>workflows/__init__.py</code> file. We can use relative Python imports to achieve this with minimal code:</p> <pre><code># in workflows/__init__.py\n\nfrom .example_1 import Example__Python__MyExample1\nfrom .example_2 import Example__Python__MyExample2\nfrom .example_3 import Example__Python__MyExample3\n\nfrom .example_submodule.example_4 import Example__Python__MyExample4\nfrom .example_submodule.example_5 import Example__Python__MyExample5\n</code></pre> <p>Example</p> <p>If you prefer learning by example, check out Simmate's built-in <code>Vasp</code> app. The <code>workflows</code> module of this app is located here. You can also see how our <code>workflows/__init__.py</code> file lists all of our registered workflows here).</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#basic-use","title":"Basic use","text":"<p>Our workflows behave the same as before. We can run them with a YAML file or directly in Python.</p> <pre><code>workflow_name: example_app/workflows:Example__Python__MyExample1\nstructure: NaCl.cif\ninput_01: 12345\ninput_02: true\n</code></pre> <p>However, now that they are in a Simmate Project and we registered the App, we can access some extra features. We can use just the workflow name and also access our workflow with the command line and <code>get_workflow</code> utilities:</p> yamlpython <pre><code>workflow_name: example.python.my-example1\nstructure: NaCl.cif\ninput_01: 12345\ninput_02: true\n</code></pre> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"example.python.my-example1\")\nworkflow.run(\n    structure=\"NaCl.cif\",\n    input_01=12345,\n    input_02=true,\n)\n</code></pre> <p>You can also see your workflow listed now:</p> command line <pre><code>simmate workflows list-all\n</code></pre>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#link-datatables-to-workflows","title":"Link Datatables to Workflows","text":"<p>To use a custom database table in a workflow, the following conditions must be met:</p> <ul> <li> the table must use the <code>Calculation</code> mix-in</li> <li> the workflow must have <code>database_table</code> set to our table</li> <li> the table and workflow must be registered (already completed)</li> </ul> <p>Note that our database tables and workflows already meet these conditions.</p> <p>For <code>MyCustomTable1</code>, we can see it is using the <code>Calculation</code> mix-in in our <code>models.py</code> file: <pre><code>class MyCustomTable1(Structure, Calculation):\n    # ... custom columns hidden ...\n</code></pre></p> <p>This table has already been linked to a workflow too. In our <code>workflows.py</code> file, we can see the following: <pre><code>class Example__Python__MyExample1(Workflow):\n    database_table = MyCustomTable1\n</code></pre></p> <p>This completes our checklist -- so this database and workflow are already configured for us.</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#storing-inputs-parameters","title":"Storing inputs parameters","text":"<p>To store input parameters at the start of a calculation in a workflow, the following conditions must be met:</p> <ul> <li> the parameter must have been added as a column to the database</li> <li> a parameter with the exact same name must be an input option of <code>run_config</code></li> </ul> <p>For our <code>MyCustomTable1</code> and <code>Example__Python__MyExample1</code>, we can see that the following inputs match both the <code>run_config</code> input AND are table columns:</p> <ol> <li>input_01</li> <li>input_02</li> <li>structure</li> </ol> <p>Here's the relevant code that sets this up:</p> MyCustomTable1Example__Python__MyExample1 <pre><code># structure --&gt; through the Structure mix-in\ninput_01 = table_column.FloatField(null=True, blank=True)\ninput_02 = table_column.BooleanField(null=True, blank=True)\n</code></pre> <pre><code>def run_config(\n    input_01,\n    input_02,\n    structure,\n</code></pre> <p>That's it! Your workflow will store these inputs in your database when a workflow run starts.</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#storing-outputs-and-results","title":"Storing outputs and results","text":"<p>To store outputs at the end of a calculation in a workflow, the following conditions must be met:</p> <ul> <li> the parameter must have been added as a column to the database</li> <li> the <code>run_config</code> must return a dictionary of columns that need to be updated</li> <li> a key with the exact same name must be in this dictionary</li> </ul> <p>For our <code>MyCustomTable1</code> and <code>Example__Python__MyExample1</code>, we can see that the following outputs match both the <code>run_config</code>'s output dictionary AND are table columns:</p> <ol> <li>output_01</li> <li>output_02</li> </ol> <p>Here's the relevant code that sets this up:</p> MyCustomTable1Example__Python__MyExample1 <pre><code>output_01 = table_column.FloatField(null=True, blank=True)\noutput_02 = table_column.BooleanField(null=True, blank=True)\n</code></pre> <pre><code>return {\n    \"output_01\": ...,\n    \"output_02\": ...,\n}\n</code></pre> <p>That's it! Your workflow will store these results in your database when a workflow run completes.</p>"},{"location":"getting_started/custom_tables_and_apps/using_app_workflows/#viewing-results","title":"Viewing Results","text":"<p>Results are stored the same as any other workflow. You'll see a summary file written for you, and you can load all the data from your database. We only configure a small number of columns for our workflow + datatable, but check out all of the outputs!</p> <pre><code>_DATABASE_TABLE_: MyCustomTable1\n_TABLE_ID_: 1\n_WEBSITE_URL_: http://127.0.0.1:8000/workflows/example/python/my-example1/1\nchemical_system: Cl-Na\ncomputer_system: digital-storm\ncreated_at: 2022-09-04 00:10:01.844798+00:00\ndensity: 2.1053060843576104\ndensity_atomic: 0.04338757298280908\ndirectory: /home/jacksund/Documents/spyder_wd/simmate-task-ow6otw06\nformula_anonymous: AB\nformula_full: Na4 Cl4\nformula_reduced: NaCl\nid: 1\ninput_01: 12345.0\ninput_02: true\nnelements: 2\nnsites: 8\noutput_01: 1234500\noutput_02: false\nrun_id: 6872771c-c0d7-43b5-afea-b8ed87f6a5df\nspacegroup_id: 225\nupdated_at: 2022-09-04 00:10:01.875757+00:00\nvolume: 184.38459332974767\nvolume_molar: 13.87987468758872\nworkflow_name: example.python.my-example1\nworkflow_version: 0.10.0\n</code></pre>"},{"location":"getting_started/custom_workflows/creating_your_workflow/","title":"Constructing New &amp; Advanced Workflows","text":"<p>Note</p> <p>This guide provides a basic overview. For a comprehensive understanding when constructing your custom workflows, we strongly recommend referring to the complete guides.</p>"},{"location":"getting_started/custom_workflows/creating_your_workflow/#building-a-workflow-from-scratch","title":"Building a Workflow from Scratch","text":"<p>Simmate offers a foundational <code>Workflow</code> class to assist with routine material science analyses. The most basic workflow could appear as follows...</p> <pre><code>from simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n    # Remember, the extended name of this workflow class is crucial!\n\n    use_database = False  # we don't have a database table yet\n\n    @staticmethod\n    def run_config(**kwargs):\n        print(\"This workflow doesn't do much\")\n        return 42\n</code></pre>"},{"location":"getting_started/custom_workflows/creating_your_workflow/#altering-an-existing-workflow","title":"Altering an Existing Workflow","text":"<p>Creating a workflow from scratch can be time-consuming. More often than not, we simply want to modify an existing workflow by updating a few settings. This can be achieved in Python as follows...</p> <pre><code>from simmate.workflows.utilities import get_workflow\n\noriginal_workflow = get_workflow(\"static-energy.vasp.matproj\")\n\n\nclass StaticEnergy__Vasp__MyCustomPreset(original_workflow):\n    # NOTE: The name we assigned is crucial! \n    # Don't overlook the guide above\n\n    # Assign a version to help you and your team track changes\n    version = \"2022.07.04\"\n\n    _incar_updates = dict(\n        NPAR=1,\n        ENCUT=-1,\n    )\n</code></pre> <p>Danger</p> <p>Modifying workflows can often lead to unforeseen issues -- as not all workflows behave identically. More often than not, it's advisable to create your own custom  <code>VaspWorkflow</code>. Learn more in the full-guides.</p>"},{"location":"getting_started/custom_workflows/creating_your_workflow/#executing-your-workflow","title":"Executing your Workflow","text":"<p>You can now execute and interact with your workflow like any other!</p> <pre><code>state = StaticEnergy__Vasp__MyCustomPreset.run(structure=\"NaCl.cif\")\nresult = state.result()\n</code></pre> <p>Tip</p> <p>Workflows can also be executed from a YAML file. Refer to the full-guides for more information.</p>"},{"location":"getting_started/custom_workflows/creating_your_workflow/#advanced-workflow-features","title":"Advanced Workflow Features","text":"<p>There are numerous enhancements you may want to incorporate into your new workflow. For instance, you might want to:</p> <ul> <li> Modify a complex workflow (like <code>diffusion.vasp.neb-all-paths-mit</code>)</li> <li> Develop a custom workflow using a new program such as USPEX or ABINIT</li> <li> Utilize a custom database table to store your workflow results</li> <li> Access the workflow via the website interface</li> <li> Access your workflow from other scripts (and the <code>get_workflow</code> function)</li> </ul> <p>These topics will be addressed in the next tutorial, where we will simultaneously cover custom database tables.</p>"},{"location":"getting_started/custom_workflows/name_your_new_workflow/","title":"Workflow Naming Guidelines","text":""},{"location":"getting_started/custom_workflows/name_your_new_workflow/#the-importance-of-naming-conventions","title":"The Importance of Naming Conventions","text":"<p>In Simmate, the naming of your new workflow is a crucial step. </p> <p>Certain features, such as the website interface, necessitate that workflow names adhere to a specific format. This allows us to perform tasks such as locating your new workflow within the website interface. We follow a set of rules to generate workflow names like <code>relaxation.vasp.mit</code>.</p> <p>Here's how a workflow name appears in different contexts:</p> Readable NameWebsite URLPython Class Name <pre><code>static-energy.vasp.matproj\n</code></pre> <pre><code>https://simmate.org/workflows/static-energy/vasp/matproj\n</code></pre> <pre><code>StaticEnergy__Vasp__Matproj\n</code></pre>"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#understanding-naming-conventions","title":"Understanding Naming Conventions","text":"<p>Simmate's naming conventions consist of three components:</p> <ol> <li>The type of analysis the workflow performs</li> <li>The \"app\" (or program) that the workflow uses for execution</li> <li>A unique name to distinguish the settings used</li> </ol> <p>Examples for each component are:</p> <ol> <li>relaxation, static-energy, dynamics, ...</li> <li>vasp, abinit, qe, deepmd, ...</li> <li>jacks-test, matproj, quality00, ...</li> </ol> <p>Combined, example workflow names would be:</p> <ul> <li><code>relaxation.vasp.jacks-test</code></li> <li><code>static-energy.abinit.matproj</code></li> <li><code>dynamics.qe.quality00</code></li> </ul> <p>To convert this to our workflow name in python, we replace periods with two underscores each and convert our words to pascal case. For instance, our workflow names become:</p> <ul> <li><code>Relaxation__Vasp__JacksTest</code></li> <li><code>StaticEnergy__Abinit__Matproj</code></li> <li><code>Dynamics__Qe__Quality00</code></li> </ul> <p>Warning</p> <p>Be mindful of capitalization as it is crucial in this context. Always double-check your workflow names.</p>"},{"location":"getting_started/custom_workflows/name_your_new_workflow/#implementing-it-in-python","title":"Implementing it in Python","text":"<p>Let's put this into practice in Python using a similar workflow name: <pre><code>from simmate.engine import Workflow\n\nclass Example__Python__MyFavoriteSettings(Workflow):\n    pass  # we will build the rest of workflow later\n\n# These names can be lengthy and complex, so it's helpful to\n# assign them to a variable name for easier access.\nmy_workflow = Example__Python__MyFavoriteSettings\n\n# Now verify that our naming convention functions as expected\nassert my_workflow.name_full == \"example.python.my-favorite-settings\"\nassert my_workflow.name_type == \"example\"\nassert my_workflow.name_app == \"python\"\nassert my_workflow.name_preset == \"my-favorite-settings\"\n</code></pre></p> <p>You now have a valid workflow name!</p> <p>Tip</p> <p><code>assert</code> essentially means \"ensure this statement returns <code>True</code>\". It's commonly used by Python developers to verify that their code functions as intended.</p>"},{"location":"getting_started/custom_workflows/quick_start/","title":"Build custom workflows","text":"<p>In this tutorial, you will learn how to build customized workflows.</p> <p>This tutorial only covers the bare minimum to creating a custom workflow. See the full guides and reference for more information.</p> <p>Warning</p> <p>There is no \"quick tutorial\" for this topic. Even advanced users should read everything!</p>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/","title":"Modifying Workflow Settings","text":"<p>Danger</p> <p>The techniques outlined in this section are generally not recommended, but they can be useful for initial setup. Following this, we will discuss the optimal method for modifying settings and creating new workflows.</p>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#why-isnt-there-a-custom_settings-option","title":"Why isn't there a <code>custom_settings</code> option?","text":"<p>We deliberately do not use <code>workflow.run(custom_settings=...)</code>. This will NOT work. Simmate takes this approach because we want to avoid storing results from custom settings in the same results table. This would (a) complicate the analysis of multiple structures/systems and (b) make navigating results extremely challenging for beginners. </p> <p>For instance, altering the <code>ENCUT</code> or changing the dispersion correction of a VASP calculation would prevent energy comparisons between all materials in the table, rendering features like calculated hull energies inaccurate.</p> <p>Instead, Simmate promotes the creation of new workflows and result tables for custom settings. This emphasizes Simmate's focus on \"scaling up\" workflows (i.e., running a fixed workflow on thousands of materials) rather than \"scaling out\" workflows (i.e., a flexible workflow that changes on a structure-by-structure basis).</p>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#modifying-settings-for-an-existing-workflow","title":"Modifying settings for an existing workflow","text":"<p>For quick testing, it can be useful to adjust a workflow's settings without creating a new workflow. There are two methods to edit your settings:</p>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#option-1","title":"OPTION 1","text":"<p>Write input files and manually submit a separate program</p> <pre><code># This simply writes input files\nsimmate workflows setup-only static-energy.vasp.mit --structure POSCAR\n\n# access your files in the new directory\ncd static-energy.vasp.mit.SETUP-ONLY\n\n# Customize input files as you see fit.\n# For example, you may want to edit INCAR settings\nnano INCAR\n\n# You can then submit VASP manually. Note, this will not use\n# simmate at all! So there is no error handling and no results\n# will be saved to your database.\nvasp_std &gt; vasp.out\n</code></pre>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#option-2","title":"OPTION 2","text":"<p>Use the \"customized\" workflow for an app (e.g., <code>customized.vasp.user-config</code>)</p> <pre><code># In a file named \"my_example.yaml\".\n\n# Indicates we want to change the settings, using a specific workflow as a starting-point\nworkflow_name: customized.vasp.user-config\nworkflow_base: static-energy.vasp.mit\n\n# \"Updated settings\" indicated that we are updating some class attribute. \n# These fundamentally change the settings of a workflow. \n# Currently, only updating dictionary-based attributes are supported\nupdated_settings:\n    incar: \n        ENCUT: 600\n        KPOINTS: 0.25\n    potcar_mappings:\n        Y: Y_sv\n\n# Then the remaining inputs are the same as the base workflow\ninput_parameters:\n    structure: POSCAR\n    command: mpirun -n 5 vasp_std &gt; vasp.out\n</code></pre> <pre><code># Now run our workflow from the settings file above.\n# Results will be stored in a separate table from the\n# base workflow's results.\nsimmate workflows run-yaml my_example.yaml\n</code></pre> <p>Warning</p> <p>These methods are only applicable to single-calculation workflows (i.e., \"nested\" workflows that call several workflows within them are not supported)</p>"},{"location":"getting_started/custom_workflows/update_an_existing_workflow/#avoid-these-methods-if-possible","title":"Avoid these methods if possible!","text":"<p>Both options above are only suitable for customizing settings for a few calculations, and you lose some key Simmate features. If you are submitting many calculations (&gt;20) and these methods do not meet your needs, continue reading!</p>"},{"location":"getting_started/database/access_thirdparty_data/","title":"c) Third-party data","text":""},{"location":"getting_started/database/access_thirdparty_data/#accessing-third-party-data","title":"Accessing Third-Party Data","text":"<p>In order to perform calculations with Simmate, it's crucial to understand what calculations have already been performed by other researchers for a specific material. Numerous research teams globally have created databases consisting of over 100,000 structures, with calculations performed on each. In this section, we'll use Simmate to explore these databases.</p>"},{"location":"getting_started/database/access_thirdparty_data/#loading-a-database","title":"Loading a Database","text":"<p>We'll begin with one of the smaller databases: JARVIS. Despite being smaller than others, it still contains approximately 56,000 structures. Simmate allows you to download all of these in under 0.01 GB.</p> <p>Previously, we loaded our <code>DatabaseTable</code> from the workflow. However, in this case, we want to directly access the table. To do this, we execute the following:</p> <pre><code># This line MUST be executed before any tables can be loaded\nfrom simmate.database import connect  # this connects to our database\n\n# This provides the database_table we used in the previous section\nfrom simmate.database.workflow_results import StaticEnergy\n\n# This loads the table where we store all of the JARVIS data.\nfrom simmate.database.third_parties import JarvisStructure\n</code></pre> <p>The <code>table</code> from the previous section (on accessing workflow data) and the <code>StaticEnergy</code> class here refer to the same class/table. These are just different methods of loading it. While loading a workflow automatically sets up a database connection, we have to manually perform this step here (with <code>from simmate.database import connect</code>). </p> <p>Warning</p> <p>The most common error when loading database tables directly from the <code>simmate.database</code> module is forgetting to connect to your database. Don't forget to include <code>from simmate.database import connect</code>!</p>"},{"location":"getting_started/database/access_thirdparty_data/#populating-data","title":"Populating Data","text":"<p>With our datatable class (<code>JarvisStructure</code>) loaded, let's check if it contains any data:</p> <pre><code>JarvisStructure.objects.count()\n</code></pre> <p>Note</p> <p>If you accepted the download during the <code>simmate database reset</code> command, you should see thousands of structures already in this database table! </p> <p>If the count returns 0, it means you still need to load data. You can quickly load all the data using the <code>load_remote_archive</code> method. This method downloads the JARVIS data from simmate.org and transfers it to your database. This process can take approximately 10 minutes as it saves all these structures to your computer, enabling you to load these structures in under a second in the future.</p> <pre><code># NOTE: This line is only needed if you did NOT accept the download\n# when running `simmate database reset`.\nJarvisStructure.load_remote_archive()  # This may take ~10min to complete\n</code></pre> <p>Warning</p> <p>Please read the warnings printed by <code>load_remote_archive</code>. This data was NOT created by Simmate. We are merely distributing it on behalf of other teams. Please credit them for their work!</p> <p>The <code>load_remote_archive</code> function downloads ALL data to your computer and saves it. This data will not be updated unless you call <code>load_remote_archive</code> again. This should only be done when we release a new archive version (usually once per year). Avoid overusing this feature.</p>"},{"location":"getting_started/database/access_thirdparty_data/#exploring-the-data","title":"Exploring the Data","text":"<p>Now that our database is populated with data, we can start exploring it:</p> <pre><code># We use [:150] to just show the first 150 rows\ndata = JarvisStructure.objects.to_dataframe()[:150]\n</code></pre> <p>Let's test our filtering ability with this new data:</p> <pre><code>from simmate.database import connect  # this connects to our database\nfrom simmate.database.third_parties import JarvisStructure\n\n# EXAMPLE 1: all structures that have less than 6 sites in their unitcell\nstructures_1 = JarvisStructure.objects.filter(nsites__lt=6).all()\n\n# EXAMPLE 2: all MoS2 structures that are less than 5/A^3 and have a spacegroup\n# symbol of R3mH\nstructures_2 = JarvisStructure.objects.filter(\n   formula_full=\"Mo1 S2\",\n   density__lt=5,\n   spacegroup__symbol=\"R3mH\",\n).all()\n\n# You can use to_dataframe() to convert these to a pandas Dataframe object and \n# then view them in Spyder's variable explorer\ndf_1 = structures_1.to_dataframe()\ndf_2 = structures_2.to_dataframe()\n</code></pre>"},{"location":"getting_started/database/access_thirdparty_data/#advanced-data-manipulation","title":"Advanced Data Manipulation","text":"<p>There are numerous ways to search through your tables, and we've only covered the basics here. </p> <p>Advanced users should know that we use Django's query api under the hood. It can take a while to master, so we recommend going through Django's full tutorial only if you plan on joining our team or are a fully computational student. </p> <p>Beginners can simply ask for help. Determining the correct filter can take new users hours, while it will only take our team a minute or two. Save your time and post questions here.</p>"},{"location":"getting_started/database/access_workflow_data/","title":"Accessing Results from Local Calculations","text":""},{"location":"getting_started/database/access_workflow_data/#loading-a-table","title":"Loading a Table","text":"<p>In the \"Run a Workflow\" tutorial, we executed a calculation and stored the results in our database table. This section will guide you through accessing these results. </p> <p>The results database table is always linked to the workflow via the <code>database_table</code> attribute. Here's how to load it:</p> <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.mit\")\ntable = workflow.database_table\n</code></pre>"},{"location":"getting_started/database/access_workflow_data/#viewing-available-columns","title":"Viewing Available Columns","text":"<p>To view the data stored in this table, use the <code>show_columns()</code> method. This will display all the columns in the table:</p> <pre><code>table.show_columns()\n</code></pre> <p>The output will be a list of all the columns in the table. Simmate automatically generates all these columns as they require minimal storage space.</p>"},{"location":"getting_started/database/access_workflow_data/#converting-to-an-excel-like-table","title":"Converting to an Excel-like Table","text":"<p>To view the table with all its data, use the <code>objects</code> attribute to access the table rows. Then, convert this to a \"dataframe\" to view the table. A dataframe is a filtered section of a database table. Since we haven't applied any filters to our results, our dataframe will display the entire table. </p> <p><pre><code>data = table.objects.to_dataframe()\n</code></pre> To view the table, double-click <code>data</code> in Spyder's variable explorer (top right window). Here's what a typical dataframe looks like in Spyder:</p> <p> </p>"},{"location":"getting_started/database/access_workflow_data/#filtering-results-from-the-table","title":"Filtering Results from the Table","text":"<p>You can use the table columns to filter your results. The filtered results will be returned as a list of rows that meet the filtering criteria. In the previous example, we converted this list of results into a dataframe for easier viewing. You can also convert each row into our <code>ToolkitStructure</code> from tutorial 3! Feel free to experiment with each:</p> <pre><code># You can filter rows in the table using any column!\nsearch_results = table.objects.filter(\n    formula_reduced=\"NaCl\",  # check an exact match for any column\n    nelements=2,  # filter a column based on a greater or equal to (gte) condition\n).all()\n\n# This is just a list of database objects (1 object = 1 row)\nprint(search_results)\n\n# You can convert this list of objects to a dataframe like we did above\ndata = search_results.to_dataframe()\n\n# Or you can convert to a list of structure objects (ToolkitStructure)\nstructures = search_results.to_toolkit()\n</code></pre> <p>This may not seem very exciting now as we only have one row/structure in our table, but we'll explore more advanced filtering in the next section.</p>"},{"location":"getting_started/database/dbeaver/","title":"DBeaver","text":"<p>We recommend using DBeaver to explore your database and all of its tables.</p> <p>A full tutorial walkthrough is actively being written by our team and will become available soon.</p>"},{"location":"getting_started/database/intro_to_python_inheritance/","title":"Python Inheritance in Datatables","text":""},{"location":"getting_started/database/intro_to_python_inheritance/#recap-of-key-concepts","title":"Recap of Key Concepts","text":"<p>Before we proceed, let's quickly recap the main concepts we've covered so far...</p> <ul> <li> We've established our database and populated it with calculation results.</li> <li> We've introduced Python classes, focusing on the significance of the <code>Structure</code> class.</li> <li> We've explored how to navigate documentation and utilize new classes.</li> </ul> <p>Now, we'll integrate these concepts to navigate our database. </p>"},{"location":"getting_started/database/intro_to_python_inheritance/#python-table-example","title":"Python Table Example","text":"<p>Let's start with the basics... All datatables are represented by a class, and the general format is as follows:</p> <pre><code>from simmate.database.base_data_types import DatabaseTable, table_column\n\nclass MyExampleTable(DatabaseTable):\n   column_01 = table_columns.CharField()  # CharField --&gt; text storage\n   column_02 = table_columns.BoolField()  # BoolField --&gt; True/False storage\n   column_03 = table_columns.FloatField()  # FloatField --&gt; number/decimal storage\n</code></pre> <p>The corresponding table (populated with random data) would look like this:</p> column_01 column_02 column_03 jack True 3.1456 lauren False 299792458 siona True 1.6180 scott False 1.602e-19 ... ... ... <p>Creating tables is as simple as defining a class, declaring it as a <code>DatabaseTable</code>, and specifying the desired columns.</p>"},{"location":"getting_started/database/intro_to_python_inheritance/#constructing-tables-with-inheritance","title":"Constructing Tables with Inheritance","text":"<p>However, if we have multiple tables with similar data, this process can become repetitive. For instance, we might want to store structures in various tables, each with columns like density, number of sites, number of elements, etc. To streamline this process, we use Python \"inheritance\". Here's how it works:</p> <p>First, we define a table with common data (let's use <code>Person</code> as an example).</p> <pre><code>from simmate.database.base_data_types import DatabaseTable, table_column\n\nclass Person(DatabaseTable):\n   name = table_columns.CharField()\n   age = table_columns.IntField()\n   height = table_columns.FloatField()\n</code></pre> <p>Next, we create a separate table that includes this data and more:</p> <pre><code>class Student(Person):  # &lt;--- note we have Person here instead of DatabaseTable\n   year = table_columns.IntField()  # e.g. class of 2020\n   gpa = table_columns.FloatField()\n</code></pre> <p>The <code>Student</code> datatable now looks like this:</p> name age height year gpa jack 15 6.1 2020 3.6 lauren 16 5.8 2019 4.0 siona 15 5.6 2020 3.7 scott 14 6.2 2021 3.2 ... ... ... ... ... <p>Simmate employs this concept with common materials science data, such as structures, thermodynamic data, site forces, and more. Our fundamental building blocks for tables are found in the <code>simmate.database.base_data_types</code> module (covered here).</p> <p>All our datatables are built upon these classes. Next, we'll examine an actual database table and learn how to use it to view data.</p>"},{"location":"getting_started/database/quick_start/","title":"Database Exploration","text":"<p>Tip</p> <p>We recommend using DBeaver to explore your database and all of its tables. DBeaver is free and production-ready for all database backends that we support (SQLite, Postgres, etc.).</p>"},{"location":"getting_started/database/quick_start/#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Ensure your database is initialized. This was done in earlier tutorials with the command <code>simmate database reset</code>. Do NOT rerun this command as it will clear your database and erase your results.</p> </li> <li> <p>Navigate to the <code>simmate.database</code> module to see all available tables.</p> </li> <li> <p>The results table for Tutorial 2 is found in the <code>StaticEnergy</code> datatable class, which can be accessed via either of these options: <pre><code># OPTION 1\nfrom simmate.database import connect # this connects to our database\nfrom simmate.database.workflow_results import StaticEnergy\n\n# OPTION 2 (recommended for convenience)\nfrom simmate.workflows.utilities import get_workflow\nworkflow = get_workflow(\"static-energy.vasp.mit\")\ntable = workflow.database_table  # yields the StaticEnergy class\n</code></pre></p> </li> <li> <p>Use <code>show_columns</code> to see all possible table columns <pre><code>table.show_columns()\n</code></pre></p> </li> <li> <p>Convert the full table to a pandas dataframe <pre><code>df = table.objects.to_dataframe()\n</code></pre></p> </li> <li> <p>Use django-based queries to filter results. For example: <pre><code>filtered_results = table.objects.filter(\n    formula_reduced=\"NaCl\", \n    nsites__lte=2,\n).all()\n</code></pre></p> </li> <li> <p>Convert the final structure from a database object (aka <code>DatabaseStructure</code>) to a structure object (aka <code>ToolkitStructure</code>). <pre><code>single_relaxation = StaticEnergy.objects.filter(\n    formula_reduced=\"NaCl\", \n    nsites__lte=2,\n).first()\nnacl_structure = single_relaxation.to_toolkit()\n</code></pre></p> </li> <li> <p>Third-party data is automatically included in the prebuilt database (includes Material Project, AFLOW, COD, and more): <pre><code>from simmate.database import connect\nfrom simmate.database.third_parties import JarvisStructure\n\nfirst_150_rows = JarvisStructure.objects.all()[:150]\ndataframe = first_150_rows.to_dataframe()\n</code></pre></p> </li> </ol>"},{"location":"getting_started/example_scripts/example-001/","title":"Example 001","text":""},{"location":"getting_started/example_scripts/example-001/#about","title":"About","text":"<p>This script queries the Material Project database for all ZnSnF6 structures with spacegroup=148 and then runs a (i) relaxation, (ii) static-energy, and (iii) bandstructure + density of states calculation on each -- passing the results between each step.</p> Key Info Contributor Becca Radomsky Github User @becca9835 Last updated 2023.05.01 Simmate Version v0.13.2"},{"location":"getting_started/example_scripts/example-001/#prerequisites","title":"Prerequisites","text":"<ul> <li> use a postgres database (guide)</li> <li> load the matproj database into your postgres database (guide)</li> <li> start a bunch of simmate workers (or a \"cluster\") (guide)</li> </ul>"},{"location":"getting_started/example_scripts/example-001/#the-script","title":"The script","text":"<p>Info</p> <p>We recommend submitting this script as it's own slurm job! This script will handle submitting other workflows and will finish when ALL workflows finish. </p> <p>Additionally, we run each job below with 8 cores, so our workers are also submitted to a SLURM cluster with n=8.</p> <pre><code>from simmate.database import connect\nfrom simmate.database.third_parties import MatprojStructure\nfrom simmate.workflows.utilities import get_workflow\n\n# filter all the structures you want\nstructures = MatprojStructure.objects.filter(\n    spacegroup=148,\n    formula_reduced=\"ZnSnF6\",\n).all()\n\n# as an extra, you can make this a pandas dataframe that you can easily\n# view in spyder + write to a csv to open up in excel\ndata = structures.to_dataframe()\ndata.to_csv(\"mydata.csv\")\n\n# now let's run some workflows in parallel. To do this,\n# make sure you are using a postgres database and\n# have submitted a bunch of workers.\nrelax_workflow = get_workflow(\"relaxation.vasp.matproj\")\nrelax_jobs = []\nfor structure in structures:\n    status = relax_workflow.run_cloud(\n        structure=structure,\n        command=\"mpirun -n 8 vasp_std &gt; vasp.out\",\n    )\n    relax_jobs.append(status)\n\n# once each job finishes, submit another workflow using the result\nstatic_workflow = get_workflow(\"static-energy.vasp.matproj\")\nstatic_jobs = []\nfor job in relax_jobs:\n    # BUG: This assumes all jobs will complete successfully! You may want a\n    # try/except clause here that catched any jobs that failed.\n    status = static_workflow.run_cloud(\n        structure=job.result(),  # result() here says to wait for the job before to finish\n        command=\"mpirun -n 8 vasp_std &gt; vasp.out\",\n    )\n    static_jobs.append(status)\n\n# and do the same thing again with a band structure + density of states\nelec_workflow = get_workflow(\"electronic-structure.vasp.matproj-full\")\nelec_jobs = []\nfor job in static_jobs:\n    status = elec_workflow.run_cloud(\n        structure=job.result(),  # result() here says to wait for the job before to finish\n        command=\"mpirun -n 8 vasp_std &gt; vasp.out\",\n    )\n    elec_jobs.append(status)\n\n# then you can have the job sit and wait for all results to finish\nresults = [job.result() for job in elec_jobs]\n</code></pre>"},{"location":"getting_started/example_scripts/example-002/","title":"Example 002","text":""},{"location":"getting_started/example_scripts/example-002/#about","title":"About","text":"<p>This script configures and runs a custom NEB workflow that borrows features of path finding, endpoint relaxations, and more.</p> Key Info Contributor Becca Radomsky Github User @becca9835 Last updated 2023.12.30 Simmate Version v0.13.2"},{"location":"getting_started/example_scripts/example-002/#prerequisites","title":"Prerequisites","text":"<ul> <li> Simmate configured with default settings will work!</li> <li> The NEB settings below use CI-NEB, which requires some of the VTST-tools installed. You modify these settings to just use a normal VASP NEB run if you'd like (see the comment <code>Modify these if you don't want CI-NEB</code>).</li> </ul>"},{"location":"getting_started/example_scripts/example-002/#the-script","title":"The script","text":"<p>Info</p> <p>Run this script where you'd like the VASP calculations to run!</p> <p>The script can be called with something like <code>python myscript.py</code> and this line can be on your desktop, inside a SLURM job, or whereever!</p> <pre><code># -*- coding: utf-8 -*-\n\nfrom simmate.apps.vasp.inputs import PBE_POTCAR_MAPPINGS\nfrom simmate.apps.vasp.workflows.base import VaspWorkflow\nfrom simmate.apps.vasp.workflows.diffusion import (\n    NebAllPathsWorkflow,\n    SinglePathWorkflow,\n    VaspNebFromImagesWorkflow,\n)\n\n# -----------------------------------------------------------------------------\n\n# BULK UNITCELL RELAXATION\n\n\nclass Relaxation__Vasp__WarrenLab(VaspWorkflow):\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS\n    _incar = dict(\n        ALGO=\"Fast\",\n        EDIFF=1e-06,\n        ENCUT=520,\n        IBRION=2,\n        ICHARG=1,\n        ISIF=3,\n        ISPIN=2,\n        ISYM=0,\n        IVDW=12,\n        LORBIT=11,\n        LREAL=\"Auto\",\n        LWAVE=False,\n        NELM=200,\n        NELMIN=4,\n        NSW=99,\n        PREC=\"Accurate\",\n        ISMEAR=0,\n        SIGMA=0.05,\n        KSPACING=0.35,\n        LMAXMIX=4,\n    )\n\n\n# -----------------------------------------------------------------------------\n\n# BULK UNITCELL STATIC ENERGY\n\n\nclass StaticEnergy__Vasp__WarrenLab(VaspWorkflow):\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS\n    _incar = dict(\n        ALGO=\"Fast\",\n        EDIFF=1e-06,\n        ENCUT=520,\n        IBRION=-1,\n        ICHARG=1,\n        ISIF=3,\n        ISPIN=2,\n        ISYM=0,\n        IVDW=12,\n        LORBIT=11,\n        LREAL=\"Auto\",\n        LWAVE=False,\n        NELM=200,\n        NELMIN=4,\n        NSW=0,\n        PREC=\"Accurate\",\n        ISMEAR=0,\n        SIGMA=0.05,\n        KSPACING=0.35,\n        LMAXMIX=4,\n    )\n\n\n# -----------------------------------------------------------------------------\n\n# ENDPOINT SUPERCELL RELAXATIONS\n\n\nclass Relaxation__Vasp__WarrenLabNebEndpoint(VaspWorkflow):\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS\n    _incar = dict(\n        ALGO=\"Fast\",\n        EDIFF=5e-05,\n        EDIFFG=-0.01,\n        ENCUT=520,\n        IBRION=2,\n        ICHARG=1,\n        ISIF=2,\n        ISPIN=2,\n        ISYM=0,\n        IVDW=12,\n        LORBIT=11,\n        LREAL=\"Auto\",\n        LWAVE=False,\n        LCHARG=False,\n        NELM=200,\n        NSW=99,\n        PREC=\"Accurate\",\n        ISMEAR=0,\n        SIGMA=0.05,\n        KSPACING=0.4,\n        LMAXMIX=4,\n    )\n\n\n# -----------------------------------------------------------------------------\n\n# ENDPOINT SUPERCELL STATIC ENERGY\n\n\nclass StaticEnergy__Vasp__WarrenLabNebEndpoint(VaspWorkflow):\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS\n    _incar = dict(\n        ALGO=\"Fast\",\n        EDIFF=5e-05,\n        EDIFFG=-0.01,\n        ENCUT=520,\n        IBRION=-1,\n        ICHARG=1,\n        ISIF=2,\n        ISPIN=2,\n        ISYM=0,\n        IVDW=12,\n        LORBIT=11,\n        LREAL=\"Auto\",\n        LWAVE=False,\n        LCHARG=False,\n        NELM=200,\n        NSW=0,\n        PREC=\"Accurate\",\n        ISMEAR=0,\n        SIGMA=0.05,\n        KSPACING=0.4,\n        LMAXMIX=4,\n    )\n\n\n# -----------------------------------------------------------------------------\n\n# NEB FROM IMAGES\n\n\nclass Diffusion__Vasp__WarrenLabCiNebFromImages(VaspNebFromImagesWorkflow):\n    functional = \"PBE\"\n    potcar_mappings = PBE_POTCAR_MAPPINGS\n    _incar = dict(\n        ALGO=\"Fast\",\n        EDIFF=5e-05,\n        EDIFFG=-0.01,\n        ENCUT=520,\n        IBRION=3,\n        ICHARG=1,\n        ISIF=2,\n        ISPIN=2,\n        ISYM=0,\n        IVDW=12,\n        LORBIT=11,\n        LREAL=\"Auto\",\n        LWAVE=False,\n        LCHARG=False,\n        NELM=200,\n        NSW=99,\n        PREC=\"Accurate\",\n        ISMEAR=0,\n        SIGMA=0.05,\n        KSPACING=0.4,\n        LMAXMIX=4,\n        NIMAGES__auto=True,  # set automatically by simmate\n        # Modify these if you don't want CI-NEB\n        LCLIMB=True,\n        SPRING=-5,\n        POTIM=0,\n        IOPT=1,\n    )\n\n\n# -----------------------------------------------------------------------------\n# The two sections below use Simmate to piece together our individual\n# VASP calculations above.\n# -----------------------------------------------------------------------------\n\n# SINGLE PATH NEB\n\n\nclass Diffusion__Vasp__WarrenLabNebSinglePath(SinglePathWorkflow):\n    endpoint_relaxation_workflow = Relaxation__Vasp__WarrenLabNebEndpoint\n    endpoint_energy_workflow = StaticEnergy__Vasp__WarrenLabNebEndpoint\n    from_images_workflow = Diffusion__Vasp__WarrenLabCiNebFromImages\n\n\n# -----------------------------------------------------------------------------\n\n# ALL PATHS NEB\n\n\nclass Diffusion__Vasp__NebAllPathsWarrenLab(NebAllPathsWorkflow):\n    bulk_relaxation_workflow = Relaxation__Vasp__WarrenLab\n    bulk_static_energy_workflow = StaticEnergy__Vasp__WarrenLab\n    single_path_workflow = Diffusion__Vasp__WarrenLabNebSinglePath\n\n\n# -----------------------------------------------------------------------------\n# Now that we have our new &amp; custom NEB workflow, we can run that\n# full workflow analysis. Here, we just run it locally and one vasp calc\n# at a time.\n# -----------------------------------------------------------------------------\n\n# now run the workflow!\nresult = Diffusion__Vasp__NebAllPathsWarrenLab.run(\n    structure=\"example.cif\",\n    migrating_specie=\"Li\",\n    command=\"mpirun -n 10 vasp_std &gt; vasp.out\",  # make sure -n is divisible by nimages!\n    # Then any extra optional settings below.\n    # These parameters are automatically available thanks\n    # to the configuration we did above.\n    relax_bulk=True,\n    relax_endpoints=True,\n    nimages=5,\n    min_supercell_atoms=80,\n    max_supercell_atoms=240,\n    min_supercell_vector_lengths=10,\n    max_path_length=5,\n    percolation_mode=\"&gt;1d\",\n    vacancy_mode=True,\n)\n</code></pre>"},{"location":"getting_started/example_scripts/example_template/","title":"Example 001","text":""},{"location":"getting_started/example_scripts/example_template/#about","title":"About","text":"<p>This script does x, y, z using a, b, c...</p> Key Info Contributor Jack Sundberg Github User @jacksund Last updated 2023.05.31 Simmate Version v0.13.2"},{"location":"getting_started/example_scripts/example_template/#prerequisites","title":"Prerequisites","text":"<p>(note: these are common prerequisites for complex setups so we keep these below)</p> <ul> <li> use a postgres database (guide)</li> <li> load the matproj database into your postgres database (guide))</li> <li> start a bunch of simmate workers (or a \"cluster\") (guide)</li> </ul>"},{"location":"getting_started/example_scripts/example_template/#the-script","title":"The script","text":"<p>Info</p> <p>We recommend running this script as ....</p> <pre><code># paste in your python script here!\n</code></pre>"},{"location":"getting_started/example_scripts/overview/","title":"Example Scripts from Users","text":"<p>This section contains example Python scripts demonstrating the use of Simmate.</p> <p>Warning</p> <p>These examples do NOT cover all the features of Simmate, but they can be useful for beginners. </p> <p>Before referring to these examples, make sure to check the \"Full Guides\" and \"Apps\" sections for help and guides.</p> Script Name Description Example-001 Demonstrates relaxation, static-energy, and bandstruct/DOS calculations on a selected set of structures from the Materials Project. Example-002 Illustrates how to set up and execute a custom NEB workflow incorporating features of path finding, endpoint relaxations, and more."},{"location":"getting_started/installation/command_line/","title":"Transitioning to the Command-line","text":"<p>While the Anaconda Navigator interface is user-friendly for beginners, using the command-line is faster and more efficient. Don't panic, it's simpler than it seems. Even if you're not familiar with coding, you can learn the command-line basics in a few minutes.</p>"},{"location":"getting_started/installation/command_line/#executing-our-first-command","title":"Executing our first command","text":"<p>Let's get started with our command-line.</p> <ul> <li>On Windows, search for and open \"Anaconda Powershell Prompt\" using your Start menu.</li> <li>On Mac and Linux, search for and open the app named \"Terminal\"</li> </ul> <p>You should see something like this:</p> <p> </p> <p>You'll notice <code>(base)</code> at the start of the line. This represents our current anaconda environment. Following that, you'll see the \"current working directory\", which is the folder we currently have open. On Windows, this will be your user folder (e.g. <code>C:\\Users\\jacksund</code>) and for Mac/Linux, you'll see <code>~</code> which is shorthand for your user folder (e.g. <code>home/jacksund</code>).</p> <p>Now, type in the command <code>cd Desktop</code> and press enter. This will open your Desktop folder. Then enter the command <code>ls</code>, which will list all files and folders on your Desktop. </p> <pre><code># run these two commands\ncd Desktop\nls\n</code></pre> <p>Tip</p> <p>Think of each command as a \"button\". For instance, the command <code>cd</code> stands for \"change directory\". When you use it, it opens a new folder to view its contents -- similar to double-clicking a folder to open it.</p>"},{"location":"getting_started/installation/command_line/#mastering-new-commands","title":"Mastering new commands","text":"<p>Tip</p> <p>For more basic commands, refer to this cheat sheet or take a comprehensive tutorial. Remembering commands will come gradually, so keep this cheat-sheet handy. We strongly suggest that you spend 30 minutes going through these links after completing this tutorial.</p> <p>The challenging part with the command-line is knowing what to type. However, most programs have a single command that forms the base of more complex commands. For anaconda, the command is <code>conda</code>. If you're unsure about its function or usage, simply add <code>--help</code> to it. Type in the command <code>conda --help</code> and you'll see an output like this:</p> <pre><code>conda --help\n</code></pre> <pre><code>usage: conda [-h] [-V] command ...\n\nconda is a tool for managing and deploying applications, environments and packages.\n\nOptions:\n\npositional arguments:\n  command\n    clean        Remove unused packages and caches.\n    compare      Compare packages between conda environments.\n    config       Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .\n    ...\n\noptional arguments:\n  -h, --help     Show this help message and exit.\n  -V, --version  Show the conda version number and exit.\n\nconda commands available from other packages:\n  build\n  content-trust\n  ...\n</code></pre> <p>Don't get overwhelmed by the amount of information displayed. Each line conveys a simple concept. </p> <p>For instance, the line <code>-h, --help     Show this help message and exit.</code> explains what the <code>conda --help</code> command does! It also indicates that we could have used <code>conda -h</code> for the same output.</p> <p>This help message also reveals other \"subcommands\" available. One is <code>create</code> which creates a new environment. To learn more about that, we can run the command <code>conda create --help</code>. </p> <p>There's a lot here... But remember, you don't need to memorize all of this. Just remember how to access this help page when you need it. Next, we'll use these commands to create our environment and install Simmate.</p>"},{"location":"getting_started/installation/create_your_env/","title":"Setting Up Your Environment and Installing Simmate","text":"<p>To begin, we'll establish a new environment using the conda-forge channel. Anaconda uses channels to download packages, and for simplicity, we'll consistently use conda-forge, a standard practice in the Python community.</p>"},{"location":"getting_started/installation/create_your_env/#1-environment-creation","title":"1. Environment Creation","text":"<p>Execute the command below to create your environment. You can replace <code>my_env</code> with any name of your choice, but remember to use underscores instead of spaces (<code>my_env</code> is acceptable, <code>my env</code> will cause an error).</p> <pre><code>conda create -c conda-forge -n my_env python=3.11\n</code></pre> <p>Confirm the installation when prompted.</p>"},{"location":"getting_started/installation/create_your_env/#2-environment-activation","title":"2. Environment Activation","text":"<p>Switch to the newly created environment using the following command:</p> <pre><code>conda activate my_env\n</code></pre> <p>If the operation is successful, the start of your command line will change from <code>(base)</code> to <code>(my_env)</code>.</p>"},{"location":"getting_started/installation/create_your_env/#3-simmate-installation","title":"3. Simmate Installation","text":"<p>Now, let's install Simmate.</p> <pre><code>conda install -c conda-forge -n my_env simmate\n</code></pre> <p>The installation may take a few minutes. Once completed, Simmate is successfully installed! </p> <p>If you encounter any errors with this command, please inform our team immediately by posting a new issue.</p>"},{"location":"getting_started/installation/create_your_env/#4-spyder-installation","title":"4. Spyder Installation","text":"<p>Lastly, let's install Spyder using Anaconda. We'll use Spyder for writing Python in later tutorials:</p> <pre><code>conda install -c conda-forge -n my_env spyder\n</code></pre>"},{"location":"getting_started/installation/explore_simmate_cli/","title":"Navigating Simmate's Command-Line Interface","text":"<p>Just as we used <code>conda --help</code> earlier, we can also request assistance with Simmate. Begin by executing the command <code>simmate --help</code>. The following output should be displayed:</p> <pre><code>simmate --help\n</code></pre> <pre><code> Usage: simmate [OPTIONS] COMMAND [ARGS]...                                                                     \n\n This is the primary command from which all other Simmate commands originate. If you're new to the command line, we recommend starting with our tutorials. Below, you'll find a list of sub-commands to try. For instance, you can run `simmate database --help` to learn more about it.                  \n\n TIP: Many Simmate commands are lengthy and verbose. You can use --install-completion to add ipython-like autocomplete to your shell.                                                                                    \n\nOptions:\n--install-completion          Install completion for the current shell.                                      \n--show-completion             Show completion for the current shell, to copy it or customize the installation.                                                                  \n--help                        Show this message and exit.                                                    \n\nCommands:\ndatabase         A group of commands for managing your database                                              \nengine           A group of commands for starting up computational resources (Workers, Agents, and Clusters)          \nrun-server       Runs a local test server for the Simmate website interface                                  \nstart-project    Creates a new folder and fills it with an example project to get you started with custom Simmate workflows/datatables                                                                \nutilities        A group of commands for various simple tasks (such as file handling)                        \nworkflows        A group of commands for running workflows or viewing their settings                         \n</code></pre> <p>As you can see, there are numerous other commands like <code>simmate database</code> and <code>simmate workflows</code> that we will delve into in subsequent tutorials.</p>"},{"location":"getting_started/installation/install_anaconda/","title":"Installing Anaconda","text":""},{"location":"getting_started/installation/install_anaconda/#why-anaconda","title":"Why Anaconda?","text":"<p>In an ideal world, you could download Simmate like any other desktop app and be ready to go. However, have you ever updated your computer only to find that everything else on your computer goes haywire? This is a common occurrence with Python, so we need to tread carefully. For instance, consider the following scenario:</p> <ol> <li>Simmate which requires Python version 3.10 or greater</li> <li>A separate package which requires Python version 2.7</li> </ol> <p>The conflicting Python versions pose a problem. To address this, we use Anaconda. Anaconda installs Python and all our additional packages, including Simmate. To ensure nothing breaks, it segregates each of our installations into folders known as \"environments\". </p> <p>Anaconda also prevents the installation of conflicting package versions within a single environment.</p> <p>Example</p> <p>With the two programs mentioned above, we could have two environments: one named <code>simmate_env</code> and another named <code>other_env</code> (you can name them anything). The two different Python versions and codes would be installed into separate folders to prevent interaction.</p>"},{"location":"getting_started/installation/install_anaconda/#installing-anaconda-and-a-first-look","title":"Installing Anaconda and a first look","text":"<p>In this tutorial, we'll install Anaconda on your local desktop/laptop. Even if you plan to use a university supercomputer (or any other remote computer system) to run workflows, stick to your local computer for now. We'll transition to your remote supercomputer in a subsequent tutorial.</p>"},{"location":"getting_started/installation/install_anaconda/#1-install-anaconda","title":"1. Install Anaconda","text":"<p>To install Anaconda, you don't need to create an account on their website. Simply visit their download page and install Anaconda. Stick to the default options during installation.</p>"},{"location":"getting_started/installation/install_anaconda/#2-open-anaconda","title":"2. Open Anaconda","text":"<p>After the download is complete, launch the application, which will be named Anaconda Navigator.</p> <p>On the home screen, you'll see several IDEs, such as Orange3, Jupyter Notebook, Spyder, and others. These IDEs are for writing your own Python code. Just as there are multiple platforms like Microsoft Word, Google Docs, LibreOffice for writing documents, these IDEs offer different ways to write Python. We recommend Spyder, which we will introduce in a later tutorial.</p> <p> </p>"},{"location":"getting_started/installation/install_anaconda/#3-view-environments","title":"3. View environments","text":"<p>On the left side of the application window, you'll find an Environments tab. Click on it. When you first install Anaconda, there will only be a \"base\" environment with popular packages already installed. You can create new environments here and install new packages into each -- all without affecting what's already installed.</p> <p> </p>"},{"location":"getting_started/installation/install_anaconda/#final-comments","title":"Final comments","text":"<p>That's all there is to the Anaconda interface! While we can install Simmate using this interface, it's actually simpler with the command-line. The rest of this tutorial will use the command-line instead of the Anaconda Navigator interface.</p> <p>Tip</p> <p>If you want a more comprehensive overview of Anaconda, they offer a series of getting-started guides. However, these guides aren't necessary for using Simmate (so don't spend more than 10 minutes browsing through them).</p>"},{"location":"getting_started/installation/local_server_setup/","title":"Launching Your Local Test Server","text":"<p>Our official website allows you to view all past workflow results. Even if you haven't run any workflows yet, you can replicate this on your local computer using two simple commands.</p>"},{"location":"getting_started/installation/local_server_setup/#1-reset-the-database","title":"1. Reset the Database","text":"<p>Firstly, we need to configure our database. We'll delve into the specifics in the next tutorial, but for now, consider this as creating an empty Excel spreadsheet that we'll populate with data later. This can be done with...</p> <pre><code>simmate database reset # (1)\n</code></pre> <ol> <li>When prompted, confirm that you wish to reset/delete your \"old\" database. Also, agree to download and utilize a prebuilt database.</li> </ol> <p>Tip</p> <p>You won't need to run this command again unless you want to erase all your data and start anew.</p>"},{"location":"getting_started/installation/local_server_setup/#2-start-the-server","title":"2. Start the Server","text":"<p>For our second step, we simply instruct Simmate to launch the server:</p> <pre><code>simmate run-server\n</code></pre> <p>... and after a few moments, you should see the following output ...</p> <pre><code>Watching for file changes with StatReloader\nApril 05, 2022 - 00:06:54\nDjango version 4.0.2, using settings 'simmate.configuration.django.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CTRL-BREAK.\n</code></pre> <p>Keep this command running in your terminal and open the link http://127.0.0.1:8000/ in your preferred browser (Chrome, Firefox, etc.). You should see a site that resembles the simmate.org website! </p>"},{"location":"getting_started/installation/local_server_setup/#can-i-share-these-links","title":"Can I Share These Links?","text":"<p>This website is hosted on your local computer. It's not accessible via the internet, and it will cease to function as soon as you close your terminal running the <code>simmate run-server</code> command.</p> <p>However, Simmate's true potential is unleashed when we transition to a server that's accessible via the internet. This allows you to share results and computational resources with your entire team. Additionally, Python and command-line offer many more features than the website interface. To discover these, continue reading our tutorials! </p>"},{"location":"getting_started/installation/quick_start/","title":"Installation","text":""},{"location":"getting_started/installation/quick_start/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Download and install anaconda</p> </li> <li> <p>Set up a conda environment, install Simmate within it, and activate it. (Note: We recommend using Spyder as your IDE, but it's not mandatory) <pre><code>conda create -n my_env -c conda-forge python=3.11 simmate\nconda install -n my_env -c conda-forge spyder  # optional but recommended\nconda activate my_env\n</code></pre></p> </li> <li> <p>Run the help command to verify the installation <pre><code>simmate --help\n</code></pre></p> </li> <li> <p>For first-time setup, initialize your local database (SQLite) <pre><code>simmate database reset\n</code></pre></p> </li> <li> <p>Start the local dev server and keep this command running <pre><code>simmate run-server\n</code></pre></p> </li> <li> <p>Visit http://127.0.0.1:8000/ to access your local server!</p> </li> </ol> <p>Note</p> <p>While Simmate itself is less than 1MB, the total download size for Simmate and all its dependencies in a fresh conda environment is approximately 1GB. Additional storage space may be required for optional downloads, such as third-party data.</p> <p>Tip</p> <p>If the environment takes more than 2 minutes to resolve, you might be using an outdated version of conda.</p> <p>Ensure your conda version is updated (&gt;=23.10.0) to utilize the new libmamba solver.</p>"},{"location":"getting_started/toolkit/advanced_classes/","title":"Advanced Classes","text":""},{"location":"getting_started/toolkit/advanced_classes/#exploring-beyond-the-structure-class","title":"Exploring Beyond the Structure Class","text":"<p>This section provides a glimpse into some advanced classes and functionalities. Please note that Simmate is still in its early development stages, and there are many more features available through the PyMatGen and MatMiner packages, which were installed alongside Simmate.</p> <p>Tip</p> <p>If you're attempting to follow a paper and analyze a structure, chances are there's a class/function already created for that analysis! Make sure to explore the documentation (the next tutorial will guide you on how to do this) or simply post a question and we'll direct you to the right path.</p>"},{"location":"getting_started/toolkit/advanced_classes/#example-1-creating-structures","title":"Example 1: Creating Structures","text":"<p>Currently, Simmate's toolkit is most effective for structure creation. This includes generating structures from random symmetry, prototype structures, and more. All we need to do is provide these \"creator\" classes with a composition object:</p> <pre><code>from simmate.toolkit import Composition\nfrom simmate.toolkit.creators import RandomSymStructure\n\ncomposition = Composition(\"Ca2N\")\ncreator = RandomSymStructure(composition)\n\nstructure1 = creator.create_structure(spacegroup=166)\nstructure2 = creator.create_structure(spacegroup=225)\n</code></pre>"},{"location":"getting_started/toolkit/advanced_classes/#example-2-fingerprint-analysis","title":"Example 2: Fingerprint Analysis","text":"<p>Matminer is especially useful for analyzing a structure's \"features\" and creating machine-learning inputs. A common analysis provides the structure's \"fingerprint\", which aids in characterizing bonding in the crystal. The most basic fingerprint is the radial distribution function (rdf) -- it displays the distance between all atoms. We can take any structure object and feed it to a matminer <code>Featurizer</code> object:</p> <pre><code>from matminer.featurizers.structure.rdf import RadialDistributionFunction\n\nrdf_analyzer = RadialDistributionFunction(bin_size=0.1)\n\nrdf1 = rdf_analyzer.featurize(structure1)\nrdf2 = rdf_analyzer.featurize(structure2)\n</code></pre> <p>We can also plot an RDF using python. Since Matminer doesn't currently offer a convenient way to plot this (with Simmate, there would be a <code>show_plot()</code> method), we can use this opportunity to learn how to plot things ourselves:</p> <pre><code>import matplotlib.pyplot as plt\n\n# The x-axis ranges from 0.1 to 20 in steps of 0.1 (in Angstroms).\n# Matminer doesn't provide a list of these values but\n# we can generate it using this line.\nrdf_x = [n/10 + 0.1 for n in range(0, 200)]\n\n# Create a simple line plot with lists of (x,y) values\nplt.plot(rdf_x, rdf1)\n\n# Display the plot without any additional formatting or labels.\nplt.show()\n</code></pre>"},{"location":"getting_started/toolkit/advanced_classes/#example-3-matching-structures","title":"Example 3: Matching Structures","text":"<p>Pymatgen is currently the most extensive package and offers the most toolkit-like features. For instance, it's common to compare two structures to determine if they are symmetrically equivalent (within a given tolerance). You provide it with two structures, and it will return True or False based on whether they match:</p> <pre><code>from pymatgen.analysis.structure_matcher import StructureMatcher\n\nmatcher = StructureMatcher()\n\n# Now let's compare our two random structures!\n# This should return False. You can verify this in your Spyder variable explorer.\nis_matching = matcher.fit(structure1, structure2)  \n</code></pre>"},{"location":"getting_started/toolkit/intro_to_spyder/","title":"An Introduction to Spyder","text":"<p>Prebuilt workflows are beneficial for materials research, but they have their limitations. When you need to create new crystal structures, modify existing ones, or conduct unique analyses, Python becomes an essential tool. </p>"},{"location":"getting_started/toolkit/intro_to_spyder/#python-introductory-lessons","title":"Python Introductory Lessons","text":"<p>If you're new to coding and Python, we'll introduce the basic Python skills needed to use Simmate's toolkit in this tutorial. However, we strongly recommend dedicating 2-3 days to learn Python fundamentals. Codecademy's Python lessons are an excellent resource for beginners.</p> <p>Tip</p> <p>Investing a full day or more in these tutorials will save you time and frustration in the long run. We strongly encourage you to take the time to complete them.</p>"},{"location":"getting_started/toolkit/intro_to_spyder/#choosing-our-ide","title":"Choosing our IDE","text":"<p>Whether you're ready or not, it's time to learn how to use Simmate's Python code. </p> <p>Remember from the Installation tutorial: Anaconda provided several programs on their home screen, including Orange3, Jupyter Notebook, Spyder, and others. These programs allow you to write your own Python code. Just as there are various platforms for writing papers, like Microsoft Word, Google Docs, and LibreOffice, these programs offer different ways to write Python. </p> <p>Our team prefers Spyder, and we highly recommend it to our users.</p>"},{"location":"getting_started/toolkit/intro_to_spyder/#launching-spyder","title":"Launching Spyder","text":"<p>If you followed the installation tutorial correctly, Spyder should be installed and ready to use. To launch it, search for Spyder in your computer's apps (use the search bar at the bottom-left of your screen on Windows 10) and select <code>Spyder (my_env)</code>. </p> <p>Spyder will be empty when you first launch it. Here's a glimpse of what Spyder looks like when it's in use:</p> <p> </p> <p>For this tutorial, we'll only be using the Python console (located at the bottom-right of the screen).</p> <p>Tip</p> <p>If you're already comfortable with Python or have completed the Codecademy intro course, you can quickly familiarize yourself with Spyder by watching their intro videos. There are 3 videos, each under 4 minutes long.</p>"},{"location":"getting_started/toolkit/quick_start/","title":"Analyzing &amp; Modifying Structures","text":""},{"location":"getting_started/toolkit/quick_start/#quick-start","title":"Quick Start","text":"<p>Tip</p> <p>Simmate is built on pymatgen. Therefore, this tutorial also serves as a guide to using their package. Explore their guides for extra help.</p> <ol> <li> <p>Ensure you have the <code>POSCAR</code> file of NaCl from the previous tutorial.</p> </li> <li> <p>You can load the structure into python and then convert it to another file format: <pre><code>from simmate.toolkit import Structure\n\nstructure = Structure.from_file(\"POSCAR\")\nstructure.to(filename=\"NaCl.cif\", fmt=\"cif\")\n</code></pre></p> </li> <li> <p>You can run workflows by providing a filename or <code>Structure</code> object <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.mit\")\n\n# option 1\nworkflow.run(structure=\"POSCAR\")\n\n# option 2\nworkflow.run(structure=structure)  # uses var from previous step\n</code></pre></p> </li> <li> <p>Access various properties of the structure, lattice, and composition: <pre><code># explore structure-based properties\nstructure.density\nstructure.distance_matrix\nstructure.cart_coords\nstructure.num_sites\n\n# access the structure's composition and its properties\ncomposition = structure.composition\ncomposition.reduced_formula\ncomposition.elements\n\n# access the structure's lattice and its properties\nlattice = structure.lattice\nlattice.volume\nlattice.matrix\nlattice.beta\n</code></pre></p> </li> <li> <p>Create new structures using some transformation or analysis: <pre><code>structure.add_oxidation_state_by_guess()\nstructure.make_supercell([2,2,2])\nnew_structure = structure.get_primitive_structure()\n</code></pre></p> </li> </ol>"},{"location":"getting_started/toolkit/quick_start/#extra","title":"Extra","text":"<p>Looking for advanced features? Simmate is gradually incorporating these into our toolkit module, but many more are available through PyMatGen and MatMiner (which are preinstalled for you).</p> <pre><code># Simmate is in the process of adding new features. One example\n# creating a random structure from a spacegroup and composition\n\nfrom simmate.toolkit import Composition\nfrom simmate.toolkit.creators import RandomSymStructure\n\ncomposition = Composition(\"Ca2N\")\ncreator = RandomSymStructure(composition)\n\nstructure1 = creator.create_structure(spacegroup=166)\nstructure2 = creator.create_structure(spacegroup=225)\n\n# ----------------------------------------------------------------------\n\n# Matminer is useful for analyzing structures and creating\n# machine-learning inputs. One common analysis is the generating\n# a RDF fingerprint to help analyze bonding and compare structures\n\nfrom matminer.featurizers.structure.rdf import RadialDistributionFunction\n\nrdf_analyzer = RadialDistributionFunction(bin_size=0.1)\n\nrdf1 = rdf_analyzer.featurize(structure1)\nrdf2 = rdf_analyzer.featurize(structure2)\n\n# ----------------------------------------------------------------------\n\n# Pymatgen currently offers the most functionality. One common\n# function is checking if two structures are symmetrically\n# equivalent (under some tolerance).\n\nfrom pymatgen.analysis.structure_matcher import StructureMatcher\n\nmatcher = StructureMatcher()\n\n# Now compare our two random structures!\n# This should give False. Check in your Spyder variable explorer.\nis_matching = matcher.fit(structure1, structure2)\n</code></pre> <p>There are many more features available! If you can't find what you're looking for, don't hesitate to ask for help before trying to code something on your own. The feature you're looking for probably exists somewhere -- and if we don't have it, we'll guide you to a package that does.</p>"},{"location":"getting_started/toolkit/structure_methods/","title":"<code>Structure</code> Object Methods","text":""},{"location":"getting_started/toolkit/structure_methods/#understanding-properties-and-methods","title":"Understanding Properties and Methods","text":"<p>A <code>Structure</code> object can have both <code>properties</code> and <code>methods</code>. While a property is a characteristic of an object, a method performs a specific task. For instance, the <code>get_primitive_structure()</code> method can convert a conventional unit cell into a primitive unit cell. Here's how you can use it:</p> <pre><code>nacl_structure.get_primitive_structure()\n</code></pre> <p>This command will display a new structure, which should match the primitive structure we already have.</p>"},{"location":"getting_started/toolkit/structure_methods/#storing-method-output-in-a-new-variable","title":"Storing Method Output in a New Variable","text":"<p>You can store the output of a method in a new <code>Structure</code> object. For instance, you can assign the output of the <code>get_primitive_structure()</code> method to a new object called <code>nacl_prim</code>:</p> <pre><code>nacl_prim = nacl_structure.get_primitive_structure()\nnacl_prim.density\n</code></pre>"},{"location":"getting_started/toolkit/structure_methods/#modifying-methods-with-parameters","title":"Modifying Methods with Parameters","text":"<p>All methods end with parentheses <code>()</code>, which allow you to modify the method. For example, the <code>get_primitive_structure()</code> method uses symmetry in its calculations. You can adjust the tolerance for symmetry with:</p> <pre><code>nacl_structure.get_primitive_structure(tolerance=0.1)\n</code></pre> <p>This command will identify atoms as symmetrical if they are nearly in their 'symmetrically correct' positions (within 0.1 Angstrom). If you don't specify a tolerance, the method will use a default value. Some methods, like <code>make_supercell</code>, require you to specify parameters, such as the supercell size. </p> <p>Here are some other methods you can use with structures:</p> <pre><code>nacl_structure.add_oxidation_state_by_guess()\nnacl_structure.make_supercell([2,2,2])\n</code></pre>"},{"location":"getting_started/toolkit/structure_methods/#discovering-other-methods-and-properties","title":"Discovering Other Methods and Properties","text":"<p>To see all the available properties and methods, type <code>nacl_structure.</code> into the terminal and press <code>tab</code>. A list of options will appear. </p> <p>You can also explore the properties and methods of other classes, such as <code>lattice</code>, using the same method. The list should look something like this (note that this image is not for a structure object):</p> <p> </p> <p>While <code>Structure</code> is the most commonly used class in Simmate, there are many others. To fully understand all the options for these classes, refer to the code's documentation, which we will discuss in the next guide.</p>"},{"location":"getting_started/toolkit/structure_properties/","title":"Understanding the <code>Structure</code> Object Properties","text":""},{"location":"getting_started/toolkit/structure_properties/#the-role-of-classes-in-defining-properties","title":"The Role of Classes in Defining Properties","text":"<p>Classes are created to automate routine calculations. </p> <p>For instance, all structures possess a <code>density</code> property, which can be computed once the lattice and atomic sites are known. The formula for this calculation remains constant, allowing for automation. </p> <p>Access this and other properties through the structure object. For example, input <code>nacl_structure.density</code> in the python terminal and press enter to display the density of the structure:</p> <pre><code>nacl_structure.density\n</code></pre>"},{"location":"getting_started/toolkit/structure_properties/#exploring-lattice-properties","title":"Exploring Lattice Properties","text":"<p>What about other lattice properties such as volume, angles, and vectors? </p> <p>For better organization, the <code>Structure</code> class includes an associated class called <code>Lattice</code>. Within the <code>lattice</code> object, properties like <code>volume</code> can be found. Test these in your python terminal (run one line at a time):</p> <pre><code>nacl_structure.lattice.volume\nnacl_structure.lattice.matrix\nnacl_structure.lattice.beta\n</code></pre> <p>The expected outputs are...</p> <pre><code># EXPECTED OUTPUTS\n\n46.09614820053437\n\narray([[3.48543651, 0.        , 2.01231771],\n       [1.16181217, 3.28610106, 2.01231771],\n       [0.        , 0.        , 4.02463542]])\n\n59.99999999999999\n</code></pre> <p>For convenience, you can use a shortcut. Save the <code>Lattice</code> object to a new variable name (here, it's <code>nacl_lat</code>, but you can choose a different name) and then call its properties:</p> <pre><code>nacl_lat = nacl_structure.lattice\nnacl_lat.volume\nnacl_lat.matrix\nnacl_lat.beta\n</code></pre> <p>The outputs will be identical to the previous ones.</p>"},{"location":"getting_started/toolkit/structure_properties/#investigating-composition-properties","title":"Investigating Composition Properties","text":"<p>The same concept can be applied to other <code>Structure</code> sub-classes, such as <code>Composition</code>. This enables us to view properties related to composition: <pre><code>nacl_compo = nacl_structure.composition\nnacl_compo.reduced_formula\nnacl_compo.elements\n</code></pre></p> <p>The expected outputs are...</p> <pre><code># EXPECTED OUTPUTS\n\nComp: Na1 Cl1\n\n'NaCl'\n\n[Element Na, Element Cl]  # &lt;-- these are Element objects!\n</code></pre>"},{"location":"getting_started/toolkit/structure_properties/#managing-variables","title":"Managing Variables","text":"<p>As you create new python objects and assign them different names, you'll need a way to keep track of them. Spyder's variable explorer (located in the top right window tab) can help with this! Try double-clicking on some of your variables to explore what Spyder can do:</p> <p> </p>"},{"location":"getting_started/toolkit/the_structure_class/","title":"The <code>Structure</code> Class","text":""},{"location":"getting_started/toolkit/the_structure_class/#understanding-classes-in-python","title":"Understanding Classes in Python","text":"<p>Python \"classes\" and \"objects\" are key concepts in Python programming. </p> <p>Consider this analogy: McDonald's, Burger King, and Wendy's are examples of restaurants. In Python, we could say that <code>mcdonalds</code>, <code>burgerking</code>, and <code>wendys</code> are objects of the class <code>Restaurants</code>.  </p> <p>By grouping objects into classes, Python streamlines programming. For instance, we could design the <code>Restaurants</code> class to have a property called <code>menu</code>. Then, we could view the menu simply by typing <code>wendys.menu</code>. This sets a rule that the menu info can be accessed with <code>example_restaurant.menu</code> for any restaurant.</p> <p>This becomes incredibly powerful when we start building out functionality and analyses.</p>"},{"location":"getting_started/toolkit/the_structure_class/#importing-the-structure-class","title":"Importing the Structure Class","text":"<p>In materials science, the most commonly used class is for crystal structures. In Simmate, this class is called <code>Structure</code>. A crystal structure always consists of a lattice and a list of atomic sites. This is exactly what we have in our <code>POSCAR</code> file from tutorial 2, so let's use Simmate to create an object of the <code>Structure</code>.</p> <p>Enter this line into the Python console:</p> <pre><code>from simmate.toolkit import Structure\n</code></pre> <p>This line imports the <code>Structure</code> class from Simmate's code. The <code>Structure</code> class is now loaded into memory and ready for use.</p>"},{"location":"getting_started/toolkit/the_structure_class/#loading-a-structure-from-a-file","title":"Loading a Structure from a File","text":"<p>Next, ensure you have the correct working directory (as we did with the command-line). Spyder displays this in the top right, and you can change it by clicking the folder icon. We want to be in the same folder as our POSCAR file. Run this line in your Python terminal:</p> <pre><code>nacl_structure = Structure.from_file(\"POSCAR\")\n</code></pre> <p>Here, we're telling Python that we have a <code>Structure</code> and its information is located in the <code>POSCAR</code> file. This could be in many other formats, such as a CIF file. But now we have a <code>Structure</code> object named <code>nacl_structure</code>. To verify it loaded correctly, run this line:</p> <pre><code>nacl_structure\n</code></pre> <p>It should print out the same information from our POSCAR. </p>"},{"location":"getting_started/toolkit/the_structure_class/#creating-a-structure-manually-in-python","title":"Creating a Structure Manually in Python","text":"<p>Alternatively, we could have created this structure manually:</p> <pre><code>s = Structure(\n    lattice=[\n        [3.48543651, 0.0, 2.01231771],\n        [1.16181217, 3.28610106, 2.01231771],\n        [0.0, 0.0, 4.02463542],\n    ],\n    species=[\"Na\", \"Cl\"],\n    coords=[\n        [0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.5000],\n    ],\n)\n</code></pre>"},{"location":"getting_started/toolkit/the_structure_class/#using-the-structure-in-a-workflow","title":"Using the Structure in a Workflow","text":"<p>Regardless of how you created a structure, we now have our <code>Structure</code> object and we can use its properties (and methods) to simplify our calculations.</p> <p>For instance, we can use it to run a workflow. We did this with the command-line in the last tutorial but can accomplish the same thing with Python:</p> python <pre><code>from simmate.toolkit import Structure\nfrom simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.mit\")\n\nnacl_structure = Structure.from_file(\"POSCAR\")\n\nresult = workflow.run(structure=nacl_structure)\n</code></pre> <p>If we don't need to modify the structure, we could have just given the filename to our workflow:</p> pythonyaml <pre><code>from simmate.workflows.utilities import get_workflow\n\nworkflow = get_workflow(\"static-energy.vasp.mit\")\nresult = workflow.run(structure=\"POSCAR\")\n</code></pre> <pre><code>workflow_name: static-energy.vasp.mit\nstructure: POSCAR\n</code></pre>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/","title":"b) Building a database","text":"<p>Tip</p> <p>Ensure you've read the previous section! Database setup can be complex, and most users can bypass it entirely.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#selecting-your-database-engine","title":"Selecting your database engine","text":"<p>Simmate employs Django ORM for database construction and management, meaning any Django-supported database can be used with Simmate. </p> <p>This encompasses PostgreSQL, MariaDB, MySQL, Oracle, SQLite, and others via third-party providers. noSQL databases like MongoDB are supported through djongo. Comprehensive documentation for Django databases is available here. </p> <p>However, we strongly recommend opting for Postgres, which we discuss in the following section.</p> <p>Warning</p> <p>Our team utilizes SQLite (for local testing) and PostgreSQL (for production), so currently, we can only provide guidance on these two backends. You're free to use others, but be aware that we haven't extensively tested these backends and may not be able to assist with troubleshooting if issues occur.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#introduction-to-postgres-setup","title":"Introduction to Postgres setup","text":"<p>PostgreSQL is free and open-source, allowing you to avoid costs and set it up manually.</p> <p>However, using a database service such as DigitalOcean, Linode, GoogleCloud, AWS, Azure, or another provider is MUCH simpler. These providers set up the database for you through a user-friendly interface.</p> <p>If you prefer to manually build a Postgres server, numerous tutorials and guides are available (1, 2, etc.). Be aware that this can be time-consuming and your final database connection may be slower if your team operates from multiple locations. </p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#setting-up-postgres-with-digitalocean","title":"Setting up Postgres with DigitalOcean","text":""},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#introduction-expected-costs","title":"Introduction &amp; expected costs","text":"<p>Our team uses DigitalOcean, where the basic database server (~$15/month) is sufficient for Simmate usage. You'll only need &gt;10GB if you're running &gt;100,000 structure relaxations or frequently using unit cells with &gt;1000 atoms.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#i-account-creation","title":"(i) Account creation","text":"<p>Start by creating an account on DigitalOcean using this link (our referral). We suggest signing in with your Github account. This referral link provides:</p> <ol> <li>$100 credit for servers from DigitalOcean (valid for 60 days)</li> <li>$10 credit for the Simmate team from DigitalOcean, helping fund our servers</li> </ol> <p>If you encounter any issues, please verify that DigitalOcean is still offering this deal here. Simmate is not affiliated with DigitalOcean.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#ii-cloud-database-creation","title":"(ii) Cloud database creation","text":"<ol> <li>On the DigitalOcean dashboard, click the green \"Create\" button in the top right and select \"Database\". This should take you to this page.</li> <li>For \"database engine\", select the latest version of PostgreSQL (currently v14)</li> <li>Leave the rest of the page's options at their default values.</li> <li>Click Create a Database Cluster when ready.</li> <li>On your new cluster's homepage, there's a \"Get Started\" button. We'll go through this dialog in the next section.</li> </ol> <p>Note, this is the database cluster, which can host multiple databases (each with their own tables).</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#iii-database-connection","title":"(iii) Database connection","text":"<p>Before setting up our database on this cluster, we'll first try connecting to the default database on it (named <code>defaultdb</code>).</p> <ol> <li>On your new database's page, you'll see a \"Getting Started\" dialog -- select it!</li> <li>\"Restrict inbound connections\" is optional and beginners should skip it for now. We skip this because if you're running calculations on a supercomputer/cluster, you'll need to add ALL the associated IP addresses for connections to work properly. That's a lot of IP addresses to collect and configure correctly -- so we leave this to advanced users.</li> <li>\"Connection details\" is the information we need to provide to Simmate/Django. Let's copy this information. For example, here's what the details look like on DigitalOcean: <pre><code>username = doadmin\npassword = asd87a9sd867fasd\nhost = db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com\nport = 25060\ndatabase = defaultdb\nsslmode = require\n</code></pre></li> <li> <p>In your Simmate Python environment, ensure you have the Postgres engine installed. The package is <code>psycopg2</code>, which allows Django to communicate with Postgres. To install this, run the command: <pre><code>conda install -n my_env -c conda-forge psycopg2\n</code></pre></p> </li> <li> <p>We need to pass this information to Simmate (which connects using Django). To do this, add a file named <code>my_env-database.yaml</code> (using your conda env name) to your Simmate config directory (<code>~/simmate</code>) with the following content -- make sure to substitute in your connection information and note that ENGINE tells Django we are using Postgres: <pre><code>default:\n  ENGINE: django.db.backends.postgresql\n  HOST: db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com\n  NAME: defaultdb\n  USER: doadmin\n  PASSWORD: asd87a9sd867fasd\n  PORT: 25060\n  OPTIONS:\n    sslmode: require\n</code></pre></p> </li> <li>Verify that you can connect to this database on your local computer by running the following in Spyder: <pre><code>from simmate.configuration import settings\n\nprint(settings.database)  # this should display your connect info!\n</code></pre></li> </ol>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#iv-creating-a-separate-database-for-testing-on-the-same-server","title":"(iv) Creating a separate database for testing (on the same server)","text":"<p>Just as we don't use the <code>(base)</code> environment in Anaconda, we don't want to use the default database <code>defaultdb</code> on our cluster. Here we'll create a new database -- one that we can delete if we want to start over.</p> <ol> <li>On DigitalOcean with your Database Cluster page, select the \"Users&amp;Databases\" tab.</li> <li>Create a new database using the \"Add new database\" button and name it <code>simmate-database-00</code>. We name it this way because you may want to create new/separate databases and numbering is a quick way to keep track of these.</li> <li>In your connection settings (from the section above), switch the NAME from defaultdb to <code>simmate-database-00</code>. You will change this in your <code>my_env-database.yaml</code> file.</li> </ol>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#v-building-our-database-tables","title":"(v) Building our database tables","text":"<p>Now that we've set up and connected to our database, we can create our Simmate database tables and start populating them with data! We do this the same way we did without a cloud database:</p> <ol> <li>In your terminal, ensure you have your Simmate environment activated</li> <li>Run the following command:  <pre><code>simmate database reset\n</code></pre></li> <li>You're now ready to start using Simmate with your new database!</li> </ol>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#vi-creating-a-connection-pool","title":"(vi) Creating a connection pool","text":"<p>When we have multiple calculations running simultaneously, we need to ensure our database can handle all these connections. Therefore, we create a connection pool which allows for thousands of connections. This \"pool\" operates like a waitlist where the database handles each connection request in sequence.</p> <ol> <li>Select the \"Connection Pools\" tab and then \"Create a Connection Pool\"</li> <li>Name your pool <code>simmate-database-00-pool</code> and select <code>simmate-database-00</code> for the database</li> <li>Select \"Transaction\" for our mode (the default) and set our pool size to 10 (or adjust this value as needed)</li> <li>Create the pool when ready!</li> <li>You'll need to update your <code>my_env-database.yaml</code> file to these connection settings. At this point your file will look similar to this (note, our NAME and PORT values have changed): <pre><code>default:\n  ENGINE: django.db.backends.postgresql\n  HOST: db-postgresql-nyc3-49797-do-user-8843535-0.b.db.ondigitalocean.com\n  NAME: simmate-database-00-pool  # THIS LINE WAS UPDATED\n  USER: doadmin\n  PASSWORD: asd87a9sd867fasd\n  PORT: 25061\n  OPTIONS:\n    sslmode: require\n</code></pre></li> </ol> <p>Warning</p> <p>Calling <code>simmate database reset</code> when using a connection pool will NOT work! If you ever need to reset your database, ensure you connect to the database directly instead of through a database pool.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#vii-loading-third-party-data","title":"(vii) Loading third-party data","text":"<p>This step is optional.</p> <p>With Sqlite, we could download a prebuilt database with data from third parties already in it. However, creating our Postgres database means our database is entirely empty.</p> <p>To load ALL third-party data (~5GB total), you can use the following command. We can also use Dask to run this in parallel and speed things up. Depending on your internet connection and CPU speed, this can take up to 24 hours.</p> <pre><code>simmate database load-remote-archives --parallel\n</code></pre> <p>Warning</p> <p><code>--parallel</code> will use all cores on your CPU. Keep this in mind if you are running other programs/calculations on your computer already.</p>"},{"location":"getting_started/use_a_cloud_database/build_a_postgres_database/#viii-sharing-the-database","title":"(viii) Sharing the database","text":"<p>If you want to share this database with others, they simply need to copy your config file: <code>my_env-database.yaml</code>. They won't need to run <code>simmate database reset</code> because you did it for them.</p>"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/","title":"Should I Create My Own Database?","text":""},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#sharing-a-database-with-others","title":"Sharing a Database with Others","text":"<p>A cloud database allows you to store your results on a remote server via an internet connection. Once a database is established, you can add unlimited users and connections. </p> <p>If you're part of a team, only ONE member needs to set up and manage ONE cloud database. Collaboration is possible for anyone with a username and password.</p>"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#collaborating-with-the-warren-lab","title":"Collaborating with the Warren Lab","text":"<p>Ideally, the entire scientific community could work together, sharing their results. Our Simmate team encourages as many labs as possible to collaborate. If you're interested in joining this effort, simply email simmate.team@gmail.com. As a team member, you won't need to set up or manage any cloud database.</p>"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#using-a-private-database","title":"Using a Private Database","text":"<p>If you prefer a private database for your team, appoint one member as the database manager. This person alone needs to complete the next section (on setting up your cloud database). All other members should wait for connection information before proceeding to the final section (on connecting to your cloud database).</p> <p>In summary, only establish your own cloud database if:</p> <ol> <li>You prefer a private database over Simmate's collaborative effort.</li> <li>You are the designated manager of your team's private database.</li> </ol>"},{"location":"getting_started/use_a_cloud_database/private_vs_collab/#connecting-to-a-cloud-database","title":"Connecting to a Cloud Database","text":"<p>If you're collaborating with someone who has already set up a database, connecting to it will be straightforward. </p> <p>Once you have your cloud database's connection parameters, create the file <code>~/simmate/my_env-database.yaml</code> and input the connection parameters provided by your point-person. For instance, this <code>my_env-database.yaml</code> file designates a <code>default</code> database: <pre><code>default:\n  ENGINE: django.db.backends.postgresql\n  HOST: simmate-database-do-user-8843535-0.b.db.ondigitalocean.com\n  NAME: simmate-database-00-pool\n  USER: doadmin\n  PASSWORD: ryGEc5PDxC2IHDSM\n  PORT: 25061\n  OPTIONS:\n    sslmode: require\n</code></pre></p> <p>That's all! When you run a new workflow, results will be saved to this cloud database instead of your local file.</p> <p>Danger</p> <p>If your lab uses postgres, ensure you have the necessary database dependencies installed. For postgres, execute the command: <pre><code>conda install -n my_env -c conda-forge psycopg2\n</code></pre></p>"},{"location":"getting_started/use_a_cloud_database/quick_start/","title":"Utilize a Cloud Database","text":""},{"location":"getting_started/use_a_cloud_database/quick_start/#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Consider joining forces! Simmate is designed for sharing results, so reach out to simmate.team@gmail.com to discuss becoming part of our team. This will help you bypass the complexities of managing your own database. If you decide to join, you'll only need to follow steps 3 and 4 of this guide.</p> </li> <li> <p>Establish a cloud database that is supported by django. We strongly suggest setting up a connection pool for your database as well. If you need assistance with this setup, you can utilize our \"deploy\" button in the following section. A 10GB database is sufficient to begin with.</p> </li> <li> <p>Ensure you have the necessary database dependencies installed. For postgres, execute the following command: <pre><code>conda install -n my_env -c conda-forge psycopg2\n</code></pre></p> </li> <li> <p>Include the file <code>~/simmate/my_env-database.yaml</code> with your connection details that align with the django format. For instance, this <code>my_env-database.yaml</code> file provides a <code>default</code> database to use: <pre><code>default:\n  ENGINE: django.db.backends.postgresql\n  HOST: simmate-database-do-user-8843535-0.b.db.ondigitalocean.com\n  NAME: simmate-database-00-pool\n  USER: doadmin\n  PASSWORD: ryGEc5PDxC2IHDSM\n  PORT: 25061\n  OPTIONS:\n    sslmode: require\n</code></pre></p> </li> <li> <p>If you have created a brand new database in step 2, you will need to reset your database to build initial tables. Use the command <code>simmate database reset</code> to do this.  Do NOT execute this command if you have joined a shared database!</p> </li> </ol>"},{"location":"getting_started/use_a_cloud_database/quick_start/#setup-with-digital-ocean","title":"Setup with Digital Ocean","text":"<p>We recommend using Postgres via DigitalOcean. If you don't have a Digital Ocean account, please sign up using our referral link. The button below will direct you to the appropriate page.</p> <p>Please note, we are not affiliated with Digital Ocean -- it's simply the platform our team prefers to use.</p> <p> </p>"},{"location":"getting_started/workflows/configure_database/","title":"Database Configuration","text":"<p>Simmate enables multi-material calculations by pre-building database tables. These tables, akin to Excel spreadsheets, have pre-set column headers. </p> <p>Take for example a table of structures, which may have columns for formula, density, and number of sites, among others. Simmate not only generates these tables but also fills the columns with data once a calculation is completed. </p>"},{"location":"getting_started/workflows/configure_database/#1-database-initialization","title":"1. Database Initialization","text":"<p>We'll explore the structure of these tables in tutorial 5. For now, we need Simmate to create them. This can be done by running the following command:</p> <pre><code>simmate database reset\n</code></pre> <p>Upon execution, Simmate will show a series of messages. These can be ignored at this point as they relate to the table creation process.</p> <p>Warning</p> <p>Be aware that running the command <code>simmate database reset</code> will delete your existing database and replace it with a new one with empty tables. To keep your previous runs, make sure to backup your database by copying and pasting the database file.</p>"},{"location":"getting_started/workflows/configure_database/#2-locating-the-database-file","title":"2. Locating the Database File","text":"<p>After running <code>simmate database reset</code>, the database can be located in a file named <code>~/simmate/my_env-database.sqlite3</code>. </p> <p>Note</p> <p>Note that the name of your conda environment (<code>my_env</code> in this case) is part of the database file name. This is a Simmate feature that lets you switch between databases by simply changing your Anaconda environment. This is especially handy when testing and developing new workflows, which we'll discuss in a future tutorial.</p> <p>To find this file, remember from tutorial 1 that <code>~</code> is a shorthand for our home directory, typically something like <code>/home/jacksund/</code> or <code>C:\\Users\\jacksund</code>.</p> <p>This file can't be opened by double-clicking. Just as Excel is needed to open and read Excel (.xlsx) files, a separate program is required to read database (.sqlite3) files. We'll use Simmate for this later.</p> <p>With just one command, our database is ready for use! We can now run workflows and start filling it with data.</p>"},{"location":"getting_started/workflows/configure_potcars/","title":"Setting Up Potentials (for VASP users)","text":""},{"location":"getting_started/workflows/configure_potcars/#what-is-vasp","title":"What is VASP?","text":"<p>VASP is a widely used software for executing DFT calculations. However, our team cannot install it for you due to its commercial licensing. You need to acquire it directly from the VASP team, with whom we have no affiliation. </p>"},{"location":"getting_started/workflows/configure_potcars/#setting-up-vasp","title":"Setting Up VASP","text":"<p>Even though VASP can only be installed on Linux, we will guide you through the process of configuring VASP on your local computer, regardless of whether it's a Windows or Mac system. For this, you only need the Potentials that come with the VASP installation files. You can either:</p> <ol> <li>Extract these from the VASP installation files located at <code>vasp/5.x.x/dist/Potentials</code>. Remember to unpack the <code>tar.gz</code> files.</li> <li>Request a copy of these files from a team member or your IT department.</li> </ol> <p>After obtaining the potentials, place them in a folder named <code>~/simmate/vasp/Potentials</code>. This is the same directory where your database is located (<code>~/simmate</code>). Here, you need to create a new folder named <code>vasp</code>. This folder should contain the potentials that came with VASP, maintaining their original folder and file names. Once you've completed these steps, your folder should look like this:</p> <pre><code># Located at /home/my_username (~)\nsimmate/\n\u251c\u2500\u2500 my_env-database.sqlite3\n\u2514\u2500\u2500 vasp\n    \u2514\u2500\u2500 Potentials\n        \u251c\u2500\u2500 LDA\n        \u2502   \u251c\u2500\u2500 potpaw_LDA\n        \u2502   \u251c\u2500\u2500 potpaw_LDA.52\n        \u2502   \u251c\u2500\u2500 potpaw_LDA.54\n        \u2502   \u2514\u2500\u2500 potUSPP_LDA\n        \u251c\u2500\u2500 PBE\n        \u2502   \u251c\u2500\u2500 potpaw_PBE\n        \u2502   \u251c\u2500\u2500 potpaw_PBE.52\n        \u2502   \u2514\u2500\u2500 potpaw_PBE.54\n        \u2514\u2500\u2500 PW91\n            \u251c\u2500\u2500 potpaw_GGA\n            \u2514\u2500\u2500 potUSPP_GGA\n</code></pre>"},{"location":"getting_started/workflows/configure_potcars/#verifying-your-configuration","title":"Verifying Your Configuration","text":"<p>If the folder is not set up correctly, subsequent commands may fail, resulting in an error like this:</p> <pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/home/jacksund/simmate/vasp/Potentials/PBE/potpaw_PBE.54/Na/POTCAR'\n</code></pre> <p>If you encounter this error, revisit your folder setup.</p> <p>Danger</p> <p>Our team only has access to VASP v5.4.4. If your folder structure differs for newer versions of VASP, please inform us by opening an issue.</p>"},{"location":"getting_started/workflows/make_a_structure/","title":"Creating a Structure File","text":"<p>Before initiating a workflow, a crystal structure is required. There are numerous ways to obtain a crystal structure, such as downloading one online or creating one from scratch. In this guide, we will create a structure file from scratch without using any software.</p>"},{"location":"getting_started/workflows/make_a_structure/#creating-a-text-txt-file","title":"Creating a Text (txt) File","text":"<p>Firstly, create a new text file on your Desktop named <code>POSCAR.txt</code>. You can use any text editor of your choice (Notepad, Sublime, etc.). Alternatively, you can create the file using the command line:</p> <pre><code>nano POSCAR.txt\n</code></pre> <p>Ensure that the <code>.txt</code> extension is visible by enabling \"show file name extensions\".</p> <p>Next, copy and paste the following text into the file:</p> <pre><code>Na1 Cl1\n1.0\n3.485437 0.000000 2.012318\n1.161812 3.286101 2.012318\n0.000000 0.000000 4.024635\nNa Cl\n1 1\ndirect\n0.000000 0.000000 0.000000 Na\n0.500000 0.500000 0.500000 Cl\n</code></pre> <p>This text represents a structure, which consists of a lattice and a list of atomic sites. The lattice is defined by a 3x3 matrix (lines 3-5), and the sites are a list of xyz coordinates with an element (lines 8-9 show fractional coordinates). </p>"},{"location":"getting_started/workflows/make_a_structure/#exploring-a-different-format-cif","title":"Exploring a Different Format (cif)","text":"<p>There are various ways to write structure information. The example above uses the VASP's \"POSCAR\" format. Another common format is CIF. Although it's not as neat as a POSCAR, it contains similar information. You can use either CIFs or POSCAR formats when using Simmate.</p> <pre><code>data_NaCl\n_symmetry_space_group_name_H-M   'P 1'\n_cell_length_a   4.02463542\n_cell_length_b   4.02463542\n_cell_length_c   4.02463542\n_cell_angle_alpha   60.00000000\n_cell_angle_beta   60.00000000\n_cell_angle_gamma   60.00000000\n_symmetry_Int_Tables_number   1\n_chemical_formula_structural   NaCl\n_chemical_formula_sum   'Na1 Cl1'\n_cell_volume   46.09614833\n_cell_formula_units_Z   1\nloop_\n _symmetry_equiv_pos_site_id\n _symmetry_equiv_pos_as_xyz\n  1  'x, y, z'\nloop_\n _atom_site_type_symbol\n _atom_site_label\n _atom_site_symmetry_multiplicity\n _atom_site_fract_x\n _atom_site_fract_y\n _atom_site_fract_z\n _atom_site_occupancy\n  Na  Na0  1  0.00000000  0.00000000  0.00000000  1\n  Cl  Cl1  1  0.50000000  0.50000000  0.50000000  1\n</code></pre>"},{"location":"getting_started/workflows/make_a_structure/#understanding-file-extensions","title":"Understanding File Extensions","text":"<p>Most files you will interact with are text files, just in different formats. This is where file extensions come in (<code>.txt</code>, <code>.cif</code>, <code>.csv</code>, ...). </p> <p>These extensions indicate the format we are using. Files named <code>something.cif</code> inform programs that we have a text file written in the CIF structure format. </p> <p>VASP uses the name POSCAR (without any file extension) to indicate its format. If we rename our file from <code>POSCAR.txt</code> to <code>POSCAR</code>, all programs (VESTA, OVITO, and others) will recognize the structure. </p> <p>Note</p> <p>In Windows, you may receive a warning about changing the file extension. You can safely ignore this warning and change the extension.</p> <p>Fun-fact</p> <p>A Microsoft Word document is essentially a folder of text files. The .docx file extension tells Word that we have the folder in their desired format. Try renaming a word file from <code>my_file.docx</code> to <code>my_file.zip</code> and open it to explore. Most programs operate in a similar manner!</p>"},{"location":"getting_started/workflows/make_a_structure/#renaming-your-file-in-the-command-line","title":"Renaming Your File in the Command-Line","text":"<p>If you're using the command-line to create/edit this file, you can use the copy (<code>cp</code>) command to rename your <code>POSCAR.txt</code> file to <code>POSCAR</code>:</p> <pre><code>cp POSCAR.txt POSCAR\n</code></pre> <p>Your structure file is now ready to use!</p>"},{"location":"getting_started/workflows/quick_start/","title":"Explore &amp; Run Workflows","text":""},{"location":"getting_started/workflows/quick_start/#quick-start","title":"Quick Start","text":"<p>Danger</p> <p>This tutorial assumes that you have VASP installed and that the <code>vasp_std</code> command is accessible in your path. </p> <p>Simmate's next release (planned for spring 2024) will not require VASP.</p> <ol> <li> <p>Initialize your Simmate database before running a workflow. Your database will be created at <code>~/simmate/my_env-database.sqlite3</code>, where \"my_env\" is the name of your active conda environment: <pre><code>simmate database reset\n</code></pre></p> </li> <li> <p>Create a structure file for sodium chloride (NaCl) to practice calculations. Name it <code>POSCAR</code> and fill it with the following content... <pre><code>Na1 Cl1\n1.0\n3.485437 0.000000 2.012318\n1.161812 3.286101 2.012318\n0.000000 0.000000 4.024635\nNa Cl\n1 1\ndirect\n0.000000 0.000000 0.000000 Na\n0.500000 0.500000 0.500000 Cl\n</code></pre></p> </li> <li> <p>Use the following command to view a list of all available workflows: <pre><code>simmate workflows list-all\n</code></pre></p> </li> <li> <p>Learn about all workflows interactively with the following command: <pre><code>simmate workflows explore\n</code></pre></p> </li> <li> <p>Copy and paste VASP POTCAR files into the <code>~/simmate/vasp/Potentials</code> folder. Make sure to unpack the <code>tar.gz</code> files. This folder should contain the potentials that came with VASP, maintaining their original folder and file names: <pre><code># Located at /home/my_username (~)\nsimmate/\n\u2514\u2500\u2500 vasp\n    \u2514\u2500\u2500 Potentials\n        \u251c\u2500\u2500 LDA\n        \u2502   \u251c\u2500\u2500 potpaw_LDA\n        \u2502   \u251c\u2500\u2500 potpaw_LDA.52\n        \u2502   \u251c\u2500\u2500 potpaw_LDA.54\n        \u2502   \u2514\u2500\u2500 potUSPP_LDA\n        \u251c\u2500\u2500 PBE\n        \u2502   \u251c\u2500\u2500 potpaw_PBE\n        \u2502   \u251c\u2500\u2500 potpaw_PBE.52\n        \u2502   \u2514\u2500\u2500 potpaw_PBE.54\n        \u2514\u2500\u2500 PW91\n            \u251c\u2500\u2500 potpaw_GGA\n            \u2514\u2500\u2500 potUSPP_GGA\n</code></pre></p> </li> <li> <p>Once everything is configured, you can submit your workflow using the website interface, command-line, or Python. Here, we'll use a settings file in YAML format. Create a file named <code>my_example.yaml</code> with the following content: <pre><code>workflow_name: static-energy.vasp.mit\nstructure: POSCAR\ncommand: mpirun -n 5 vasp_std &gt; vasp.out  # OPTIONAL\ndirectory: my_new_folder  # OPTIONAL\n</code></pre></p> </li> <li> <p>Run the workflow configuration file we just created <pre><code>simmate workflows run my_example.yaml\n</code></pre></p> </li> <li> <p>Once the workflow is complete, you'll find files named <code>simmate_metadata.yaml</code> and <code>simmate_summary.yaml</code> which contain some quick information. Other workflows (like <code>band-structure</code> calculations) will also generate plots for you.</p> </li> <li> <p>While the plots and summary files are useful for quick testing, more detailed information is stored in our database. We'll cover how to access your database in a subsequent tutorial.</p> </li> </ol>"},{"location":"getting_started/workflows/running_the_workflow/","title":"Executing our Workflow!","text":"<p>Warning</p> <p>The following commands will fail unless you have VASP installed on your local computer.  That's perfectly fine!  We'll attempt to run these commands regardless. This will demonstrate how Simmate workflows fail when VASP isn't configured correctly. If VASP isn't installed, you'll encounter an error stating that the <code>vasp_std</code> command is unrecognized (like <code>vasp_std: not found</code> on Linux). We'll transition to a remote computer with VASP installed in the subsequent section.</p>"},{"location":"getting_started/workflows/running_the_workflow/#executing-a-workflow-via-the-command-line","title":"Executing a Workflow via the Command-line","text":"<p>By default, Simmate runs everything immediately and locally on your desktop. When executing the workflow, it creates a new folder, writes the inputs, runs the calculation, and saves the results to your database.</p> <p>The command to execute our POSCAR and static-energy/mit workflow is... </p> <pre><code>simmate workflows run-quick static-energy.vasp.mit --structure POSCAR\n</code></pre> <p>Tip</p> <p>The commands in this section may seem long and tedious to write out, and even more challenging to remember. Don't worry, this will become easier once we learn how to submit using YAML files in the next section.</p> <p>Tip</p> <p>We refer to this command as <code>run-quick</code> because it's typically used for quick testing by advanced users. Most of the time, you'll be using the <code>run</code> command, which we will cover below.</p> <p>By default, Simmate uses the command <code>vasp_std &gt; vasp.out</code> and creates a new <code>simmate-task</code> folder with a unique identifier (e.g., <code>simmate-task-j8djk3mn8</code>).</p> <p>What if we wanted to modify this command or the directory it's executed in? Remember the output from the <code>simmate workflows explore</code> command, which listed parameters for us. We can use any of these to modify how our workflow runs.</p> <p>For instance, we can change our folder name (<code>--directory</code>) and the command used to run VASP (<code>--command</code>). With these, we can update our command to:</p> <pre><code>simmate workflows run-quick static-energy.vasp.mit --structure POSCAR --command \"mpirun -n 4 vasp_std &gt; vasp.out\" --directory my_custom_folder\n</code></pre> <p>If you encounter any errors, please let our team know by posting a question. </p> <p>If not, congratulations   !!! You now know how to execute workflows with a single command and understand what Simmate is doing behind the scenes.</p>"},{"location":"getting_started/workflows/running_the_workflow/#executing-a-workflow-with-a-settings-file","title":"Executing a Workflow with a Settings File","text":"<p>In the previous section, you may have noticed that our <code>simmate workflows run</code> command was becoming quite long and thus difficult to remember. Instead of typing out this lengthy command each time, we can create a settings file that contains all this information. We will write our settings into a <code>YAML</code> file, a simple text file. The name of our settings file doesn't matter, so we'll just use <code>my_settings.yaml</code>. To create this file, do the following:</p> <pre><code>nano my_settings.yaml\n</code></pre> <p>... and input the following information ...</p> <pre><code>workflow_name: static-energy.vasp.mit\nstructure: POSCAR\ncommand: mpirun -n 4 vasp_std &gt; vasp.out  # OPTIONAL\ndirectory: my_custom_folder  # OPTIONAL\n</code></pre> <p>This file contains all the information from our <code>simmate workflows run-quick</code> command above. But now it's stored in a file that we can read/edit later if needed. To submit this file, we simply run...</p> <pre><code>simmate workflows run my_settings.yaml\n</code></pre> <p>Your workflow will execute the same as before. It's entirely up to you whether workflows are submitted using a yaml file or the longer command.</p> <p>Tip</p> <p>Remember how the command <code>simmate workflows explore</code> listed all the parameters for us to use. These are all your options when submitting the workflow. In the example command above, we decided to set two of the optional parameters.</p> <p>Tip</p> <p>Want to customize a specific setting (e.g., set ENCUT to a custom value)? Customizing workflow settings is covered in tutorial 6. However, try to resist jumping ahead! There are still several important steps to learn before customizing workflows.</p>"},{"location":"getting_started/workflows/running_the_workflow/#mastering-workflow-options","title":"Mastering Workflow Options","text":"<p>In the previous examples, we provided our input structure as a <code>POSCAR</code> -- but what if we wanted to use a different format? Or use a structure from a previous calculation or the Materials Project database?</p> <p>Refer back to the Parameters section of our documentation.</p> <p>Under <code>structure</code>, we see that we can use...</p> <ul> <li> cif or poscar files </li> <li> reference a database entry</li> <li> point to a third-party database</li> <li> use advanced python objects</li> </ul> <p>For instance, you can try running the following workflow:</p> <pre><code>workflow_name: static-energy.vasp.mit\nstructure:\n    database_table: MatprojStructure\n    database_id: mp-123\ncommand: mpirun -n 4 vasp_std &gt; vasp.out  # OPTIONAL\ndirectory: my_custom_folder  # OPTIONAL\n</code></pre> <p>Even though we didn't create a structure file, Simmate fetched one for us from the Materials Project database.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/","title":"Switching to a Remote Cluster","text":"<p>Warning</p> <p>This section may be challenging for beginners. If possible, work through it with an experienced user or someone from your IT department. Don't be discouraged if it takes more than an hour -- there's a lot to learn!</p> <p>Up until now, you've been running Simmate on your local desktop or laptop. However, as we saw in the previous section, we need VASP (which requires Linux) for Simmate's workflows to run. Most of the time, you'll be using a University or Federal supercomputer (also known as \"high performance computing (HPC) clusters\"), which will already have VASP installed.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#cluster-specific-guides","title":"Cluster-Specific Guides","text":"<p>For teams actively using Simmate, we provide additional notes and examples for submitting to specific clusters. This includes:</p> <ul> <li>WarWulf: The Warren lab's \"BeoWulf\" cluster at UNC Chapel Hill</li> <li>LongLeaf: UNC's university cluster for most use-cases (1 node limit)</li> <li>DogWood: UNC's university cluster designed for massively parallel jobs (&gt;1 node)</li> </ul> <p>Tip</p> <p>If your cluster/university is not listed, contact your IT team for assistance in completing this tutorial.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#a-checklist-for-clusters","title":"A Checklist for Clusters","text":"<p>For workflows to run correctly, the following requirements must be met:</p> <ul> <li> A VASP license for your team (purchased on their site)</li> <li> A remote cluster that you have a profile with (e.g., UNC's LongLeaf)</li> <li> VASP installed on the remote cluster</li> <li> Anaconda installed on the remote cluster</li> </ul> <p>Ensure these steps are completed before proceeding.</p> <p>Tip</p> <p>For the Warren Lab, these items are already configured on <code>WarWulf</code>, <code>LongLeaf</code>, and <code>DogWood</code>.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#1-sign-in-to-the-cluster","title":"1. Sign in to the Cluster","text":"<p>If you've never signed into a remote cluster before, we will do this using SSH (Secure Shell). Run the following command in your local terminal:</p> exampleWarWulfLongLeafDogWood <pre><code>ssh my_username@my_cluster.edu\n</code></pre> <pre><code>ssh WarrenLab@warwulf.net\n</code></pre> <p>Note</p> <p>Everyone shares the profile \"WarrenLab\". Ask Scott for the password (scw@email.unc.edu)</p> <pre><code>ssh my_onyen@longleaf.unc.edu\n</code></pre> <pre><code>ssh my_username@my_cluster.edu\n</code></pre> <p>Danger</p> <p>On Windows, use your Command Prompt -- not the Anaconda Powershell Prompt.</p> <p>After entering your password, you are now using a terminal on the remote supercomputer. Try running the command <code>pwd</code> (\"print working directory\") to show that your terminal is indeed running commands on the remote cluster, not your desktop:</p> <pre><code># This is the same for all Linux clusters\npwd\n</code></pre>"},{"location":"getting_started/workflows/submit_to_a_cluster/#2-load-vasp","title":"2. Load VASP","text":"<p>To load VASP into your environment, you typically need to run a 'load module' command:</p> exampleWarWulfLongLeafDogWood <pre><code>module load vasp\n</code></pre> <pre><code>module load vasp; source /opt/ohpc/pub/intel/bin/ifortvars.sh;\n</code></pre> <pre><code>module load vasp/5.4.4\n</code></pre> <pre><code>module load vasp/5.4.4\n</code></pre> <p>Then check that the VASP command is found. If the <code>vasp_std</code> command worked correctly, you will see the following output (because their command doesn't print help information like <code>simmate</code> or <code>conda</code>):</p> <pre><code>vasp_std\n</code></pre> <pre><code># Error output may vary between different VASP versions\nError reading item 'VCAIMAGES' from file INCAR.\n</code></pre>"},{"location":"getting_started/workflows/submit_to_a_cluster/#3-build-your-personal-simmate-environment","title":"3. Build Your Personal Simmate Environment","text":"<p>Next, we need to ensure Simmate is installed. </p> <p>If you see <code>(base)</code> at the start of your command line, Anaconda is already installed.</p> <p>If not, ask your IT team how they want you to install it. Typically, it's by using miniconda, which is just Anaconda without the graphical user interface. </p> <p>With Anaconda set up, you can create your environment and install Simmate just like we did in the first tutorial:</p> <pre><code># Create your conda environment with...\n\nconda create -n my_env -c conda-forge python=3.11 simmate\nconda activate my_env\n\n# Initialize your database on this new installation.\nsimmate database reset\n</code></pre> <p>Danger</p> <p>On WarWulf, we share a profile, so make sure you name your environment something unique. For example, use <code>yourname_env</code> (e.g., <code>jacks_env</code>).</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#4-set-up-vasp-potentials","title":"4. Set up VASP Potentials","text":"<p>Note</p> <p>This step is already completed for you on the <code>WarWulf</code> cluster.</p> <p>Next, copy your Potentials into <code>~/simmate/vasp/Potentials</code> and also copy the <code>POSCAR</code> file above onto your cluster. </p> <p>Moving files around or transferring them between your local computer and the supercomputer can be challenging in the command line. It's much easier with a program like FileZilla, MobaXTerm, or another file transfer program. We recommend FileZilla, but it's entirely optional and up to you.</p> <p>Review our POTCAR guide from before if you need help on this step.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#5-move-to-your-scratch-directory","title":"5. Move to Your 'Scratch' Directory","text":"<p>Typically, clusters have a \"scratch\" directory that you should submit jobs from -- which is different from your home directory. Make sure you switch to that before submitting any workflows. (Note, your <code>POSCAR</code> and all input files should be in this directory too):</p> exampleWarWulfLongLeafDogWood <pre><code>cd /path/to/my/scratch/space/\n</code></pre> <pre><code>cd /media/synology/user/your_name\n</code></pre> <pre><code>cd /pine/scr/j/a/jacksund\n</code></pre> <pre><code>cd /21dayscratch/scr/y/o/youronyen\n</code></pre>"},{"location":"getting_started/workflows/submit_to_a_cluster/#6-build-our-input-files","title":"6. Build Our Input Files","text":"<p>Just like we did on our laptop, we need to make our input files. For now, let's use this sample YAML file:</p> <pre><code>workflow_name: static-energy.vasp.mit\nstructure:\n    database_table: MatprojStructure\n    database_id: mp-22862\ncommand: mpirun -n 4 vasp_std &gt; vasp.out  # OPTIONAL\ndirectory: my_custom_folder  # OPTIONAL\n</code></pre> <p>Put this in a file named <code>my_settings.yaml</code> in your scratch directory.</p> <p>Danger</p> <p>Take note of the <code>-n 4</code> in our command. This is the number of cores that we want our calculation to use. Make sure this number matches your  <code>cpus-per-task</code> setting in the next section.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#7-build-our-submit-script","title":"7. Build Our Submit Script","text":"<p>Earlier in this tutorial, we called <code>simmate workflows run ...</code> directly in our terminal, but this should NEVER be done on a supercomputer. Instead, we should submit the workflow to the cluster's job queue. Typically, supercomputers use SLURM or PBS to submit jobs.</p> <p>For example, UNC's <code>WarWulf</code>, <code>LongLeaf</code>, and <code>DogWood</code> clusters each use SLURM. </p> <p>To submit, we would make a file named <code>submit.sh</code>:</p> <pre><code>nano submit.sh\n</code></pre> <p>... and use contents like ...</p> exampleWarWulfLongLeafDogWood <pre><code>#! /bin/sh\n\n#SBATCH --job-name=my_example_job\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=4GB\n#SBATCH --time=01:00:00\n#SBATCH --partition=general\n#SBATCH --output=slurm.out \n#SBATCH --mail-type=ALL \n#SBATCH --mail-user=my_username@live.unc.edu\n\nsimmate workflows run my_settings.yaml\n</code></pre> <pre><code>#!/bin/bash\n\n#. /opt/ohpc/pub/suppress.sh  #suppress infiniband output, set vasp path\n\n#SBATCH --job-name=my_example_job\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=4GB\n#SBATCH --time=01:00:00\n#SBATCH --partition=p1\n#SBATCH --output=slurm.out \n#SBATCH --mail-type=ALL \n#SBATCH --mail-user=my_username@live.unc.edu\n\nsimmate workflows run my_settings.yaml\n</code></pre> <pre><code>#! /bin/sh\n\n#SBATCH --job-name=my_example_job\n#SBATCH --nodes=20\n#SBATCH --ntasks=1\n#SBATCH --mem=40g\n#SBATCH --partition=general\n#SBATCH --output=slurm.out\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=youronyen@live.unc.edu\n#SBATCH --time=11-00:00\n\nsimmate workflows run my_settings.yaml\n</code></pre> <p>Danger</p> <p>Note the large <code>ntasks</code> and <code>node</code> values here. DogWood is only meant for large calculations, so talk with our team before submitting.</p> <pre><code>#!/bin/sh\n\n#SBATCH --job-name=NEB\n#SBATCH --ntasks=704\n#SBATCH --nodes=16\n#SBATCH --time=2-00:00\n#SBATCH --mem=300g\n#SBATCH --partition=2112_queue\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=lamcrae@live.unc.edu\n\nsimmate workflows run my_settings.yaml\n</code></pre> <p>Info</p> <p>Each of these <code>SBATCH</code> parameters sets how we would like to submit a job and how many resources we expect to use. These are explained in SLURM's documentation for sbatch, but you may need help from your IT team to update them. But to break down these example parameters...</p> <ul> <li><code>job-name</code>: the name that identifies your job. It will be visible when you check the status of your job.</li> <li><code>nodes</code>: the number of server nodes (or CPUs) that you request. Typically leave this at 1.</li> <li><code>ntasks</code>: the number tasks that you'll be running. We run one workflow at a time here, so we use 1.</li> <li><code>cpus-per-task</code>: the number of CPU tasks required for each run. We run our workflow using 4 cores (<code>mpirun -n 4</code>), so we need to request 4 cores for it here.</li> <li><code>mem</code>: the memory requested for this job. If it is exceeded, the job will be terminated.</li> <li><code>time</code>: the maximum time requested for this job. If it is exceeded, the job will be terminated.</li> <li><code>partition</code>: the group of nodes that we request resources on. You can often remove this line and use the cluster's default.</li> <li><code>output</code>: the name of the file to write the job output (including errors).</li> <li><code>mail-type</code> + <code>mail-user</code>: will send an email alerts when a jobs starts/stops/fails/etc.</li> </ul>"},{"location":"getting_started/workflows/submit_to_a_cluster/#8-double-check-everything","title":"8. Double Check Everything","text":"<p>Let's go back through our checklist before we submit:</p> <ul> <li> Loaded the VASP module</li> <li> Activated your conda environment</li> <li> In the temporary working directory</li> <li> Have our <code>yaml</code> file (+ extra inputs like a POSCAR) in the directory</li> <li> Have our <code>submit.sh</code> in the directory</li> <li> Structure file (e.g., <code>POSCAR</code>) is present in working directory</li> </ul> <p>If all of these are set, you're good to go.</p>"},{"location":"getting_started/workflows/submit_to_a_cluster/#9-submit-a-workflow-to-the-queue","title":"9. Submit a Workflow to the Queue","text":"<p>Finally, let's submit to our cluster! </p> <pre><code>sbatch submit.sh\n</code></pre>"},{"location":"getting_started/workflows/submit_to_a_cluster/#10-monitor-its-progress","title":"10. Monitor Its Progress","text":"<p>You can then monitor your job's progress with:</p> exampleWarWulfLongLeafDogWood <pre><code>squeue -u my_username\n</code></pre> <pre><code>sq\n# or\nsq | grep my_name\n</code></pre> <p>Example</p> <p><code>sq | grep jack</code></p> <pre><code>squeue -u my_onyen\n</code></pre> <pre><code>squeue -u my_onyen\n</code></pre>"},{"location":"getting_started/workflows/submit_to_a_cluster/#success","title":"Success!","text":"<p>Congratulations! You've now submitted a Simmate workflow to a remote cluster   !!! </p> <p>Tip</p> <p>Be sure to review this section a few times before moving on. Submitting remote jobs can be tedious, but it's important to understand. Advanced features of Simmate will let you skip a lot of this work down the road, but that won't happen until we reach the \"Adding Computational Resources\" guide.</p>"},{"location":"getting_started/workflows/view_all_workflows/","title":"Exploring Available Workflows","text":""},{"location":"getting_started/workflows/view_all_workflows/#recap","title":"Recap","text":"<p>In the preceding sections, we accomplished the following prerequisites for running a workflow:</p> <ul> <li> Specified the location of our VASP files to Simmate</li> <li> Configured our database for result storage</li> <li> Selected a structure for our calculation</li> </ul> <p>Now, let's delve into the various workflows at our disposal and select one to execute.</p>"},{"location":"getting_started/workflows/view_all_workflows/#accessing-all-workflows","title":"Accessing All Workflows","text":"<p>Primarily, Simmate is used to compute a material's energy, structure, or properties. For each of these tasks, we have preconfigured workflows. You can access all of these via the <code>simmate workflows</code> command.</p> <p>To view all available workflows, run:</p> <pre><code>simmate workflows list-all\n</code></pre> <p>The output will resemble the following:</p> <pre><code>These are the workflows that have been registered:\n        (01) customized.vasp.user-config\n        (02) diffusion.vasp.neb-all-paths-mit\n        (03) diffusion.vasp.neb-from-endpoints-mit\n        (04) diffusion.vasp.neb-from-images-mit\n        (05) diffusion.vasp.neb-single-path-mit\n        (06) dynamics.vasp.matproj\n        (07) dynamics.vasp.mit\n        (08) dynamics.vasp.mvl-npt\n        (09) electronic-structure.vasp.matproj-full\n  ... &lt;&lt; additional workflows truncated for brevity &gt;&gt;\n</code></pre>"},{"location":"getting_started/workflows/view_all_workflows/#understanding-a-workflow","title":"Understanding a Workflow","text":"<p>Next, use the <code>explore</code> command for a more interactive way to view the available workflows.</p> <pre><code>simmate workflows explore\n</code></pre> <p>When prompted, select a workflow type or a specific preset. A description of the chosen workflow will be displayed at the end. For instance, here's the output of the <code>relaxation.vasp.staged</code> workflow, frequently used in our evolutionary search algorithm. This output was obtained by running <code>simmate workflows explore</code>, selecting option <code>6</code> (relaxation), and then option <code>1</code> (matproj):</p> <pre><code>===================== relaxation.vasp.matproj =====================\n\n\nDescription:\n\nThis task is a reimplementation of pymatgen's MPRelaxSet.                                                                                                                       \n\nRuns a VASP geometry optimization using Materials Project settings.                                                                                                             \n\nMaterials Project settings are often considered the minimum-required quality for publication and is sufficient for most applications. If you are looking at one structure in    \ndetail (for electronic, vibrational, and other properties), you should still test for convergence using higher-quality settings.                                                \n\n\nParameters:\n\nREQUIRED PARAMETERS\n--------------------\n- structure\n\nOPTIONAL PARAMETERS (+ their defaults):\n---------------------------------------\n- directory: null\n- command: vasp_std &gt; vasp.out\n- is_restart: false\n- run_id: null\n- compress_output: false\n- source: null\n- standardize_structure: false\n- symmetry_precision: 0.01\n- angle_tolerance: 0.5\n\n*** 'null' indicates the parameter is set with advanced logic\n\nTo understand each parameter, you can read through our parameter docs, which give full descriptions and examples.                                                               \n\n\n==================================================================\n</code></pre>"},{"location":"getting_started/workflows/view_all_workflows/#understanding-parameters","title":"Understanding Parameters","text":"<p>In the above message, there's a reference to our Parameter docs. You can access this page by clicking the Parameters section at the top of this webpage (or click here).</p> <p>This page provides a comprehensive list of ALL parameters for ALL workflows. If you want to learn more about a specific input, this is your go-to resource.</p>"},{"location":"getting_started/workflows/view_all_workflows/#choosing-a-workflow-for-practice","title":"Choosing a Workflow for Practice","text":"<p>For the remainder of this tutorial, we will use the <code>static-energy.vasp.mit</code> workflow, which performs a basic static energy calculation using MIT Project settings (these settings are based on pymatgen's MITRelaxSet).</p>"},{"location":"getting_started/workflows/view_the_results/","title":"Reviewing Workflow Results","text":""},{"location":"getting_started/workflows/view_the_results/#understanding-basic-output-files","title":"Understanding Basic Output Files","text":"<p>Upon completion of your job, you will find additional files in your output. One such file is <code>simmate_summary.yaml</code>, which provides a brief summary of your results.</p> <p>This file contains a subset of the data available in the database: <pre><code>_DATABASE_TABLE_: StaticEnergy\n_TABLE_ID_: 1\n_WEBSITE_URL_: http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1\nband_gap: 4.9924\nchemical_system: Cl-Na\ncomputer_system: digital-storm\nconduction_band_minimum: 4.306\ncorrections: []\ncreated_at: 2022-09-10 14:32:35.857088+00:00\ndensity: 2.1053060843576104\ndensity_atomic: 0.04338757298280908\ndirectory: /home/jacksund/Documents/spyder_wd/simmate-task-e9tddsyw\nenergy: -27.25515165\nenergy_fermi: -0.63610593\nenergy_per_atom: -3.40689395625\nformula_anonymous: AB\nformula_full: Na4 Cl4\nformula_reduced: NaCl\nid: 42\nis_gap_direct: true\nlattice_stress_norm: 8.428394235089161\nlattice_stress_norm_per_atom: 1.0535492793861452\nnelements: 2\nnsites: 8\nrun_id: 3a1bd23f-705c-4947-96fa-3740865ed12d\nsite_force_norm_max: 1.4907796617877505e-05\nsite_forces_norm: 2.257345786537809e-05\nsite_forces_norm_per_atom: 2.8216822331722614e-06\nspacegroup_id: 225\nupdated_at: 2022-09-10 14:33:09.419637+00:00\nvalence_band_maximum: -0.6864\nvolume: 184.38459332974767\nvolume_molar: 13.87987468758872\nworkflow_name: static-energy.vasp.mit\nworkflow_version: 0.10.0\n</code></pre></p> <p>Different workflows may generate additional files and plots. For instance, <code>electronic-structure</code> workflows compute a band structure using Materials Project settings and create an image of your final band structure named <code>band_structure.png</code>. These additional files and plots, which vary by workflow, facilitate a quick review of your results.</p>"},{"location":"getting_started/workflows/view_the_results/#accessing-results-via-the-website","title":"Accessing Results via the Website","text":"<p>The <code>simmate_summary.yaml</code> file includes a <code>_WEBSITE_URL_</code>. To view your results interactively, copy and paste this URL into your browser. Ensure your local server is running before doing so:</p> <pre><code>simmate run-server\n</code></pre> <p>Then, open the link provided by <code>_WEBSITE_URL_</code>:</p> <pre><code>http://127.0.0.1:8000/workflows/static-energy/vasp/mit/1\n</code></pre> <p>Note</p> <p>Keep in mind that the server and your database are confined to your local computer. Attempting to access a URL on a computer that doesn't have the same database file will fail. You may need to transfer your database file from the cluster to your local computer. Alternatively, if you wish to access results online, consider switching to a cloud database. This process is explained in a subsequent tutorial.</p>"},{"location":"getting_started/workflows/view_the_results/#performing-advanced-data-analysis","title":"Performing Advanced Data Analysis","text":"<p>Simmate's toolkit and database allow for in-depth analysis of our final structure and complete results. As this requires Python, our next tutorial will guide you through interacting with the toolkit using Python. Subsequent tutorials will cover accessing our database. </p>"}]}